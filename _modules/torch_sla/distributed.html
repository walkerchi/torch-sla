<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="../../genindex.html"><link rel="search" title="Search" href="../../search.html">
        <link rel="canonical" href="https://walkerchi.github.io/torch-sla/_modules/torch_sla/distributed.html">
        <link rel="prefetch" href="../../_static/logo.jpg" as="image">

    <link rel="shortcut icon" href="../../_static/logo.jpg"><!-- Generated with Sphinx 7.4.7 and Furo 2025.12.19 -->
        <title>torch_sla.distributed - torch-sla: PyTorch Sparse Linear Algebra | GPU Accelerated</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=69152257" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">torch-sla: PyTorch Sparse Linear Algebra | GPU Accelerated</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/logo.jpg" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">torch-sla: PyTorch Sparse Linear Algebra | GPU Accelerated</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_sla.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <h1>Source code for torch_sla.distributed</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Distributed Sparse Matrix for large-scale CFD/FEM computations.</span>

<span class="sd">Provides domain decomposition with halo exchange, following the standard</span>
<span class="sd">approach used in Ansys, OpenFOAM, and other industrial CFD/FEM solvers.</span>

<span class="sd">Key Features:</span>
<span class="sd">- Graph-based partitioning (METIS or simple geometric methods)</span>
<span class="sd">- Halo/ghost node exchange for parallel computations</span>
<span class="sd">- Support for both CPU and CUDA devices</span>
<span class="sd">- Same API as SparseTensor for easy migration</span>

<span class="sd">Example</span>
<span class="sd">-------</span>
<span class="sd">&gt;&gt;&gt; from torch_sla import DSparseMatrix</span>
<span class="sd">&gt;&gt;&gt; </span>
<span class="sd">&gt;&gt;&gt; # Create from global matrix</span>
<span class="sd">&gt;&gt;&gt; A_global = SparseTensor(val, row, col, shape)</span>
<span class="sd">&gt;&gt;&gt; A_dist = DSparseMatrix.from_global(A_global, num_partitions=4)</span>
<span class="sd">&gt;&gt;&gt; </span>
<span class="sd">&gt;&gt;&gt; # Distributed solve</span>
<span class="sd">&gt;&gt;&gt; x_dist = A_dist.solve(b_dist)</span>
<span class="sd">&gt;&gt;&gt; </span>
<span class="sd">&gt;&gt;&gt; # Halo exchange for iterative methods</span>
<span class="sd">&gt;&gt;&gt; A_dist.halo_exchange(local_x)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Literal</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.backends</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">is_scipy_available</span><span class="p">,</span>
    <span class="n">is_eigen_available</span><span class="p">,</span>
    <span class="n">is_cusolver_available</span><span class="p">,</span>
    <span class="n">is_cudss_available</span><span class="p">,</span>
    <span class="n">select_backend</span><span class="p">,</span>
    <span class="n">select_method</span><span class="p">,</span>
    <span class="n">BackendType</span><span class="p">,</span>
    <span class="n">MethodType</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
    <span class="n">DIST_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">DIST_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Partition</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Represents a single partition/subdomain&quot;&quot;&quot;</span>
    <span class="n">partition_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">local_nodes</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>      <span class="c1"># Global indices of local nodes</span>
    <span class="n">owned_nodes</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>      <span class="c1"># Nodes owned by this partition (not halo)</span>
    <span class="n">halo_nodes</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>       <span class="c1"># Ghost/halo nodes from neighbors</span>
    <span class="n">neighbor_partitions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="c1"># Neighboring partition IDs</span>
    <span class="n">send_indices</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>  <span class="c1"># Nodes to send to each neighbor</span>
    <span class="n">recv_indices</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>  <span class="c1"># Where to place received data</span>
    <span class="n">global_to_local</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>  <span class="c1"># Mapping from global to local indices</span>
    <span class="n">local_to_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>  <span class="c1"># Mapping from local to global indices</span>
    

<span class="k">def</span><span class="w"> </span><span class="nf">partition_graph_metis</span><span class="p">(</span>
    <span class="n">row</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">col</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_parts</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Partition graph using METIS (if available) or fallback to simple method.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    partition_ids : torch.Tensor</span>
<span class="sd">        Partition ID for each node [num_nodes]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">pymetis</span>
        <span class="c1"># Build adjacency list</span>
        <span class="n">adjacency</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">)]</span>
        <span class="n">row_cpu</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">col_cpu</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">row_cpu</span><span class="p">,</span> <span class="n">col_cpu</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">r</span> <span class="o">!=</span> <span class="n">c</span><span class="p">:</span>  <span class="c1"># Skip diagonal</span>
                <span class="n">adjacency</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        
        <span class="c1"># Run METIS</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">membership</span> <span class="o">=</span> <span class="n">pymetis</span><span class="o">.</span><span class="n">part_graph</span><span class="p">(</span><span class="n">num_parts</span><span class="p">,</span> <span class="n">adjacency</span><span class="o">=</span><span class="n">adjacency</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">membership</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;pymetis not available, using simple geometric partitioning&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">partition_simple</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_parts</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">partition_simple</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_parts</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple 1D partitioning (fallback when METIS not available)&quot;&quot;&quot;</span>
    <span class="n">nodes_per_part</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_nodes</span> <span class="o">+</span> <span class="n">num_parts</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_parts</span>
    <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">):</span>
        <span class="n">partition_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="n">nodes_per_part</span><span class="p">,</span> <span class="n">num_parts</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">partition_ids</span>


<span class="k">def</span><span class="w"> </span><span class="nf">partition_coordinates</span><span class="p">(</span>
    <span class="n">coords</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">num_parts</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;rcb&#39;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Partition based on node coordinates using Recursive Coordinate Bisection (RCB).</span>
<span class="sd">    </span>
<span class="sd">    This is common in CFD/FEM for mesh partitioning.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    coords : torch.Tensor</span>
<span class="sd">        Node coordinates [num_nodes, dim]</span>
<span class="sd">    num_parts : int</span>
<span class="sd">        Number of partitions (should be power of 2 for RCB)</span>
<span class="sd">    method : str</span>
<span class="sd">        &#39;rcb&#39;: Recursive Coordinate Bisection</span>
<span class="sd">        &#39;slicing&#39;: Simple slicing along longest axis</span>
<span class="sd">        </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    partition_ids : torch.Tensor</span>
<span class="sd">        Partition ID for each node</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">coords</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;rcb&#39;</span><span class="p">:</span>
        <span class="n">_rcb_partition</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">partition_ids</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_parts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># slicing</span>
        <span class="c1"># Find longest axis</span>
        <span class="n">ranges</span> <span class="o">=</span> <span class="n">coords</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">coords</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">ranges</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="c1"># Sort by that axis</span>
        <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">coords</span><span class="p">[:,</span> <span class="n">axis</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
        <span class="n">nodes_per_part</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_nodes</span> <span class="o">+</span> <span class="n">num_parts</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_parts</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_idx</span><span class="p">):</span>
            <span class="n">partition_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="n">nodes_per_part</span><span class="p">,</span> <span class="n">num_parts</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">partition_ids</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_rcb_partition</span><span class="p">(</span>
    <span class="n">coords</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">partition_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">node_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">part_offset</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_parts</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Recursive Coordinate Bisection helper&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">num_parts</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">node_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">partition_ids</span><span class="p">[</span><span class="n">node_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">part_offset</span>
        <span class="k">return</span>
    
    <span class="c1"># Find longest axis</span>
    <span class="n">local_coords</span> <span class="o">=</span> <span class="n">coords</span><span class="p">[</span><span class="n">node_indices</span><span class="p">]</span>
    <span class="n">ranges</span> <span class="o">=</span> <span class="n">local_coords</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">local_coords</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">ranges</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    
    <span class="c1"># Find median</span>
    <span class="n">axis_vals</span> <span class="o">=</span> <span class="n">local_coords</span><span class="p">[:,</span> <span class="n">axis</span><span class="p">]</span>
    <span class="n">median</span> <span class="o">=</span> <span class="n">axis_vals</span><span class="o">.</span><span class="n">median</span><span class="p">()</span>
    
    <span class="c1"># Split</span>
    <span class="n">left_mask</span> <span class="o">=</span> <span class="n">axis_vals</span> <span class="o">&lt;=</span> <span class="n">median</span>
    <span class="n">right_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">left_mask</span>
    
    <span class="n">left_nodes</span> <span class="o">=</span> <span class="n">node_indices</span><span class="p">[</span><span class="n">left_mask</span><span class="p">]</span>
    <span class="n">right_nodes</span> <span class="o">=</span> <span class="n">node_indices</span><span class="p">[</span><span class="n">right_mask</span><span class="p">]</span>
    
    <span class="c1"># Handle uneven splits</span>
    <span class="n">left_parts</span> <span class="o">=</span> <span class="n">num_parts</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">right_parts</span> <span class="o">=</span> <span class="n">num_parts</span> <span class="o">-</span> <span class="n">left_parts</span>
    
    <span class="n">_rcb_partition</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">partition_ids</span><span class="p">,</span> <span class="n">left_nodes</span><span class="p">,</span> <span class="n">part_offset</span><span class="p">,</span> <span class="n">left_parts</span><span class="p">)</span>
    <span class="n">_rcb_partition</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">partition_ids</span><span class="p">,</span> <span class="n">right_nodes</span><span class="p">,</span> <span class="n">part_offset</span> <span class="o">+</span> <span class="n">left_parts</span><span class="p">,</span> <span class="n">right_parts</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">find_halo_nodes</span><span class="p">(</span>
    <span class="n">row</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">col</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">partition_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">partition_id</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find halo/ghost nodes for a partition.</span>
<span class="sd">    </span>
<span class="sd">    Halo nodes are nodes owned by other partitions but connected to this partition&#39;s nodes.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    halo_nodes : torch.Tensor</span>
<span class="sd">        Global indices of halo nodes</span>
<span class="sd">    send_map : Dict[int, torch.Tensor]</span>
<span class="sd">        For each neighbor, which of our owned nodes to send</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">owned_mask</span> <span class="o">=</span> <span class="n">partition_ids</span> <span class="o">==</span> <span class="n">partition_id</span>
    <span class="n">owned_nodes</span> <span class="o">=</span> <span class="n">owned_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">owned_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">owned_nodes</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
    
    <span class="c1"># Find edges crossing partition boundary</span>
    <span class="n">halo_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">neighbor_nodes</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># neighbor_id -&gt; set of nodes to send</span>
    
    <span class="n">row_cpu</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">col_cpu</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">row_cpu</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">col_cpu</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
        <span class="n">r_owned</span> <span class="o">=</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">owned_set</span>
        <span class="n">c_owned</span> <span class="o">=</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">owned_set</span>
        
        <span class="k">if</span> <span class="n">r_owned</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">c_owned</span><span class="p">:</span>
            <span class="c1"># c is a halo node</span>
            <span class="n">halo_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
            <span class="n">neighbor_id</span> <span class="o">=</span> <span class="n">partition_ids</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">neighbor_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">neighbor_nodes</span><span class="p">:</span>
                <span class="n">neighbor_nodes</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="n">neighbor_nodes</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>  <span class="c1"># We need to send r to this neighbor</span>
        
        <span class="k">if</span> <span class="n">c_owned</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">r_owned</span><span class="p">:</span>
            <span class="c1"># r is a halo node</span>
            <span class="n">halo_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
            <span class="n">neighbor_id</span> <span class="o">=</span> <span class="n">partition_ids</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">neighbor_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">neighbor_nodes</span><span class="p">:</span>
                <span class="n">neighbor_nodes</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="n">neighbor_nodes</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    
    <span class="n">halo_nodes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">halo_set</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">send_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">neighbor_nodes</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    
    <span class="k">return</span> <span class="n">halo_nodes</span><span class="p">,</span> <span class="n">send_map</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DSparseMatrix</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Distributed Sparse Matrix with halo exchange support.</span>
<span class="sd">    </span>
<span class="sd">    Designed for large-scale CFD/FEM computations following industrial</span>
<span class="sd">    practices from Ansys, OpenFOAM, etc.</span>
<span class="sd">    </span>
<span class="sd">    The matrix is partitioned across multiple processes/GPUs, with automatic</span>
<span class="sd">    halo (ghost) node management for parallel iterative solvers.</span>
<span class="sd">    </span>
<span class="sd">    Supports both CPU and CUDA devices.</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    partition : Partition</span>
<span class="sd">        Local partition information</span>
<span class="sd">    local_values : torch.Tensor</span>
<span class="sd">        Non-zero values for local portion of matrix</span>
<span class="sd">    local_row : torch.Tensor</span>
<span class="sd">        Local row indices</span>
<span class="sd">    local_col : torch.Tensor</span>
<span class="sd">        Local column indices</span>
<span class="sd">    local_shape : Tuple[int, int]</span>
<span class="sd">        Shape of local matrix (including halo)</span>
<span class="sd">    global_shape : Tuple[int, int]</span>
<span class="sd">        Shape of global matrix</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        Device where the matrix data resides (cpu or cuda)</span>
<span class="sd">    </span>
<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; # Create distributed matrix on CPU</span>
<span class="sd">    &gt;&gt;&gt; A = DSparseMatrix.from_global(val, row, col, shape, num_parts=4, my_part=0, device=&#39;cpu&#39;)</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Create distributed matrix on CUDA</span>
<span class="sd">    &gt;&gt;&gt; A_cuda = DSparseMatrix.from_global(val, row, col, shape, num_parts=4, my_part=0, device=&#39;cuda&#39;)</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Distributed matrix-vector product with halo exchange</span>
<span class="sd">    &gt;&gt;&gt; y = A.matvec(x)  # Automatically handles halo exchange</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Explicit halo exchange</span>
<span class="sd">    &gt;&gt;&gt; A.halo_exchange(x)  # Update halo values in x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">partition</span><span class="p">:</span> <span class="n">Partition</span><span class="p">,</span>
        <span class="n">local_values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">local_row</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">local_col</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">local_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">global_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">):</span>
        <span class="c1"># Convert device to torch.device</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">partition</span> <span class="o">=</span> <span class="n">partition</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_values</span> <span class="o">=</span> <span class="n">local_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_row</span> <span class="o">=</span> <span class="n">local_row</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_col</span> <span class="o">=</span> <span class="n">local_col</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_shape</span> <span class="o">=</span> <span class="n">local_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span> <span class="o">=</span> <span class="n">global_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_partitions</span> <span class="o">=</span> <span class="n">num_partitions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        
        <span class="c1"># Move partition tensors to device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partition_to_device</span><span class="p">()</span>
        
        <span class="c1"># For display</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_print_partition_info</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_partition_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move partition tensors to the target device&quot;&quot;&quot;</span>
        <span class="c1"># Note: We keep some partition info on CPU for indexing</span>
        <span class="c1"># Only move what&#39;s needed for computation</span>
        <span class="k">pass</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_print_partition_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Print partition info for user awareness&quot;&quot;&quot;</span>
        <span class="n">owned</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span><span class="p">)</span>
        <span class="n">halo</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">halo_nodes</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">neighbor_partitions</span><span class="p">)</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Partition </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">partition_id</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_partitions</span><span class="si">}</span><span class="s2">] &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Nodes: </span><span class="si">{</span><span class="n">owned</span><span class="si">}</span><span class="s2"> owned + </span><span class="si">{</span><span class="n">halo</span><span class="si">}</span><span class="s2"> halo = </span><span class="si">{</span><span class="n">total</span><span class="si">}</span><span class="s2"> local | &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Neighbors: </span><span class="si">{</span><span class="n">neighbors</span><span class="si">}</span><span class="s2"> | &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Global: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> | &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseMatrix&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Move the distributed matrix to a different device.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        device : str or torch.device</span>
<span class="sd">            Target device (&#39;cpu&#39;, &#39;cuda&#39;, &#39;cuda:0&#39;, etc.)</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseMatrix</span>
<span class="sd">            New distributed matrix on the target device</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">DSparseMatrix</span><span class="p">(</span>
            <span class="n">partition</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="p">,</span>
            <span class="n">local_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">local_row</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_row</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">local_col</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_col</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="n">local_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_shape</span><span class="p">,</span>
            <span class="n">global_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span><span class="p">,</span>
            <span class="n">num_partitions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>  <span class="c1"># Don&#39;t print again when moving</span>
        <span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseMatrix&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move to CUDA device&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseMatrix&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move to CPU&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if matrix is on CUDA&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span>
    
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_global</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">row</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">col</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">my_partition</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">partition_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseMatrix&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create distributed matrix from global COO data.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        values, row, col : torch.Tensor</span>
<span class="sd">            Global COO sparse matrix data</span>
<span class="sd">        shape : Tuple[int, int]</span>
<span class="sd">            Global matrix shape</span>
<span class="sd">        num_partitions : int</span>
<span class="sd">            Number of partitions</span>
<span class="sd">        my_partition : int</span>
<span class="sd">            This process&#39;s partition ID (0 to num_partitions-1)</span>
<span class="sd">        partition_ids : torch.Tensor, optional</span>
<span class="sd">            Pre-computed partition assignments. If None, computed automatically.</span>
<span class="sd">        coords : torch.Tensor, optional</span>
<span class="sd">            Node coordinates for geometric partitioning [num_nodes, dim]</span>
<span class="sd">        device : str or torch.device</span>
<span class="sd">            Device for local data (&#39;cpu&#39;, &#39;cuda&#39;, &#39;cuda:0&#39;, etc.)</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Whether to print partition info</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseMatrix</span>
<span class="sd">            Local portion of the distributed matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Compute partitioning if not provided</span>
        <span class="k">if</span> <span class="n">partition_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">coords</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">partition_coordinates</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">partition_graph_metis</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">)</span>
        
        <span class="c1"># Find owned and halo nodes</span>
        <span class="n">owned_mask</span> <span class="o">=</span> <span class="n">partition_ids</span> <span class="o">==</span> <span class="n">my_partition</span>
        <span class="n">owned_nodes</span> <span class="o">=</span> <span class="n">owned_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">halo_nodes</span><span class="p">,</span> <span class="n">send_map</span> <span class="o">=</span> <span class="n">find_halo_nodes</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">partition_ids</span><span class="p">,</span> <span class="n">my_partition</span><span class="p">)</span>
        
        <span class="c1"># All local nodes (owned + halo)</span>
        <span class="n">local_nodes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">owned_nodes</span><span class="p">,</span> <span class="n">halo_nodes</span><span class="p">])</span>
        <span class="n">num_local</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">local_nodes</span><span class="p">)</span>
        
        <span class="c1"># Build global-to-local mapping</span>
        <span class="n">global_to_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">num_nodes</span><span class="p">,),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">local_idx</span><span class="p">,</span> <span class="n">global_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">local_nodes</span><span class="p">):</span>
            <span class="n">global_to_local</span><span class="p">[</span><span class="n">global_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">local_idx</span>
        
        <span class="c1"># Extract local matrix entries</span>
        <span class="n">row_cpu</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="n">col_cpu</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="n">val_cpu</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        
        <span class="n">local_rows</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">local_cols</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">local_vals</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">row_cpu</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">col_cpu</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">val_cpu</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
            <span class="n">local_r</span> <span class="o">=</span> <span class="n">global_to_local</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">local_c</span> <span class="o">=</span> <span class="n">global_to_local</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            
            <span class="k">if</span> <span class="n">local_r</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">local_c</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">local_rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">local_r</span><span class="p">)</span>
                <span class="n">local_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">local_c</span><span class="p">)</span>
                <span class="n">local_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        
        <span class="n">local_row</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">local_rows</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">local_col</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">local_cols</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">local_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">local_vals</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="c1"># Build recv_indices (where to place received halo data)</span>
        <span class="n">recv_indices</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">halo_offset</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">owned_nodes</span><span class="p">)</span>
        <span class="n">halo_list</span> <span class="o">=</span> <span class="n">halo_nodes</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">neighbor_id</span> <span class="ow">in</span> <span class="n">send_map</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">neighbor_owned</span> <span class="o">=</span> <span class="p">(</span><span class="n">partition_ids</span> <span class="o">==</span> <span class="n">neighbor_id</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="n">recv_idx</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">neighbor_owned</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">halo_list</span><span class="p">:</span>
                    <span class="n">recv_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">halo_offset</span> <span class="o">+</span> <span class="n">halo_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">node</span><span class="p">))</span>
            <span class="n">recv_indices</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">recv_idx</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        
        <span class="c1"># Convert send_map from global node IDs to local indices</span>
        <span class="c1"># send_map currently contains global node IDs, but halo_exchange needs local indices</span>
        <span class="n">send_indices_local</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">neighbor_id</span><span class="p">,</span> <span class="n">global_nodes</span> <span class="ow">in</span> <span class="n">send_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">local_idx</span> <span class="o">=</span> <span class="n">global_to_local</span><span class="p">[</span><span class="n">global_nodes</span><span class="p">]</span>
            <span class="n">send_indices_local</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">local_idx</span>
        
        <span class="n">partition</span> <span class="o">=</span> <span class="n">Partition</span><span class="p">(</span>
            <span class="n">partition_id</span><span class="o">=</span><span class="n">my_partition</span><span class="p">,</span>
            <span class="n">local_nodes</span><span class="o">=</span><span class="n">local_nodes</span><span class="p">,</span>
            <span class="n">owned_nodes</span><span class="o">=</span><span class="n">owned_nodes</span><span class="p">,</span>
            <span class="n">halo_nodes</span><span class="o">=</span><span class="n">halo_nodes</span><span class="p">,</span>
            <span class="n">neighbor_partitions</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">send_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
            <span class="n">send_indices</span><span class="o">=</span><span class="n">send_indices_local</span><span class="p">,</span>  <span class="c1"># Use local indices instead of global</span>
            <span class="n">recv_indices</span><span class="o">=</span><span class="n">recv_indices</span><span class="p">,</span>
            <span class="n">global_to_local</span><span class="o">=</span><span class="n">global_to_local</span><span class="p">,</span>
            <span class="n">local_to_global</span><span class="o">=</span><span class="n">local_nodes</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">partition</span><span class="o">=</span><span class="n">partition</span><span class="p">,</span>
            <span class="n">local_values</span><span class="o">=</span><span class="n">local_values</span><span class="p">,</span>
            <span class="n">local_row</span><span class="o">=</span><span class="n">local_row</span><span class="p">,</span>
            <span class="n">local_col</span><span class="o">=</span><span class="n">local_col</span><span class="p">,</span>
            <span class="n">local_shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_local</span><span class="p">,</span> <span class="n">num_local</span><span class="p">),</span>
            <span class="n">global_shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">num_partitions</span><span class="o">=</span><span class="n">num_partitions</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_owned</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of owned (non-halo) nodes&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_halo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of halo/ghost nodes&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">halo_nodes</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_local</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Total local nodes (owned + halo)&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">nnz</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of non-zeros in local matrix&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_values</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Data type of matrix values&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_values</span><span class="o">.</span><span class="n">dtype</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">halo_exchange</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">async_op</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Exchange halo/ghost values with neighbors.</span>
<span class="sd">        </span>
<span class="sd">        This is the core operation for parallel iterative methods.</span>
<span class="sd">        Updates the halo portion of x with values from neighboring partitions.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            Local vector [num_local] with owned values filled in.</span>
<span class="sd">            Halo values will be updated.</span>
<span class="sd">        async_op : bool</span>
<span class="sd">            If True, return immediately and return a future.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            Vector with updated halo values (same tensor, modified in-place)</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; # During iterative solve</span>
<span class="sd">        &gt;&gt;&gt; for iteration in range(max_iter):</span>
<span class="sd">        &gt;&gt;&gt;     # Compute local update</span>
<span class="sd">        &gt;&gt;&gt;     x_new = local_gauss_seidel_step(A_local, x, b)</span>
<span class="sd">        &gt;&gt;&gt;     </span>
<span class="sd">        &gt;&gt;&gt;     # Exchange boundary values</span>
<span class="sd">        &gt;&gt;&gt;     A.halo_exchange(x_new)</span>
<span class="sd">        &gt;&gt;&gt;     </span>
<span class="sd">        &gt;&gt;&gt;     # Check convergence using owned nodes only</span>
<span class="sd">        &gt;&gt;&gt;     residual = compute_residual(A_local, x_new, b)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">DIST_AVAILABLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="c1"># Single-process fallback: just return (no exchange needed)</span>
            <span class="k">return</span> <span class="n">x</span>
        
        <span class="c1"># Prepare send buffers</span>
        <span class="n">send_buffers</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">recv_buffers</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="k">for</span> <span class="n">neighbor_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">neighbor_partitions</span><span class="p">:</span>
            <span class="n">send_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">send_indices</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">send_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">send_idx</span><span class="p">]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            
            <span class="n">recv_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">recv_indices</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span>
            <span class="n">recv_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">recv_idx</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Use send/recv for p2p communication</span>
        <span class="c1"># Note: For NCCL, we use synchronous send/recv</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span> <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;gloo&#39;</span>
        
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;nccl&#39;</span><span class="p">:</span>
            <span class="c1"># NCCL: use synchronous send/recv pairs</span>
            <span class="k">for</span> <span class="n">neighbor_id</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">neighbor_partitions</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">partition_id</span> <span class="o">&lt;</span> <span class="n">neighbor_id</span><span class="p">:</span>
                    <span class="c1"># Lower rank sends first, then receives</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">send_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">],</span> <span class="n">dst</span><span class="o">=</span><span class="n">neighbor_id</span><span class="p">)</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">recv_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">],</span> <span class="n">src</span><span class="o">=</span><span class="n">neighbor_id</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Higher rank receives first, then sends</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">recv_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">],</span> <span class="n">src</span><span class="o">=</span><span class="n">neighbor_id</span><span class="p">)</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">send_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">],</span> <span class="n">dst</span><span class="o">=</span><span class="n">neighbor_id</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Gloo: use non-blocking isend/irecv</span>
            <span class="n">requests</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">neighbor_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">neighbor_partitions</span><span class="p">:</span>
                <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">send_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">],</span> <span class="n">dst</span><span class="o">=</span><span class="n">neighbor_id</span><span class="p">)</span>
                <span class="n">requests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
                <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">(</span><span class="n">recv_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">],</span> <span class="n">src</span><span class="o">=</span><span class="n">neighbor_id</span><span class="p">)</span>
                <span class="n">requests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">requests</span>
            
            <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">requests</span><span class="p">:</span>
                <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        
        <span class="c1"># Update halo values</span>
        <span class="k">for</span> <span class="n">neighbor_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">neighbor_partitions</span><span class="p">:</span>
            <span class="n">recv_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">recv_indices</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x</span><span class="p">[</span><span class="n">recv_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">recv_buffers</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">halo_exchange_local</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Local halo exchange for single-process multi-partition simulation.</span>
<span class="sd">        </span>
<span class="sd">        Useful for testing/debugging without actual distributed setup.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x_list : List[torch.Tensor]</span>
<span class="sd">            List of local vectors, one per partition</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_all_partitions&#39;</span><span class="p">):</span>
            <span class="k">return</span>
        
        <span class="c1"># Build mapping from global to local for each partition</span>
        <span class="k">for</span> <span class="n">part_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_list</span><span class="p">)):</span>
            <span class="n">partition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_partitions</span><span class="p">[</span><span class="n">part_id</span><span class="p">]</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_list</span><span class="p">[</span><span class="n">part_id</span><span class="p">]</span>
            
            <span class="c1"># For each halo node, find which neighbor owns it and get the value</span>
            <span class="n">halo_offset</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">halo_idx</span><span class="p">,</span> <span class="n">global_node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">partition</span><span class="o">.</span><span class="n">halo_nodes</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
                <span class="n">local_halo_idx</span> <span class="o">=</span> <span class="n">halo_offset</span> <span class="o">+</span> <span class="n">halo_idx</span>
                
                <span class="c1"># Find which partition owns this node</span>
                <span class="k">for</span> <span class="n">neighbor_id</span> <span class="ow">in</span> <span class="n">partition</span><span class="o">.</span><span class="n">neighbor_partitions</span><span class="p">:</span>
                    <span class="n">neighbor_partition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_partitions</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span>
                    <span class="n">neighbor_g2l</span> <span class="o">=</span> <span class="n">neighbor_partition</span><span class="o">.</span><span class="n">global_to_local</span>
                    
                    <span class="k">if</span> <span class="n">global_node</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbor_g2l</span><span class="p">):</span>
                        <span class="n">local_idx_in_neighbor</span> <span class="o">=</span> <span class="n">neighbor_g2l</span><span class="p">[</span><span class="n">global_node</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">local_idx_in_neighbor</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">local_idx_in_neighbor</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbor_partition</span><span class="o">.</span><span class="n">owned_nodes</span><span class="p">):</span>
                            <span class="c1"># This neighbor owns the node</span>
                            <span class="n">x</span><span class="p">[</span><span class="n">local_halo_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_list</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">][</span><span class="n">local_idx_in_neighbor</span><span class="p">]</span>
                            <span class="k">break</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">matvec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">exchange_halo</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Local matrix-vector product y = A_local @ x.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            Local vector [num_local]</span>
<span class="sd">        exchange_halo : bool</span>
<span class="sd">            If True, perform halo exchange before multiplication</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : torch.Tensor</span>
<span class="sd">            Result vector [num_local]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">exchange_halo</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">halo_exchange</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Sparse matvec</span>
        <span class="n">A_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">local_row</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_col</span><span class="p">]),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_values</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_shape</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">A_local</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">(),</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">solve</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cg&#39;</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">distributed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve linear system Ax = b.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : torch.Tensor</span>
<span class="sd">            Right-hand side. Shape [num_owned] for owned nodes only.</span>
<span class="sd">        method : str</span>
<span class="sd">            Solver method: &#39;cg&#39; (default), &#39;jacobi&#39;, &#39;gauss_seidel&#39;</span>
<span class="sd">        atol : float</span>
<span class="sd">            Absolute tolerance for convergence</span>
<span class="sd">        maxiter : int</span>
<span class="sd">            Maximum iterations</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Print convergence info (rank 0 only for distributed)</span>
<span class="sd">        distributed : bool, default=True</span>
<span class="sd">            If True (default): Solve the GLOBAL system using distributed</span>
<span class="sd">            algorithms with all_reduce for global dot products.</span>
<span class="sd">            If False: Solve only the LOCAL subdomain problem (useful as</span>
<span class="sd">            preconditioner in domain decomposition methods).</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            Solution for owned nodes, shape [num_owned]</span>
<span class="sd">            </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; # Distributed solve (default) - all ranks cooperate</span>
<span class="sd">        &gt;&gt;&gt; x = local_matrix.solve(b_owned)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; # Local subdomain solve - no global communication</span>
<span class="sd">        &gt;&gt;&gt; x = local_matrix.solve(b_owned, distributed=False)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">distributed</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_distributed_cg</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_local</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_solve_local</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Local subdomain solve (no global communication).&quot;&quot;&quot;</span>
        <span class="c1"># Handle b size</span>
        <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">:</span>
            <span class="n">b_full</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_local</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">b_full</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">b_full</span>
        <span class="k">elif</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_local</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b must have size num_owned=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="si">}</span><span class="s2"> or num_local=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_local</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_local</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;jacobi&#39;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_jacobi</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;gauss_seidel&#39;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_gauss_seidel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># CG</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_cg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">]</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_solve_cg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Local CG solver for subdomain problems.</span>
<span class="sd">        </span>
<span class="sd">        This solves only the local subdomain problem without global reductions.</span>
<span class="sd">        Useful as a preconditioner or subdomain solver in domain decomposition.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">rs_old</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">],</span> <span class="n">r</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">])</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
            <span class="n">Ap</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">pAp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">],</span> <span class="n">Ap</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">])</span>
            
            <span class="k">if</span> <span class="n">pAp</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-30</span><span class="p">:</span>
                <span class="k">break</span>
                
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">rs_old</span> <span class="o">/</span> <span class="n">pAp</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">p</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">Ap</span>
            
            <span class="n">rs_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">],</span> <span class="n">r</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">])</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  CG iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: residual = </span><span class="si">{</span><span class="n">rs_new</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">rs_new</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">atol</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  CG converged at iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
            
            <span class="k">if</span> <span class="n">rs_old</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-30</span><span class="p">:</span>
                <span class="k">break</span>
                
            <span class="n">p</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="p">(</span><span class="n">rs_new</span> <span class="o">/</span> <span class="n">rs_old</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span>
            <span class="n">rs_old</span> <span class="o">=</span> <span class="n">rs_new</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_solve_jacobi</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Jacobi iteration with halo exchange&quot;&quot;&quot;</span>
        <span class="c1"># Extract diagonal</span>
        <span class="n">diag_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_row</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_col</span>
        <span class="n">diag_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_row</span><span class="p">[</span><span class="n">diag_mask</span><span class="p">]</span>
        <span class="n">diag_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_values</span><span class="p">[</span><span class="n">diag_mask</span><span class="p">]</span>
        
        <span class="n">D_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_local</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">D_inv</span><span class="p">[</span><span class="n">diag_indices</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">diag_values</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
            <span class="n">x_new</span> <span class="o">=</span> <span class="n">D_inv</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">D_inv</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">()</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
            
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_new</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">])</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_new</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Jacobi iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: diff = </span><span class="si">{</span><span class="n">diff</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">diff</span> <span class="o">&lt;</span> <span class="n">atol</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Jacobi converged at iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_solve_gauss_seidel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Gauss-Seidel iteration with halo exchange&quot;&quot;&quot;</span>
        <span class="c1"># Build CSR for efficient row access</span>
        <span class="n">A_csr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">local_row</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_col</span><span class="p">]),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_values</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_shape</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
        
        <span class="n">crow</span> <span class="o">=</span> <span class="n">A_csr</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">()</span>
        <span class="n">col</span> <span class="o">=</span> <span class="n">A_csr</span><span class="o">.</span><span class="n">col_indices</span><span class="p">()</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">A_csr</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
            <span class="n">x_old</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            
            <span class="c1"># Exchange halo before sweep</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">halo_exchange</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="c1"># Forward sweep on owned nodes only</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">):</span>
                <span class="n">row_start</span> <span class="o">=</span> <span class="n">crow</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">row_end</span> <span class="o">=</span> <span class="n">crow</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                
                <span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="n">diag</span> <span class="o">=</span> <span class="mf">1.0</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row_start</span><span class="p">,</span> <span class="n">row_end</span><span class="p">):</span>
                    <span class="n">c</span> <span class="o">=</span> <span class="n">col</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">v</span> <span class="o">=</span> <span class="n">val</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="n">i</span><span class="p">:</span>
                        <span class="n">diag</span> <span class="o">=</span> <span class="n">v</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">sigma</span> <span class="o">+=</span> <span class="n">v</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                
                <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">/</span> <span class="n">diag</span>
            
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_old</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">])</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  GS iter </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">: diff = </span><span class="si">{</span><span class="n">diff</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">diff</span> <span class="o">&lt;</span> <span class="n">atol</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  GS converged at iter </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_solve_distributed_cg</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">b_owned</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Distributed Conjugate Gradient solver.</span>
<span class="sd">        </span>
<span class="sd">        The key differences from local CG:</span>
<span class="sd">        1. Halo exchange before each matvec</span>
<span class="sd">        2. Global all_reduce for dot products</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_owned</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span>
        <span class="n">num_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_local</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">b_owned</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">partition_id</span>
        
        <span class="c1"># Initialize x_local = 0 (owned + halo)</span>
        <span class="n">x_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_local</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Extend b to local size (halo part is 0)</span>
        <span class="n">b_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_local</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">]</span> <span class="o">=</span> <span class="n">b_owned</span>
        
        <span class="c1"># r = b - A @ x (local, no halo exchange needed for x=0)</span>
        <span class="n">r_local</span> <span class="o">=</span> <span class="n">b_local</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">p_local</span> <span class="o">=</span> <span class="n">r_local</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        
        <span class="c1"># rs_old = r^T @ r (global reduction, only sum owned nodes)</span>
        <span class="n">rs_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">],</span> <span class="n">r_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">])</span>
        <span class="n">rs_old</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_reduce_sum</span><span class="p">(</span><span class="n">rs_local</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
            <span class="c1"># Halo exchange for p before matvec</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">halo_exchange</span><span class="p">(</span><span class="n">p_local</span><span class="p">)</span>
            
            <span class="c1"># Ap = A @ p (local matvec)</span>
            <span class="n">Ap_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">p_local</span><span class="p">,</span> <span class="n">exchange_halo</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            
            <span class="c1"># pAp = p^T @ A @ p (global reduction)</span>
            <span class="n">pAp_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">],</span> <span class="n">Ap_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">])</span>
            <span class="n">pAp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_reduce_sum</span><span class="p">(</span><span class="n">pAp_local</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">pAp</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-30</span><span class="p">:</span>
                <span class="k">break</span>
            
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">rs_old</span> <span class="o">/</span> <span class="n">pAp</span>
            
            <span class="c1"># Update x and r (local)</span>
            <span class="n">x_local</span> <span class="o">=</span> <span class="n">x_local</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">p_local</span>
            <span class="n">r_local</span> <span class="o">=</span> <span class="n">r_local</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">Ap_local</span>
            
            <span class="c1"># rs_new = r^T @ r (global reduction)</span>
            <span class="n">rs_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">],</span> <span class="n">r_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">])</span>
            <span class="n">rs_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_reduce_sum</span><span class="p">(</span><span class="n">rs_local</span><span class="p">)</span>
            
            <span class="n">residual</span> <span class="o">=</span> <span class="n">rs_new</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Distributed CG iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: residual = </span><span class="si">{</span><span class="n">residual</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">residual</span> <span class="o">&lt;</span> <span class="n">atol</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Distributed CG converged at iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, residual = </span><span class="si">{</span><span class="n">residual</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
            
            <span class="n">beta</span> <span class="o">=</span> <span class="n">rs_new</span> <span class="o">/</span> <span class="n">rs_old</span>
            <span class="n">p_local</span> <span class="o">=</span> <span class="n">r_local</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">p_local</span>
            <span class="n">rs_old</span> <span class="o">=</span> <span class="n">rs_new</span>
        
        <span class="c1"># Return only owned part</span>
        <span class="k">return</span> <span class="n">x_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">]</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_global_reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform global all_reduce sum.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">DIST_AVAILABLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">value</span>
        
        <span class="c1"># Ensure tensor is on the correct device for the backend</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;nccl&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">value</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="c1"># NCCL requires CUDA tensors</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">result</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">eigsh</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">which</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;LM&quot;</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">distributed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute k eigenvalues of symmetric matrix.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int</span>
<span class="sd">            Number of eigenvalues to compute</span>
<span class="sd">        which : str</span>
<span class="sd">            Which eigenvalues: &quot;LM&quot; (largest magnitude), &quot;SM&quot; (smallest magnitude)</span>
<span class="sd">        maxiter : int</span>
<span class="sd">            Maximum iterations</span>
<span class="sd">        tol : float</span>
<span class="sd">            Convergence tolerance</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Print convergence info (rank 0 only)</span>
<span class="sd">        distributed : bool, default=True</span>
<span class="sd">            If True (default): Use distributed LOBPCG with global reductions.</span>
<span class="sd">            If False: Gather to single SparseTensor and compute locally</span>
<span class="sd">            (not recommended for large matrices).</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        eigenvalues : torch.Tensor</span>
<span class="sd">            k eigenvalues, shape [k]</span>
<span class="sd">        eigenvectors_owned : torch.Tensor</span>
<span class="sd">            Eigenvectors for owned nodes only, shape [num_owned, k]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">distributed</span><span class="p">:</span>
            <span class="c1"># Gather to single node (not recommended)</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;distributed=False gathers entire matrix to one node. &quot;</span>
                         <span class="s2">&quot;Use distributed=True for large-scale problems.&quot;</span><span class="p">)</span>
            <span class="n">st</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse_tensor</span><span class="p">()</span>
            <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">eigsh</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="n">which</span><span class="p">)</span>
            <span class="c1"># Extract local portion</span>
            <span class="n">owned_nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span>
            <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="p">[</span><span class="n">owned_nodes</span><span class="p">]</span>
        <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">num_owned</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">partition_id</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_values</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        
        <span class="c1"># Initialize random subspace</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span> <span class="o">+</span> <span class="n">rank</span><span class="p">)</span>  <span class="c1"># Different per rank for diversity</span>
        <span class="n">m</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        
        <span class="c1"># Each rank has its local portion of X</span>
        <span class="n">X_owned</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_owned</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Orthogonalize globally</span>
        <span class="n">X_owned</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_orthogonalize</span><span class="p">(</span><span class="n">X_owned</span><span class="p">)</span>
        
        <span class="n">eigenvalues_prev</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
            <span class="c1"># Distributed matvec: AX</span>
            <span class="n">AX_owned</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_matvec_batch</span><span class="p">(</span><span class="n">X_owned</span><span class="p">)</span>
            
            <span class="c1"># Rayleigh-Ritz: H = X^T @ AX (global reduction)</span>
            <span class="c1"># Local contribution</span>
            <span class="n">H_local</span> <span class="o">=</span> <span class="n">X_owned</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">AX_owned</span>
            <span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_reduce_sum</span><span class="p">(</span><span class="n">H_local</span><span class="p">)</span>
            
            <span class="c1"># Solve small eigenvalue problem (same on all ranks)</span>
            <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
            
            <span class="c1"># Sort eigenvalues</span>
            <span class="k">if</span> <span class="n">which</span> <span class="o">==</span> <span class="s2">&quot;LM&quot;</span><span class="p">:</span>
                <span class="n">idx_sort</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">idx_sort</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
            <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">idx_sort</span><span class="p">]</span>
            <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">idx_sort</span><span class="p">]</span>
            
            <span class="c1"># Update X = X @ V (local)</span>
            <span class="n">X_owned</span> <span class="o">=</span> <span class="n">X_owned</span> <span class="o">@</span> <span class="n">eigenvectors</span>
            
            <span class="c1"># Check convergence</span>
            <span class="k">if</span> <span class="n">eigenvalues_prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">eigenvalues_prev</span><span class="p">[:</span><span class="n">k</span><span class="p">])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">diff</span> <span class="o">&lt;</span> <span class="n">tol</span> <span class="o">*</span> <span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Distributed LOBPCG converged at iteration </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">break</span>
            <span class="n">eigenvalues_prev</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Distributed LOBPCG iter </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">: _0 = </span><span class="si">{</span><span class="n">eigenvalues</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="c1"># Expand subspace with residual</span>
            <span class="k">if</span> <span class="n">iteration</span> <span class="o">&lt;</span> <span class="n">maxiter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">AX_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_matvec_batch</span><span class="p">(</span><span class="n">X_owned</span><span class="p">)</span>
                <span class="n">residual</span> <span class="o">=</span> <span class="n">AX_new</span> <span class="o">-</span> <span class="n">X_owned</span> <span class="o">*</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                
                <span class="c1"># Combine and orthogonalize</span>
                <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X_owned</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">],</span> <span class="n">residual</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">X_owned</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_orthogonalize</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
                
                <span class="c1"># Ensure correct size</span>
                <span class="k">if</span> <span class="n">X_owned</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">:</span>
                    <span class="n">extra</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_owned</span><span class="p">,</span> <span class="n">m</span> <span class="o">-</span> <span class="n">X_owned</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">X_owned</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X_owned</span><span class="p">,</span> <span class="n">extra</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">X_owned</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_orthogonalize</span><span class="p">(</span><span class="n">X_owned</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">],</span> <span class="n">X_owned</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_global_matvec_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_owned</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Distributed matvec for a batch of vectors.</span>
<span class="sd">        </span>
<span class="sd">        Each rank computes A @ X for its local portion.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_owned</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span>
        <span class="n">num_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_local</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">X_owned</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">X_owned</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        
        <span class="c1"># Extend to local size (owned + halo)</span>
        <span class="n">X_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_local</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">X_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_owned</span>
        
        <span class="c1"># Gather global X for halo (simplified - in production use p2p)</span>
        <span class="n">X_global</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_all_vectors</span><span class="p">(</span><span class="n">X_owned</span><span class="p">)</span>
        
        <span class="c1"># Fill halo from global</span>
        <span class="n">halo_nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">halo_nodes</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">halo_nodes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">X_local</span><span class="p">[</span><span class="n">num_owned</span><span class="p">:]</span> <span class="o">=</span> <span class="n">X_global</span><span class="p">[</span><span class="n">halo_nodes</span><span class="p">]</span>
        
        <span class="c1"># Local matvec for each column</span>
        <span class="n">Y_local</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_local</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">Y_local</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">X_local</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">exchange_halo</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">Y_local</span><span class="p">[:</span><span class="n">num_owned</span><span class="p">]</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_gather_all_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_owned</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Gather vectors from all ranks to build global vector.&quot;&quot;&quot;</span>
        <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">X_owned</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">X_owned</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        
        <span class="n">X_global</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">owned_nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span>
        <span class="n">X_global</span><span class="p">[</span><span class="n">owned_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_owned</span>
        
        <span class="c1"># All-reduce to combine</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_reduce_sum_inplace</span><span class="p">(</span><span class="n">X_global</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">X_global</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_global_reduce_sum_inplace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;In-place global all_reduce sum.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">DIST_AVAILABLE</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_global_orthogonalize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_owned</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Globally orthogonalize a distributed matrix using TSQR.</span>
<span class="sd">        </span>
<span class="sd">        Simplified version: gather, QR, scatter.</span>
<span class="sd">        Production version would use TSQR for better scalability.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Gather global X</span>
        <span class="n">X_global</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_all_vectors</span><span class="p">(</span><span class="n">X_owned</span><span class="p">)</span>
        
        <span class="c1"># QR on global (same result on all ranks)</span>
        <span class="n">Q</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X_global</span><span class="p">)</span>
        
        <span class="c1"># Extract local portion</span>
        <span class="n">owned_nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span>
        <span class="k">return</span> <span class="n">Q</span><span class="p">[</span><span class="n">owned_nodes</span><span class="p">]</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">gather_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_local</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gather local vectors to global vector (on rank 0).</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x_local : torch.Tensor</span>
<span class="sd">            Local vector [num_owned]</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x_global : torch.Tensor or None</span>
<span class="sd">            Global vector on rank 0, None on other ranks</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">DIST_AVAILABLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="c1"># Single process: just expand to global</span>
            <span class="n">x_global</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_local</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x_local</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x_global</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_local</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">x_global</span>
        
        <span class="c1"># Distributed gather</span>
        <span class="n">owned_vals</span> <span class="o">=</span> <span class="n">x_local</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">]</span>
        
        <span class="c1"># Gather sizes</span>
        <span class="n">local_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">)]</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">local_size</span><span class="p">)</span>
        
        <span class="c1"># Gather values</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x_global</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_local</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">gathered</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_local</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">]</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">owned_vals</span><span class="p">,</span> <span class="n">gather_list</span><span class="o">=</span><span class="n">gathered</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            
            <span class="c1"># Place in global vector (need owned_nodes from all partitions)</span>
            <span class="c1"># This requires additional communication of owned_nodes</span>
            <span class="k">return</span> <span class="n">x_global</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">owned_vals</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DSparseMatrix(partition=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="o">.</span><span class="n">partition_id</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_partitions</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;local=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_local</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_owned</span><span class="si">}</span><span class="s2">+</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_halo</span><span class="si">}</span><span class="s2">), &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;global=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">global_shape</span><span class="si">}</span><span class="s2">, nnz=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="si">}</span><span class="s2">, device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Persistence (I/O)</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;os.PathLike&quot;</span><span class="p">],</span>
        <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">world_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseMatrix&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a partition from disk for the given rank.</span>
<span class="sd">        </span>
<span class="sd">        Each rank should call this with its own rank to load only its partition.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        directory : str or PathLike</span>
<span class="sd">            Directory containing partitioned data.</span>
<span class="sd">        rank : int</span>
<span class="sd">            Rank of this process.</span>
<span class="sd">        world_size : int, optional</span>
<span class="sd">            Total number of processes (must match num_partitions).</span>
<span class="sd">        device : str or torch.device</span>
<span class="sd">            Device to load tensors to.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseMatrix</span>
<span class="sd">            The partition for this rank.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; rank = dist.get_rank()</span>
<span class="sd">        &gt;&gt;&gt; world_size = dist.get_world_size()</span>
<span class="sd">        &gt;&gt;&gt; partition = DSparseMatrix.load(&quot;matrix_dist&quot;, rank, world_size, &quot;cuda&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_partition</span>
        <span class="k">return</span> <span class="n">load_partition</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">create_distributed_matrices</span><span class="p">(</span>
    <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">row</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">col</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">DSparseMatrix</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create all distributed matrix partitions for local simulation.</span>
<span class="sd">    </span>
<span class="sd">    .. deprecated::</span>
<span class="sd">        Use DSparseTensor instead for a more Pythonic interface.</span>
<span class="sd">    </span>
<span class="sd">    Useful for testing/debugging without actual distributed setup.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    values, row, col : torch.Tensor</span>
<span class="sd">        Global COO sparse matrix data</span>
<span class="sd">    shape : Tuple[int, int]</span>
<span class="sd">        Global matrix shape</span>
<span class="sd">    num_partitions : int</span>
<span class="sd">        Number of partitions</span>
<span class="sd">    coords : torch.Tensor, optional</span>
<span class="sd">        Node coordinates for geometric partitioning</span>
<span class="sd">    device : str or torch.device</span>
<span class="sd">        Device for all partitions (&#39;cpu&#39;, &#39;cuda&#39;, &#39;cuda:0&#39;, etc.)</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    List[DSparseMatrix]</span>
<span class="sd">        List of DSparseMatrix, one per partition</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;create_distributed_matrices is deprecated. Use DSparseTensor instead.&quot;</span><span class="p">,</span>
        <span class="ne">DeprecationWarning</span><span class="p">,</span>
        <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>
    
    <span class="n">matrices</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Compute partition IDs once</span>
    <span class="k">if</span> <span class="n">coords</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">partition_coordinates</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">partition_graph_metis</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_partitions</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">):</span>
        <span class="n">mat</span> <span class="o">=</span> <span class="n">DSparseMatrix</span><span class="o">.</span><span class="n">from_global</span><span class="p">(</span>
            <span class="n">values</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span>
            <span class="n">partition_ids</span><span class="o">=</span><span class="n">partition_ids</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">matrices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span>
    
    <span class="c1"># Store reference to all partitions for local halo exchange</span>
    <span class="k">for</span> <span class="n">mat</span> <span class="ow">in</span> <span class="n">matrices</span><span class="p">:</span>
        <span class="n">mat</span><span class="o">.</span><span class="n">_all_partitions</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">partition</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">matrices</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">matrices</span>


<div class="viewcode-block" id="DSparseTensor">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DSparseTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Distributed Sparse Tensor with automatic partitioning and halo exchange.</span>
<span class="sd">    </span>
<span class="sd">    A Pythonic wrapper that provides a unified interface for distributed</span>
<span class="sd">    sparse matrix operations. Supports indexing to access individual partitions.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    values : torch.Tensor</span>
<span class="sd">        Non-zero values [nnz]</span>
<span class="sd">    row_indices : torch.Tensor</span>
<span class="sd">        Row indices [nnz]</span>
<span class="sd">    col_indices : torch.Tensor</span>
<span class="sd">        Column indices [nnz]</span>
<span class="sd">    shape : Tuple[int, int]</span>
<span class="sd">        Matrix shape (m, n)</span>
<span class="sd">    num_partitions : int</span>
<span class="sd">        Number of partitions to create</span>
<span class="sd">    coords : torch.Tensor, optional</span>
<span class="sd">        Node coordinates for geometric partitioning [num_nodes, dim]</span>
<span class="sd">    partition_method : str</span>
<span class="sd">        Partitioning method: &#39;metis&#39;, &#39;rcb&#39;, &#39;slicing&#39;, &#39;simple&#39;</span>
<span class="sd">    device : str or torch.device</span>
<span class="sd">        Device for the matrix data</span>
<span class="sd">    verbose : bool</span>
<span class="sd">        Whether to print partition info</span>
<span class="sd">    </span>
<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from torch_sla import DSparseTensor</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Create distributed tensor with 4 partitions</span>
<span class="sd">    &gt;&gt;&gt; A = DSparseTensor(val, row, col, shape, num_partitions=4)</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Access individual partitions</span>
<span class="sd">    &gt;&gt;&gt; A0 = A[0]  # First partition</span>
<span class="sd">    &gt;&gt;&gt; A1 = A[1]  # Second partition</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Iterate over partitions</span>
<span class="sd">    &gt;&gt;&gt; for partition in A:</span>
<span class="sd">    &gt;&gt;&gt;     x = partition.solve(b_local)</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Properties</span>
<span class="sd">    &gt;&gt;&gt; print(A.num_partitions)  # 4</span>
<span class="sd">    &gt;&gt;&gt; print(A.shape)           # Global shape</span>
<span class="sd">    &gt;&gt;&gt; print(len(A))            # 4</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Move to CUDA</span>
<span class="sd">    &gt;&gt;&gt; A_cuda = A.cuda()</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Local halo exchange (for testing)</span>
<span class="sd">    &gt;&gt;&gt; x_list = [torch.zeros(A[i].num_local) for i in range(4)]</span>
<span class="sd">    &gt;&gt;&gt; A.halo_exchange_local(x_list)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">row_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">col_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="n">values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_row_indices</span> <span class="o">=</span> <span class="n">row_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_col_indices</span> <span class="o">=</span> <span class="n">col_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span> <span class="o">=</span> <span class="n">num_partitions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_coords</span> <span class="o">=</span> <span class="n">coords</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partition_method</span> <span class="o">=</span> <span class="n">partition_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        
        <span class="c1"># Infer device from input tensor if not explicitly specified</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">device</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
        
        <span class="c1"># Compute partition IDs</span>
        <span class="c1"># NOTE: In distributed mode, this should be computed on rank 0 and broadcast</span>
        <span class="c1"># to ensure consistency. See _compute_partitions_distributed() for distributed-safe version.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partition_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_partitions</span><span class="p">(</span><span class="n">partition_method</span><span class="p">,</span> <span class="n">coords</span><span class="p">)</span>
        
        <span class="c1"># Create all partitions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">DSparseMatrix</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_create_partitions</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_partitions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute partition assignments for each node.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">coords</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;rcb&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;metis&#39;</span>
        
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;metis&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">partition_graph_metis</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_col_indices</span><span class="p">,</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;rcb&#39;</span><span class="p">,</span> <span class="s1">&#39;slicing&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">coords</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Partition method &#39;</span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&#39; requires coords&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">partition_coordinates</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;simple&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">partition_simple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown partition method: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_create_partitions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create all partition matrices.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">):</span>
            <span class="n">mat</span> <span class="o">=</span> <span class="n">DSparseMatrix</span><span class="o">.</span><span class="n">from_global</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_col_indices</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span>
                <span class="n">partition_ids</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partition_ids</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span>
        
        <span class="c1"># Store reference to all partitions for local halo exchange</span>
        <span class="k">for</span> <span class="n">mat</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">:</span>
            <span class="n">mat</span><span class="o">.</span><span class="n">_all_partitions</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">partition</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">]</span>
    
<div class="viewcode-block" id="DSparseTensor.from_sparse_tensor">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.from_sparse_tensor">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_sparse_tensor</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">sparse_tensor</span><span class="p">:</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">,</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create DSparseTensor from a SparseTensor.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        sparse_tensor : SparseTensor</span>
<span class="sd">            Input sparse tensor (must be 2D, not batched)</span>
<span class="sd">        num_partitions : int</span>
<span class="sd">            Number of partitions</span>
<span class="sd">        coords : torch.Tensor, optional</span>
<span class="sd">            Node coordinates for geometric partitioning</span>
<span class="sd">        partition_method : str</span>
<span class="sd">            Partitioning method</span>
<span class="sd">        device : str or torch.device, optional</span>
<span class="sd">            Target device (defaults to sparse_tensor&#39;s device)</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Whether to print partition info</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseTensor</span>
<span class="sd">            Distributed sparse tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Avoid circular import</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.sparse_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">SparseTensor</span>
        
        <span class="k">if</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;DSparseTensor does not support batched SparseTensor. &quot;</span>
                           <span class="s2">&quot;Use a 2D SparseTensor.&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">device</span>
        
        <span class="c1"># Use sparse_shape for the matrix dimensions</span>
        <span class="n">sparse_shape</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span>
            <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
            <span class="n">sparse_shape</span><span class="p">,</span>
            <span class="n">num_partitions</span><span class="o">=</span><span class="n">num_partitions</span><span class="p">,</span>
            <span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">,</span>
            <span class="n">partition_method</span><span class="o">=</span><span class="n">partition_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.from_torch_sparse">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.from_torch_sparse">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_torch_sparse</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create DSparseTensor from PyTorch sparse tensor.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">A</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">:</span>
            <span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">to_sparse_coo</span><span class="p">()</span>
        
        <span class="n">indices</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
            <span class="n">num_partitions</span><span class="o">=</span><span class="n">num_partitions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.from_global_distributed">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.from_global_distributed">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_global_distributed</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">row_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">col_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseMatrix&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create local partition in a distributed-safe manner.</span>
<span class="sd">        </span>
<span class="sd">        This method ensures that all ranks compute the same partition assignment</span>
<span class="sd">        by having rank 0 compute the partition IDs and broadcasting to all ranks.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        values : torch.Tensor</span>
<span class="sd">            Global non-zero values [nnz]</span>
<span class="sd">        row_indices : torch.Tensor</span>
<span class="sd">            Global row indices [nnz]</span>
<span class="sd">        col_indices : torch.Tensor</span>
<span class="sd">            Global column indices [nnz]</span>
<span class="sd">        shape : Tuple[int, int]</span>
<span class="sd">            Global matrix shape (M, N)</span>
<span class="sd">        rank : int</span>
<span class="sd">            Current process rank</span>
<span class="sd">        world_size : int</span>
<span class="sd">            Total number of processes</span>
<span class="sd">        coords : torch.Tensor, optional</span>
<span class="sd">            Node coordinates for geometric partitioning [num_nodes, dim]</span>
<span class="sd">        partition_method : str</span>
<span class="sd">            Partitioning method: &#39;metis&#39;, &#39;rcb&#39;, &#39;slicing&#39;, &#39;simple&#39;</span>
<span class="sd">        device : str or torch.device, optional</span>
<span class="sd">            Target device</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Whether to print partition info</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseMatrix</span>
<span class="sd">            Local partition matrix for this rank</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; </span>
<span class="sd">        &gt;&gt;&gt; # In each process:</span>
<span class="sd">        &gt;&gt;&gt; rank = dist.get_rank()</span>
<span class="sd">        &gt;&gt;&gt; world_size = dist.get_world_size()</span>
<span class="sd">        &gt;&gt;&gt; </span>
<span class="sd">        &gt;&gt;&gt; local_matrix = DSparseTensor.from_global_distributed(</span>
<span class="sd">        ...     val, row, col, shape, </span>
<span class="sd">        ...     rank=rank, world_size=world_size</span>
<span class="sd">        ... )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
        
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">device</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Compute partition IDs on rank 0 and broadcast</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Create temporary DSparseTensor to compute partitions</span>
            <span class="c1"># Use &#39;simple&#39; method if METIS might be non-deterministic</span>
            <span class="k">if</span> <span class="n">partition_method</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">coords</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">actual_method</span> <span class="o">=</span> <span class="s1">&#39;rcb&#39;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Use simple partitioning by default in distributed mode</span>
                    <span class="c1"># to ensure determinism across ranks</span>
                    <span class="n">actual_method</span> <span class="o">=</span> <span class="s1">&#39;simple&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">actual_method</span> <span class="o">=</span> <span class="n">partition_method</span>
            
            <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">actual_method</span> <span class="o">==</span> <span class="s1">&#39;simple&#39;</span><span class="p">:</span>
                <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">partition_simple</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">actual_method</span> <span class="o">==</span> <span class="s1">&#39;metis&#39;</span><span class="p">:</span>
                <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">partition_graph_metis</span><span class="p">(</span>
                    <span class="n">row_indices</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">world_size</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">actual_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;rcb&#39;</span><span class="p">,</span> <span class="s1">&#39;slicing&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">coords</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Method &#39;</span><span class="si">{</span><span class="n">actual_method</span><span class="si">}</span><span class="s2">&#39; requires coords&quot;</span><span class="p">)</span>
                <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">partition_coordinates</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">actual_method</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown method: </span><span class="si">{</span><span class="n">actual_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">partition_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Create empty tensor to receive broadcast</span>
            <span class="n">partition_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Broadcast partition IDs from rank 0 to all ranks</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">partition_ids</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Now create local partition using the consistent partition IDs</span>
        <span class="n">local_matrix</span> <span class="o">=</span> <span class="n">DSparseMatrix</span><span class="o">.</span><span class="n">from_global</span><span class="p">(</span>
            <span class="n">values</span><span class="p">,</span> <span class="n">row_indices</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span>
            <span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span>
            <span class="n">partition_ids</span><span class="o">=</span><span class="n">partition_ids</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span> <span class="ow">and</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># Only print on rank 0</span>
        <span class="p">)</span>
        
        <span class="k">return</span> <span class="n">local_matrix</span></div>

    
<div class="viewcode-block" id="DSparseTensor.from_device_mesh">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.from_device_mesh">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_device_mesh</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">row_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">col_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">device_mesh</span><span class="p">:</span> <span class="s2">&quot;DeviceMesh&quot;</span><span class="p">,</span>
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;simple&#39;</span><span class="p">,</span>
        <span class="n">placement</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;shard_rows&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseMatrix&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create local partition using PyTorch DeviceMesh.</span>
<span class="sd">        </span>
<span class="sd">        This is the recommended method for distributed training with PyTorch&#39;s</span>
<span class="sd">        DTensor ecosystem. Each rank receives only its local partition.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        values : torch.Tensor</span>
<span class="sd">            Global non-zero values [nnz] (same on all ranks)</span>
<span class="sd">        row_indices : torch.Tensor</span>
<span class="sd">            Global row indices [nnz]</span>
<span class="sd">        col_indices : torch.Tensor</span>
<span class="sd">            Global column indices [nnz]</span>
<span class="sd">        shape : Tuple[int, int]</span>
<span class="sd">            Global matrix shape (M, N)</span>
<span class="sd">        device_mesh : DeviceMesh</span>
<span class="sd">            PyTorch DeviceMesh specifying device topology</span>
<span class="sd">        coords : torch.Tensor, optional</span>
<span class="sd">            Node coordinates for geometric partitioning</span>
<span class="sd">        partition_method : str</span>
<span class="sd">            Partitioning method: &#39;metis&#39;, &#39;rcb&#39;, &#39;simple&#39;</span>
<span class="sd">            Default is &#39;simple&#39; for determinism in distributed setting</span>
<span class="sd">        placement : str</span>
<span class="sd">            How to distribute: &#39;shard_rows&#39;, &#39;shard_cols&#39;, &#39;replicate&#39;</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Whether to print partition info</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseMatrix</span>
<span class="sd">            Local partition for this rank</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.device_mesh import init_device_mesh</span>
<span class="sd">        &gt;&gt;&gt; from torch_sla import DSparseTensor</span>
<span class="sd">        &gt;&gt;&gt; </span>
<span class="sd">        &gt;&gt;&gt; # Initialize 4-GPU device mesh</span>
<span class="sd">        &gt;&gt;&gt; mesh = init_device_mesh(&quot;cuda&quot;, (4,), mesh_dim_names=(&quot;dp&quot;,))</span>
<span class="sd">        &gt;&gt;&gt; </span>
<span class="sd">        &gt;&gt;&gt; # Create distributed sparse tensor (each rank gets its partition)</span>
<span class="sd">        &gt;&gt;&gt; local_matrix = DSparseTensor.from_device_mesh(</span>
<span class="sd">        ...     val, row, col, shape,</span>
<span class="sd">        ...     device_mesh=mesh,</span>
<span class="sd">        ...     partition_method=&#39;simple&#39;</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; </span>
<span class="sd">        &gt;&gt;&gt; # Local operations</span>
<span class="sd">        &gt;&gt;&gt; y_local = local_matrix.matvec(x_local)</span>
<span class="sd">        &gt;&gt;&gt; x_local = local_matrix.solve(b_local)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.device_mesh</span><span class="w"> </span><span class="kn">import</span> <span class="n">DeviceMesh</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;DeviceMesh requires PyTorch 2.0+. &quot;</span>
                            <span class="s2">&quot;Use from_global_distributed() instead.&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">DIST_AVAILABLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;torch.distributed must be initialized. &quot;</span>
                             <span class="s2">&quot;Call dist.init_process_group() first.&quot;</span><span class="p">)</span>
        
        <span class="c1"># Get rank info from device mesh</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">device_mesh</span><span class="o">.</span><span class="n">get_local_rank</span><span class="p">()</span>
        <span class="n">world_size</span> <span class="o">=</span> <span class="n">device_mesh</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">device_type</span> <span class="o">=</span> <span class="n">device_mesh</span><span class="o">.</span><span class="n">device_type</span>
        
        <span class="c1"># Determine target device</span>
        <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span>
        
        <span class="c1"># Use the distributed-safe factory method</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">from_global_distributed</span><span class="p">(</span>
            <span class="n">values</span><span class="p">,</span> <span class="n">row_indices</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
            <span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">,</span>
            <span class="n">partition_method</span><span class="o">=</span><span class="n">partition_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Properties</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Global matrix shape.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_partitions</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of partitions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Device of the matrix data.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Data type of matrix values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">dtype</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">nnz</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Total number of non-zeros.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">partition_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Partition assignment for each node.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_ids</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if matrix is on CUDA.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Indexing and Iteration</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of partitions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DSparseMatrix</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get a specific partition.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span> <span class="o">+</span> <span class="n">idx</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Partition index </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> out of range [0, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Iterate over partitions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">)</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Device Management</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="DSparseTensor.to">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.to">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Move all partitions to a different device.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        device : str or torch.device</span>
<span class="sd">            Target device</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseTensor</span>
<span class="sd">            New distributed tensor on target device</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">DSparseTensor</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">DSparseTensor</span><span class="p">)</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_row_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_row_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_col_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_col_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_num_partitions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_coords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_coords</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_partition_method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_method</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_verbose</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Don&#39;t print again</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_partition_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_ids</span>
        
        <span class="c1"># Move partitions</span>
        <span class="n">new_tensor</span><span class="o">.</span><span class="n">_partitions</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">]</span>
        
        <span class="c1"># Update references</span>
        <span class="k">for</span> <span class="n">mat</span> <span class="ow">in</span> <span class="n">new_tensor</span><span class="o">.</span><span class="n">_partitions</span><span class="p">:</span>
            <span class="n">mat</span><span class="o">.</span><span class="n">_all_partitions</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">partition</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">new_tensor</span><span class="o">.</span><span class="n">_partitions</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">new_tensor</span></div>

    
<div class="viewcode-block" id="DSparseTensor.cuda">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.cuda">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move to CUDA device.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.cpu">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.cpu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move to CPU.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Distributed Operations</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="DSparseTensor.halo_exchange_local">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.halo_exchange_local">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">halo_exchange_local</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Local halo exchange for single-process simulation.</span>
<span class="sd">        </span>
<span class="sd">        Exchanges halo values between all partitions locally.</span>
<span class="sd">        Useful for testing without actual distributed setup.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x_list : List[torch.Tensor]</span>
<span class="sd">            List of local vectors, one per partition. Each vector is</span>
<span class="sd">            modified in-place to update halo values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_list</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="si">}</span><span class="s2"> vectors, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">x_list</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">part_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">):</span>
            <span class="n">partition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">[</span><span class="n">part_id</span><span class="p">]</span><span class="o">.</span><span class="n">partition</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_list</span><span class="p">[</span><span class="n">part_id</span><span class="p">]</span>
            
            <span class="n">halo_offset</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">halo_idx</span><span class="p">,</span> <span class="n">global_node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">partition</span><span class="o">.</span><span class="n">halo_nodes</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
                <span class="n">local_halo_idx</span> <span class="o">=</span> <span class="n">halo_offset</span> <span class="o">+</span> <span class="n">halo_idx</span>
                
                <span class="k">for</span> <span class="n">neighbor_id</span> <span class="ow">in</span> <span class="n">partition</span><span class="o">.</span><span class="n">neighbor_partitions</span><span class="p">:</span>
                    <span class="n">neighbor_partition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span><span class="o">.</span><span class="n">partition</span>
                    <span class="n">neighbor_g2l</span> <span class="o">=</span> <span class="n">neighbor_partition</span><span class="o">.</span><span class="n">global_to_local</span>
                    
                    <span class="k">if</span> <span class="n">global_node</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbor_g2l</span><span class="p">):</span>
                        <span class="n">local_idx_in_neighbor</span> <span class="o">=</span> <span class="n">neighbor_g2l</span><span class="p">[</span><span class="n">global_node</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">local_idx_in_neighbor</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">local_idx_in_neighbor</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbor_partition</span><span class="o">.</span><span class="n">owned_nodes</span><span class="p">):</span>
                            <span class="n">x</span><span class="p">[</span><span class="n">local_halo_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_list</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">][</span><span class="n">local_idx_in_neighbor</span><span class="p">]</span>
                            <span class="k">break</span></div>

    
<div class="viewcode-block" id="DSparseTensor.matvec_all">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.matvec_all">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">matvec_all</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">exchange_halo</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Matrix-vector multiply on all partitions.</span>
<span class="sd">        </span>
<span class="sd">        Performs y = A @ x for each partition, with optional halo exchange.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x_list : List[torch.Tensor]</span>
<span class="sd">            List of local vectors, one per partition. Each vector should have</span>
<span class="sd">            size = num_owned + num_halo for that partition.</span>
<span class="sd">        exchange_halo : bool</span>
<span class="sd">            Whether to perform halo exchange before multiplication.</span>
<span class="sd">            Default True.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of result vectors, one per partition. Each result has</span>
<span class="sd">            size = num_owned (only owned nodes have valid results).</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; D = SparseTensor(val, row, col, shape).partition(4)</span>
<span class="sd">        &gt;&gt;&gt; x_local = D.scatter_local(x_global)</span>
<span class="sd">        &gt;&gt;&gt; y_local = D.matvec_all(x_local)</span>
<span class="sd">        &gt;&gt;&gt; y_global = D.gather_global(y_local)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">matvec</span><span class="p">(</span><span class="n">x_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">exchange_halo</span><span class="o">=</span><span class="n">exchange_halo</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">)]</span></div>

    
<div class="viewcode-block" id="DSparseTensor.solve_all">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.solve_all">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">solve_all</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">b_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve on all partitions (subdomain solves).</span>
<span class="sd">        </span>
<span class="sd">        NOTE: This performs LOCAL subdomain solves, NOT a global distributed solve.</span>
<span class="sd">        Each partition solves its own local system independently.</span>
<span class="sd">        For a true distributed solve, use `solve_distributed()`.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b_list : List[torch.Tensor]</span>
<span class="sd">            List of local RHS vectors, one per partition</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Additional arguments passed to each partition&#39;s solve method</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of solution vectors, one per partition</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">b_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> 
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">)]</span></div>

    
<div class="viewcode-block" id="DSparseTensor.solve_distributed">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.solve_distributed">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">solve_distributed</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">b_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cg&#39;</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Distributed solve: find x such that A @ x = b using all partitions.</span>
<span class="sd">        </span>
<span class="sd">        This performs a TRUE distributed solve where all partitions collaborate</span>
<span class="sd">        to solve the global system. Uses distributed CG with global reductions.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b_global : torch.Tensor</span>
<span class="sd">            Global RHS vector [N]</span>
<span class="sd">        method : str</span>
<span class="sd">            Solver method: &#39;cg&#39; (Conjugate Gradient)</span>
<span class="sd">        atol : float</span>
<span class="sd">            Absolute tolerance for convergence</span>
<span class="sd">        maxiter : int</span>
<span class="sd">            Maximum iterations</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Print convergence info</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Global solution vector [N]</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; D = A.partition(num_partitions=4)</span>
<span class="sd">        &gt;&gt;&gt; x = D.solve_distributed(b)  # Distributed CG solve</span>
<span class="sd">        &gt;&gt;&gt; residual = torch.norm(A @ x - b)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">b_global</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        
        <span class="c1"># Initialize x = 0</span>
        <span class="n">x_global</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Scatter b to local</span>
        <span class="n">b_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scatter_local</span><span class="p">(</span><span class="n">b_global</span><span class="p">)</span>
        
        <span class="c1"># Distributed CG</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;cg&#39;</span><span class="p">:</span>
            <span class="n">x_global</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_cg</span><span class="p">(</span><span class="n">x_global</span><span class="p">,</span> <span class="n">b_global</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown method: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">. Supported: &#39;cg&#39;&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_global</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="nf">_distributed_cg</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Distributed Conjugate Gradient.</span>
<span class="sd">        </span>
<span class="sd">        All partitions work together, with global reductions for inner products.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        
        <span class="c1"># r = b - A @ x</span>
        <span class="n">Ax</span> <span class="o">=</span> <span class="bp">self</span> <span class="o">@</span> <span class="n">x</span>  <span class="c1"># Uses __matmul__ which does scatter -&gt; matvec_all -&gt; gather</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">Ax</span>
        
        <span class="c1"># p = r</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        
        <span class="c1"># rs_old = r^T @ r (global)</span>
        <span class="n">rs_old</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
            <span class="c1"># Ap = A @ p</span>
            <span class="n">Ap</span> <span class="o">=</span> <span class="bp">self</span> <span class="o">@</span> <span class="n">p</span>
            
            <span class="c1"># pAp = p^T @ A @ p (global)</span>
            <span class="n">pAp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Ap</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">pAp</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-30</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Distributed CG: pAp too small at iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
            
            <span class="c1"># alpha = rs_old / pAp</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">rs_old</span> <span class="o">/</span> <span class="n">pAp</span>
            
            <span class="c1"># x = x + alpha * p</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">p</span>
            
            <span class="c1"># r = r - alpha * Ap</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">Ap</span>
            
            <span class="c1"># rs_new = r^T @ r (global)</span>
            <span class="n">rs_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
            
            <span class="n">residual</span> <span class="o">=</span> <span class="n">rs_new</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Distributed CG iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: residual = </span><span class="si">{</span><span class="n">residual</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">residual</span> <span class="o">&lt;</span> <span class="n">atol</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Distributed CG converged at iter </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, residual = </span><span class="si">{</span><span class="n">residual</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
            
            <span class="k">if</span> <span class="n">rs_old</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-30</span><span class="p">:</span>
                <span class="k">break</span>
            
            <span class="c1"># beta = rs_new / rs_old</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">rs_new</span> <span class="o">/</span> <span class="n">rs_old</span>
            
            <span class="c1"># p = r + beta * p</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">p</span>
            
            <span class="n">rs_old</span> <span class="o">=</span> <span class="n">rs_new</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
<div class="viewcode-block" id="DSparseTensor.gather_global">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.gather_global">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">gather_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gather local vectors to global vector.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x_list : List[torch.Tensor]</span>
<span class="sd">            List of local vectors, one per partition</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Global vector</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x_global</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">):</span>
            <span class="n">partition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">partition</span>
            <span class="n">owned_nodes</span> <span class="o">=</span> <span class="n">partition</span><span class="o">.</span><span class="n">owned_nodes</span>
            <span class="n">num_owned</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">owned_nodes</span><span class="p">)</span>
            <span class="n">x_global</span><span class="p">[</span><span class="n">owned_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_list</span><span class="p">[</span><span class="n">i</span><span class="p">][:</span><span class="n">num_owned</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_global</span></div>

    
<div class="viewcode-block" id="DSparseTensor.scatter_local">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.scatter_local">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">scatter_local</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_global</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Scatter global vector to local vectors.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x_global : torch.Tensor</span>
<span class="sd">            Global vector</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of local vectors (with halo values filled)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x_list</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">):</span>
            <span class="n">partition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">partition</span>
            <span class="n">local_nodes</span> <span class="o">=</span> <span class="n">partition</span><span class="o">.</span><span class="n">local_nodes</span>
            <span class="n">x_local</span> <span class="o">=</span> <span class="n">x_global</span><span class="p">[</span><span class="n">local_nodes</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_local</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x_list</span></div>

    
<div class="viewcode-block" id="DSparseTensor.to_sparse_tensor">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.to_sparse_tensor">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_sparse_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gather all partitions into a single SparseTensor.</span>
<span class="sd">        </span>
<span class="sd">        This creates a global SparseTensor from the distributed data.</span>
<span class="sd">        Useful for verification, debugging, or when you need to perform</span>
<span class="sd">        operations that require the full matrix.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            Global sparse tensor containing all data</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; D = DSparseTensor(val, row, col, shape, num_partitions=4)</span>
<span class="sd">        &gt;&gt;&gt; A = D.to_sparse_tensor()  # Gather to global SparseTensor</span>
<span class="sd">        &gt;&gt;&gt; x = A.solve(b)  # Solve on the full matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.sparse_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">SparseTensor</span>
        
        <span class="c1"># Return the original global data as SparseTensor</span>
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_row_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_col_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>
        <span class="p">)</span></div>

    
    <span class="c1"># Alias for convenience</span>
    <span class="n">gather</span> <span class="o">=</span> <span class="n">to_sparse_tensor</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Distributed Algorithms (True Distributed, No Gather)</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_global_matvec_with_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Global matrix-vector multiplication that preserves gradients.</span>
<span class="sd">        </span>
<span class="sd">        Uses the original COO data to maintain gradient flow.</span>
<span class="sd">        For true distributed MPI execution, use _distributed_matvec instead.</span>
<span class="sd">        </span>
<span class="sd">        This method is used for gradient-enabled operations like eigsh, solve.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Use original global COO data for gradient support</span>
        <span class="c1"># y[i] = sum_j A[i,j] * x[j]</span>
        <span class="c1"># y = scatter_add(values * x[col], row)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_row_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_col_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># y[row] += values * x[col]</span>
        <span class="n">contributions</span> <span class="o">=</span> <span class="n">vals</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">scatter_add</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">contributions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_distributed_matvec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Distributed matrix-vector multiplication with gradient support.</span>
<span class="sd">        </span>
<span class="sd">        For single-node simulation with gradient support, uses _global_matvec_with_grad.</span>
<span class="sd">        For true distributed MPI execution, uses scatter -&gt; local matvec -&gt; gather.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check if we need gradients</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="c1"># Use global matvec that preserves gradients</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_matvec_with_grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Otherwise use true distributed pattern</span>
        <span class="n">x_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scatter_local</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matvec_all</span><span class="p">(</span><span class="n">x_local</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_global</span><span class="p">(</span><span class="n">y_local</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_distributed_lobpcg</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">largest</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Distributed LOBPCG eigenvalue solver.</span>
<span class="sd">        </span>
<span class="sd">        Uses distributed matvec with global QR and Rayleigh-Ritz.</span>
<span class="sd">        No data gather required - only needs global reductions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        
        <span class="c1"># Initialize random subspace (global vectors)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">k</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Global QR decomposition</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="n">eigenvalues_prev</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
            <span class="c1"># Distributed matvec: AX = D @ X (column by column or batched)</span>
            <span class="n">AX</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">AX</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_matvec</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
            
            <span class="c1"># Rayleigh-Ritz: project onto subspace</span>
            <span class="c1"># H = X^T @ AX (global reduction)</span>
            <span class="n">H</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">AX</span>
            
            <span class="c1"># Solve small eigenvalue problem</span>
            <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
            
            <span class="c1"># Sort eigenvalues</span>
            <span class="k">if</span> <span class="n">largest</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
            
            <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
            
            <span class="c1"># Update X = X @ V</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">eigenvectors</span>
            
            <span class="c1"># Check convergence</span>
            <span class="k">if</span> <span class="n">eigenvalues_prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">eigenvalues_prev</span><span class="p">[:</span><span class="n">k</span><span class="p">])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">diff</span> <span class="o">&lt;</span> <span class="n">tol</span> <span class="o">*</span> <span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                    <span class="k">break</span>
            <span class="n">eigenvalues_prev</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            
            <span class="c1"># Expand subspace with residual</span>
            <span class="k">if</span> <span class="n">iteration</span> <span class="o">&lt;</span> <span class="n">maxiter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Compute residual: R = AX - X @ diag(eigenvalues)</span>
                <span class="n">AX_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">AX_new</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_matvec</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
                
                <span class="n">residual</span> <span class="o">=</span> <span class="n">AX_new</span> <span class="o">-</span> <span class="n">X</span> <span class="o">*</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                
                <span class="c1"># Orthogonalize and expand</span>
                <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">],</span> <span class="n">residual</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
                
                <span class="c1"># Pad if needed</span>
                <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">:</span>
                    <span class="n">extra</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">m</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">extra</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
    
<div class="viewcode-block" id="DSparseTensor.eigsh">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.eigsh">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eigsh</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">which</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;LM&quot;</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_eigenvectors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute k eigenvalues for symmetric matrices using distributed LOBPCG.</span>
<span class="sd">        </span>
<span class="sd">        This is a TRUE distributed algorithm - no data gather required.</span>
<span class="sd">        Uses distributed matvec with global QR decomposition.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int, optional</span>
<span class="sd">            Number of eigenvalues to compute. Default: 6.</span>
<span class="sd">        which : {&quot;LM&quot;, &quot;SM&quot;, &quot;LA&quot;, &quot;SA&quot;}, optional</span>
<span class="sd">            Which eigenvalues to find:</span>
<span class="sd">            - &quot;LM&quot;/&quot;LA&quot;: Largest (default)</span>
<span class="sd">            - &quot;SM&quot;/&quot;SA&quot;: Smallest</span>
<span class="sd">        sigma : float, optional</span>
<span class="sd">            Find eigenvalues near sigma (not yet supported).</span>
<span class="sd">        return_eigenvectors : bool, optional</span>
<span class="sd">            Whether to return eigenvectors. Default: True.</span>
<span class="sd">        maxiter : int, optional</span>
<span class="sd">            Maximum LOBPCG iterations. Default: 1000.</span>
<span class="sd">        tol : float, optional</span>
<span class="sd">            Convergence tolerance. Default: 1e-8.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        eigenvalues : torch.Tensor</span>
<span class="sd">            Shape [k].</span>
<span class="sd">        eigenvectors : torch.Tensor or None</span>
<span class="sd">            Shape [N, k] if return_eigenvectors is True.</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        **Distributed Algorithm:**</span>
<span class="sd">        </span>
<span class="sd">        - Uses distributed LOBPCG (Locally Optimal Block PCG)</span>
<span class="sd">        - Only requires distributed matvec + global reductions</span>
<span class="sd">        - Memory: O(N * k) per node for eigenvectors</span>
<span class="sd">        - Communication: O(k^2) per iteration for Rayleigh-Ritz</span>
<span class="sd">        </span>
<span class="sd">        **Gradient Support:**</span>
<span class="sd">        </span>
<span class="sd">        - Gradients flow through the distributed matvec operations</span>
<span class="sd">        - O(iterations) graph nodes (not O(1) like adjoint)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sigma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;sigma (shift-invert) not yet supported for distributed eigsh. Ignoring.&quot;</span><span class="p">)</span>
        
        <span class="n">largest</span> <span class="o">=</span> <span class="n">which</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;LM&#39;</span><span class="p">,</span> <span class="s1">&#39;LA&#39;</span><span class="p">)</span>
        <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_lobpcg</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="n">largest</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_eigenvectors</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span>
        <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="kc">None</span></div>

    
<div class="viewcode-block" id="DSparseTensor.eigs">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.eigs">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eigs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">which</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;LM&quot;</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_eigenvectors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute k eigenvalues using distributed LOBPCG.</span>
<span class="sd">        </span>
<span class="sd">        For symmetric matrices, equivalent to eigsh().</span>
<span class="sd">        For non-symmetric, currently falls back to eigsh() (symmetric assumption).</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int, optional</span>
<span class="sd">            Number of eigenvalues to compute. Default: 6.</span>
<span class="sd">        which : str, optional</span>
<span class="sd">            Which eigenvalues to find.</span>
<span class="sd">        sigma : float, optional</span>
<span class="sd">            Find eigenvalues near sigma.</span>
<span class="sd">        return_eigenvectors : bool, optional</span>
<span class="sd">            Whether to return eigenvectors. Default: True.</span>
<span class="sd">        maxiter : int, optional</span>
<span class="sd">            Maximum iterations. Default: 1000.</span>
<span class="sd">        tol : float, optional</span>
<span class="sd">            Convergence tolerance. Default: 1e-8.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        eigenvalues : torch.Tensor</span>
<span class="sd">            Shape [k].</span>
<span class="sd">        eigenvectors : torch.Tensor or None</span>
<span class="sd">            Shape [N, k] if return_eigenvectors is True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># For now, use eigsh (assumes symmetric)</span>
        <span class="c1"># TODO: Implement Arnoldi for non-symmetric</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigsh</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="n">which</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> 
                         <span class="n">return_eigenvectors</span><span class="o">=</span><span class="n">return_eigenvectors</span><span class="p">,</span>
                         <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.svd">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.svd">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">svd</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute truncated SVD using distributed power iteration.</span>
<span class="sd">        </span>
<span class="sd">        Uses A^T @ A for eigenvalues, then recovers U from A @ V.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int, optional</span>
<span class="sd">            Number of singular values to compute. Default: 6.</span>
<span class="sd">        maxiter : int, optional</span>
<span class="sd">            Maximum iterations. Default: 1000.</span>
<span class="sd">        tol : float, optional</span>
<span class="sd">            Convergence tolerance. Default: 1e-8.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        U : torch.Tensor</span>
<span class="sd">            Left singular vectors. Shape [M, k].</span>
<span class="sd">        S : torch.Tensor</span>
<span class="sd">            Singular values. Shape [k].</span>
<span class="sd">        Vt : torch.Tensor</span>
<span class="sd">            Right singular vectors. Shape [k, N].</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        **Distributed Algorithm:**</span>
<span class="sd">        </span>
<span class="sd">        - Computes eigenvalues of A^T @ A using distributed LOBPCG</span>
<span class="sd">        - No data gather required</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        
        <span class="c1"># For SVD, we need A^T @ A which requires transpose</span>
        <span class="c1"># Create A^T as a DSparseTensor</span>
        <span class="n">A_T</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">()</span>
        
        <span class="c1"># Power iteration for A^T @ A</span>
        <span class="c1"># Initialize random vectors</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">V</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
            <span class="c1"># AV = A @ V</span>
            <span class="n">AV</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">AV</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_matvec</span><span class="p">(</span><span class="n">V</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
            
            <span class="c1"># AtAV = A^T @ (A @ V)</span>
            <span class="n">AtAV</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">AtAV</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A_T</span><span class="o">.</span><span class="n">_distributed_matvec</span><span class="p">(</span><span class="n">AV</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
            
            <span class="c1"># QR decomposition</span>
            <span class="n">V_new</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">AtAV</span><span class="p">)</span>
            
            <span class="c1"># Check convergence</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">V_new</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
            <span class="n">V</span> <span class="o">=</span> <span class="n">V_new</span>
            
            <span class="k">if</span> <span class="n">diff</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
                <span class="k">break</span>
        
        <span class="c1"># Compute singular values and U</span>
        <span class="c1"># AV = A @ V, then normalize to get U</span>
        <span class="n">AV</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
            <span class="n">AV</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_matvec</span><span class="p">(</span><span class="n">V</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
        
        <span class="c1"># S = ||AV[:, j]||</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">AV</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># U = AV / S</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">AV</span> <span class="o">/</span> <span class="n">S</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span></div>

    
<div class="viewcode-block" id="DSparseTensor.norm">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.norm">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">ord</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;fro&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;fro&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute matrix norm (distributed).</span>
<span class="sd">        </span>
<span class="sd">        For Frobenius norm, computed locally and aggregated.</span>
<span class="sd">        For spectral norm, uses distributed SVD.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ord : {&#39;fro&#39;, 1, 2}</span>
<span class="sd">            Type of norm:</span>
<span class="sd">            - &#39;fro&#39;: Frobenius norm (distributed sum)</span>
<span class="sd">            - 1: Maximum column sum</span>
<span class="sd">            - 2: Spectral norm (largest singular value via distributed SVD)</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Scalar tensor containing the norm value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="s1">&#39;fro&#39;</span><span class="p">:</span>
            <span class="c1"># Frobenius norm: sqrt(sum(values^2))</span>
            <span class="c1"># This is truly distributed - each partition has its own values</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
        <span class="k">elif</span> <span class="nb">ord</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># Spectral norm: largest singular value</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">ord</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Maximum column sum - need to gather</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;1-norm requires data gather. Using to_sparse_tensor().&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse_tensor</span><span class="p">()</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown norm order: </span><span class="si">{</span><span class="nb">ord</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.condition_number">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.condition_number">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">condition_number</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">ord</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Estimate condition number using distributed SVD.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ord : int, optional</span>
<span class="sd">            Norm order. Default: 2 (spectral).</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Condition number estimate (_max / _min).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># Need largest and smallest singular values</span>
            <span class="c1"># Compute k=6 singular values</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">S</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ord=</span><span class="si">{</span><span class="nb">ord</span><span class="si">}</span><span class="s2"> requires data gather. Using to_sparse_tensor().&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse_tensor</span><span class="p">()</span><span class="o">.</span><span class="n">condition_number</span><span class="p">(</span><span class="nb">ord</span><span class="o">=</span><span class="nb">ord</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.T">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.T">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transpose the distributed sparse tensor.</span>
<span class="sd">        </span>
<span class="sd">        Returns a new DSparseTensor with swapped row/column indices.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseTensor</span>
<span class="sd">            Transposed matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Swap row and column indices</span>
        <span class="k">return</span> <span class="n">DSparseTensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_col_indices</span><span class="p">,</span>  <span class="c1"># swap</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_row_indices</span><span class="p">,</span>  <span class="c1"># swap</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">num_partitions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">,</span>
            <span class="n">coords</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_coords</span><span class="p">,</span>
            <span class="n">partition_method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partition_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Methods that require data gather (with warnings)</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="DSparseTensor.to_dense">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.to_dense">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_dense</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert to dense tensor.</span>
<span class="sd">        </span>
<span class="sd">        WARNING: This gathers all data to a single node.</span>
<span class="sd">        Only use for small matrices or debugging.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Dense matrix of shape (M, N).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;to_dense() gathers all data to a single node. &quot;</span>
                     <span class="s2">&quot;Only use for debugging or small matrices.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse_tensor</span><span class="p">()</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span></div>

    
<div class="viewcode-block" id="DSparseTensor.is_symmetric">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.is_symmetric">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_symmetric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if matrix is symmetric.</span>
<span class="sd">        </span>
<span class="sd">        Can be done distributedly by comparing values with transpose.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        atol : float</span>
<span class="sd">            Absolute tolerance for symmetry check.</span>
<span class="sd">        rtol : float</span>
<span class="sd">            Relative tolerance for symmetry check.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Boolean scalar tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># This can be done without gather by checking local values</span>
        <span class="c1"># For now, use simple implementation</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse_tensor</span><span class="p">()</span><span class="o">.</span><span class="n">is_symmetric</span><span class="p">(</span><span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.is_positive_definite">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.is_positive_definite">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_positive_definite</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if matrix is positive definite.</span>
<span class="sd">        </span>
<span class="sd">        Uses distributed eigenvalue computation.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Boolean scalar tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check smallest eigenvalue &gt; 0</span>
        <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigsh</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;SA&#39;</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span></div>

    
<div class="viewcode-block" id="DSparseTensor.lu">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.lu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">lu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute LU decomposition.</span>
<span class="sd">        </span>
<span class="sd">        WARNING: LU is inherently not distributed-friendly.</span>
<span class="sd">        This gathers data to a single node.</span>
<span class="sd">        </span>
<span class="sd">        For distributed solves, use solve_distributed() with iterative methods.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        LUFactorization</span>
<span class="sd">            Factorization object with solve() method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;LU decomposition is not distributed. &quot;</span>
                     <span class="s2">&quot;Use solve_distributed() for distributed solves.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse_tensor</span><span class="p">()</span><span class="o">.</span><span class="n">lu</span><span class="p">()</span></div>

    
<div class="viewcode-block" id="DSparseTensor.spy">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.spy">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">spy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Visualize sparsity pattern.</span>
<span class="sd">        </span>
<span class="sd">        Gathers data for visualization.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Arguments passed to SparseTensor.spy().</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse_tensor</span><span class="p">()</span><span class="o">.</span><span class="n">spy</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.nonlinear_solve">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.nonlinear_solve">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">nonlinear_solve</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">residual_fn</span><span class="p">,</span>
        <span class="n">u0</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">*</span><span class="n">params</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;newton&#39;</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">line_search</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve nonlinear equation F(u, D, *params) = 0 using distributed Newton-Krylov.</span>
<span class="sd">        </span>
<span class="sd">        Uses Jacobian-free Newton-Krylov with distributed CG for linear solves.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        residual_fn : callable</span>
<span class="sd">            Function F(u, D, *params) -&gt; residual tensor.</span>
<span class="sd">            D is this DSparseTensor.</span>
<span class="sd">        u0 : torch.Tensor</span>
<span class="sd">            Initial guess (global vector).</span>
<span class="sd">        *params : torch.Tensor</span>
<span class="sd">            Additional parameters.</span>
<span class="sd">        method : str</span>
<span class="sd">            &#39;newton&#39;: Newton-Krylov with distributed CG</span>
<span class="sd">            &#39;picard&#39;: Fixed-point iteration</span>
<span class="sd">        tol : float</span>
<span class="sd">            Relative tolerance.</span>
<span class="sd">        atol : float</span>
<span class="sd">            Absolute tolerance.</span>
<span class="sd">        max_iter : int</span>
<span class="sd">            Maximum outer iterations.</span>
<span class="sd">        line_search : bool</span>
<span class="sd">            Use Armijo line search.</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Print convergence info.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Solution u such that F(u, D, *params)  0.</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        **Distributed Algorithm:**</span>
<span class="sd">        </span>
<span class="sd">        - Uses Jacobian-free Newton-Krylov (JFNK)</span>
<span class="sd">        - Linear solves use distributed CG</span>
<span class="sd">        - Jacobian-vector products computed via finite differences</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u0</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">device</span>
        
        <span class="k">for</span> <span class="n">outer_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="c1"># Compute residual</span>
            <span class="n">F</span> <span class="o">=</span> <span class="n">residual_fn</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
            <span class="n">F_norm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
            
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Newton iter </span><span class="si">{</span><span class="n">outer_iter</span><span class="si">}</span><span class="s2">: ||F|| = </span><span class="si">{</span><span class="n">F_norm</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">F_norm</span> <span class="o">&lt;</span> <span class="n">atol</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Converged (atol) at iteration </span><span class="si">{</span><span class="n">outer_iter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
            
            <span class="k">if</span> <span class="n">outer_iter</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">F_norm</span> <span class="o">&lt;</span> <span class="n">tol</span> <span class="o">*</span> <span class="n">F_norm_init</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Converged (rtol) at iteration </span><span class="si">{</span><span class="n">outer_iter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
            
            <span class="k">if</span> <span class="n">outer_iter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">F_norm_init</span> <span class="o">=</span> <span class="n">F_norm</span>
            
            <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;picard&#39;</span><span class="p">:</span>
                <span class="c1"># Simple fixed-point: u = u - F (assuming F = Au - b form)</span>
                <span class="n">u</span> <span class="o">=</span> <span class="n">u</span> <span class="o">-</span> <span class="n">F</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Newton-Krylov: solve J @ du = -F using CG with Jacobian-vector products</span>
                <span class="c1"># J @ v  (F(u + eps*v) - F(u)) / eps</span>
                <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-7</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">norm</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
                
                <span class="k">def</span><span class="w"> </span><span class="nf">matvec</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;Jacobian-vector product via finite differences.&quot;&quot;&quot;</span>
                    <span class="n">F_plus</span> <span class="o">=</span> <span class="n">residual_fn</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
                    <span class="k">return</span> <span class="p">(</span><span class="n">F_plus</span> <span class="o">-</span> <span class="n">F</span><span class="p">)</span> <span class="o">/</span> <span class="n">eps</span>
                
                <span class="c1"># Distributed CG for J @ du = -F</span>
                <span class="n">du</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
                <span class="n">r</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span> <span class="o">-</span> <span class="n">matvec</span><span class="p">(</span><span class="n">du</span><span class="p">)</span>  <span class="c1"># r = -F - J @ 0 = -F</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">rs_old</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
                
                <span class="k">for</span> <span class="n">cg_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">N</span><span class="p">)):</span>
                    <span class="n">Jp</span> <span class="o">=</span> <span class="n">matvec</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
                    <span class="n">pJp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Jp</span><span class="p">)</span>
                    
                    <span class="k">if</span> <span class="n">pJp</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-30</span><span class="p">:</span>
                        <span class="k">break</span>
                    
                    <span class="n">alpha</span> <span class="o">=</span> <span class="n">rs_old</span> <span class="o">/</span> <span class="n">pJp</span>
                    <span class="n">du</span> <span class="o">=</span> <span class="n">du</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">p</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">Jp</span>
                    <span class="n">rs_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
                    
                    <span class="k">if</span> <span class="n">rs_new</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">:</span>
                        <span class="k">break</span>
                    
                    <span class="n">beta</span> <span class="o">=</span> <span class="n">rs_new</span> <span class="o">/</span> <span class="n">rs_old</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">p</span>
                    <span class="n">rs_old</span> <span class="o">=</span> <span class="n">rs_new</span>
                
                <span class="c1"># Line search</span>
                <span class="k">if</span> <span class="n">line_search</span><span class="p">:</span>
                    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span>
                    <span class="n">F_new_norm</span> <span class="o">=</span> <span class="n">residual_fn</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">du</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
                    <span class="k">while</span> <span class="n">F_new_norm</span> <span class="o">&gt;</span> <span class="n">F_norm</span> <span class="ow">and</span> <span class="n">alpha</span> <span class="o">&gt;</span> <span class="mf">1e-8</span><span class="p">:</span>
                        <span class="n">alpha</span> <span class="o">*=</span> <span class="mf">0.5</span>
                        <span class="n">F_new_norm</span> <span class="o">=</span> <span class="n">residual_fn</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">du</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
                    <span class="n">u</span> <span class="o">=</span> <span class="n">u</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">du</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">u</span> <span class="o">=</span> <span class="n">u</span> <span class="o">+</span> <span class="n">du</span>
        
        <span class="k">return</span> <span class="n">u</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Matrix Operations</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Distributed matrix-vector multiplication: y = D @ x</span>
<span class="sd">        </span>
<span class="sd">        Automatically handles scatter, distributed matvec, and gather.</span>
<span class="sd">        Supports gradient computation when values have requires_grad=True.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            Global vector of shape (N,) where N = shape[1]</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Global result vector of shape (M,) where M = shape[0]</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; D = A.partition(num_partitions=4)</span>
<span class="sd">        &gt;&gt;&gt; y = D @ x  # Equivalent to A @ x</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        **Gradient Support:**</span>
<span class="sd">        </span>
<span class="sd">        For single-node simulation with gradient support, uses global COO matvec.</span>
<span class="sd">        For true MPI distributed execution without gradients, uses partition-based matvec.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_matvec</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Representation</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DSparseTensor(shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="si">}</span><span class="s2">, num_partitions=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;nnz=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="si">}</span><span class="s2">, device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Persistence (I/O)</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="DSparseTensor.save">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.save">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;os.PathLike&quot;</span><span class="p">],</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save DSparseTensor to disk.</span>
<span class="sd">        </span>
<span class="sd">        Creates a directory with metadata and per-partition files.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        directory : str or PathLike</span>
<span class="sd">            Output directory.</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Print progress.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; D = A.partition(num_partitions=4)</span>
<span class="sd">        &gt;&gt;&gt; D.save(&quot;matrix_dist&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">save_dsparse</span>
        <span class="n">save_dsparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">directory</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="DSparseTensor.load">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.DSparseTensor.load">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;os.PathLike&quot;</span><span class="p">],</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a complete DSparseTensor from disk.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        directory : str or PathLike</span>
<span class="sd">            Directory containing saved data.</span>
<span class="sd">        device : str or torch.device</span>
<span class="sd">            Device to load to.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseTensor</span>
<span class="sd">            The loaded distributed sparse tensor.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; D = DSparseTensor.load(&quot;matrix_dist&quot;, device=&quot;cuda&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dsparse</span>
        <span class="k">return</span> <span class="n">load_dsparse</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span></div>
</div>

</pre></div>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, walker chi
            </div>
            Made with 
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/walkerchi/torch-sla" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=b14783f5"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>