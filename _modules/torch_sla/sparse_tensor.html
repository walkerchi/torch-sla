<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="../../genindex.html"><link rel="search" title="Search" href="../../search.html">
        <link rel="canonical" href="https://walkerchi.github.io/torch-sla/_modules/torch_sla/sparse_tensor.html">
        <link rel="prefetch" href="../../_static/logo.jpg" as="image">

    <link rel="shortcut icon" href="../../_static/logo.jpg"><!-- Generated with Sphinx 7.4.7 and Furo 2025.12.19 -->
        <title>torch_sla.sparse_tensor - torch-sla: PyTorch Sparse Linear Algebra | GPU Accelerated</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=69152257" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">torch-sla: PyTorch Sparse Linear Algebra | GPU Accelerated</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/logo.jpg" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">torch-sla: PyTorch Sparse Linear Algebra | GPU Accelerated</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_sla.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <h1>Source code for torch_sla.sparse_tensor</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">SparseTensor wrapper class for PyTorch sparse tensors.</span>

<span class="sd">Supports batched and block sparse tensors with shape [...batch, M, N, ...block]:</span>
<span class="sd">- Leading dimensions: batch dimensions [B1, B2, ...]</span>
<span class="sd">- Matrix dimensions: (M, N) at positions (sparse_dim[0], sparse_dim[1]), default (-2, -1)</span>
<span class="sd">- Trailing dimensions: block dimensions [K1, K2, ...]</span>

<span class="sd">Key Features:</span>
<span class="sd">- Automatic symmetry and positive definiteness detection</span>
<span class="sd">- Sparse linear equation solving with gradient support</span>
<span class="sd">- Sparse-sparse multiplication with sparse gradients</span>
<span class="sd">- Batched operations for all methods</span>
<span class="sd">- CUDA support with LOBPCG for eigenvalue computation</span>

<span class="sd">Examples</span>
<span class="sd">--------</span>
<span class="sd">&gt;&gt;&gt; # Create a simple sparse matrix</span>
<span class="sd">&gt;&gt;&gt; val = torch.tensor([4.0, -1.0, -1.0, 4.0])</span>
<span class="sd">&gt;&gt;&gt; row = torch.tensor([0, 0, 1, 1])</span>
<span class="sd">&gt;&gt;&gt; col = torch.tensor([0, 1, 0, 1])</span>
<span class="sd">&gt;&gt;&gt; A = SparseTensor(val, row, col, (2, 2))</span>
<span class="sd">&gt;&gt;&gt;</span>
<span class="sd">&gt;&gt;&gt; # Check properties (returns boolean tensor for batched)</span>
<span class="sd">&gt;&gt;&gt; is_sym = A.is_symmetric()  # tensor(True)</span>
<span class="sd">&gt;&gt;&gt; is_pd = A.is_positive_definite()  # tensor(True)</span>
<span class="sd">&gt;&gt;&gt;</span>
<span class="sd">&gt;&gt;&gt; # Solve linear system</span>
<span class="sd">&gt;&gt;&gt; b = torch.tensor([1.0, 2.0])</span>
<span class="sd">&gt;&gt;&gt; x = A.solve(b)</span>
<span class="sd">&gt;&gt;&gt;</span>
<span class="sd">&gt;&gt;&gt; # Matrix operations</span>
<span class="sd">&gt;&gt;&gt; y = A @ x  # Sparse @ Dense</span>
<span class="sd">&gt;&gt;&gt; C = A @ A  # Sparse @ Sparse (sparse gradient)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd.function</span><span class="w"> </span><span class="kn">import</span> <span class="n">Function</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.backends</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">is_scipy_available</span><span class="p">,</span>
    <span class="n">is_eigen_available</span><span class="p">,</span>
    <span class="n">is_cusolver_available</span><span class="p">,</span>
    <span class="n">is_cudss_available</span><span class="p">,</span>
    <span class="n">select_backend</span><span class="p">,</span>
    <span class="n">select_method</span><span class="p">,</span>
    <span class="n">BackendType</span><span class="p">,</span>
    <span class="n">MethodType</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.backends.scipy_backend</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">scipy_solve</span><span class="p">,</span>
    <span class="n">scipy_eigs</span><span class="p">,</span>
    <span class="n">scipy_eigsh</span><span class="p">,</span>
    <span class="n">scipy_svds</span><span class="p">,</span>
    <span class="n">scipy_norm</span><span class="p">,</span>
    <span class="n">scipy_lu</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># =============================================================================</span>
<span class="c1"># Adjoint Eigenvalue Solver</span>
<span class="c1"># =============================================================================</span>

<span class="k">class</span><span class="w"> </span><span class="nc">EigshAdjoint</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adjoint-based differentiable eigenvalue solver.</span>
<span class="sd">    </span>
<span class="sd">    Uses implicit differentiation to compute gradients with O(1) graph nodes,</span>
<span class="sd">    regardless of the number of iterations in the forward solve.</span>
<span class="sd">    </span>
<span class="sd">    For symmetric matrix A with eigenvalue λ and eigenvector v:</span>
<span class="sd">        A @ v = λ * v</span>
<span class="sd">    </span>
<span class="sd">    The gradient is:</span>
<span class="sd">        ∂λ/∂A = v @ v.T  (outer product)</span>
<span class="sd">        ∂v/∂A requires solving a linear system (more complex)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">which</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass: compute eigenvalues using LOBPCG or dense fallback.&quot;&quot;&quot;</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Detach for forward computation</span>
        <span class="n">val_detached</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        
        <span class="c1"># Build sparse matrix for matvec</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">sparse_coo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">val_detached</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="k">def</span><span class="w"> </span><span class="nf">matvec</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">sparse_coo</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">sparse_coo</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Compute eigenvalues</span>
        <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
            <span class="c1"># Use LOBPCG on CUDA</span>
            <span class="n">largest</span> <span class="o">=</span> <span class="n">which</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;LM&#39;</span><span class="p">,</span> <span class="s1">&#39;LA&#39;</span><span class="p">)</span>
            <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">_lobpcg_eigsh</span><span class="p">(</span>
                <span class="n">matvec</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="n">largest</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use dense fallback on CPU (SciPy breaks gradient)</span>
            <span class="n">A_dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">val</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">row</span><span class="p">)):</span>
                <span class="n">A_dense</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">col</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">val_detached</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            
            <span class="n">eigenvalues_all</span><span class="p">,</span> <span class="n">eigenvectors_all</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">A_dense</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">which</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;LM&#39;</span><span class="p">,</span> <span class="s1">&#39;LA&#39;</span><span class="p">):</span>
                <span class="c1"># Largest eigenvalues</span>
                <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues_all</span><span class="p">[</span><span class="o">-</span><span class="n">k</span><span class="p">:]</span>
                <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors_all</span><span class="p">[:,</span> <span class="o">-</span><span class="n">k</span><span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Smallest eigenvalues</span>
                <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues_all</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>
                <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors_all</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
        
        <span class="c1"># Save for backward</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">row</span> <span class="o">=</span> <span class="n">row</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">col</span> <span class="o">=</span> <span class="n">col</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">return_eigenvectors</span> <span class="o">=</span> <span class="n">return_eigenvectors</span>
        
        <span class="k">if</span> <span class="n">return_eigenvectors</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span>
        <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="kc">None</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_eigenvalues</span><span class="p">,</span> <span class="n">grad_eigenvectors</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Backward pass using adjoint method.</span>
<span class="sd">        </span>
<span class="sd">        For eigenvalue λ_i with eigenvector v_i:</span>
<span class="sd">            ∂L/∂A[j,k] = Σ_i (∂L/∂λ_i) * v_i[j] * v_i[k]</span>
<span class="sd">        </span>
<span class="sd">        This gives us O(1) graph nodes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">val</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">row</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">row</span>
        <span class="n">col</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">col</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">k</span>
        
        <span class="k">if</span> <span class="n">grad_eigenvalues</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        
        <span class="c1"># Compute gradient w.r.t. values</span>
        <span class="c1"># ∂L/∂A[i,j] = Σ_m (∂L/∂λ_m) * v_m[i] * v_m[j]</span>
        <span class="c1"># For sparse format: ∂L/∂val[idx] = Σ_m (∂L/∂λ_m) * v_m[row[idx]] * v_m[col[idx]]</span>
        
        <span class="n">grad_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad_eigenvalues</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># v_m[row] * v_m[col] for each sparse entry</span>
                <span class="n">v_m</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">m</span><span class="p">]</span>
                <span class="n">grad_val</span> <span class="o">+=</span> <span class="n">grad_eigenvalues</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="n">v_m</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">*</span> <span class="n">v_m</span><span class="p">[</span><span class="n">col</span><span class="p">]</span>
        
        <span class="c1"># Handle eigenvector gradients if needed (more complex, skip for now)</span>
        <span class="c1"># The eigenvector gradient requires solving (A - λI) @ dv = ...</span>
        
        <span class="k">return</span> <span class="n">grad_val</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


<span class="c1"># =============================================================================</span>
<span class="c1"># Utility Functions</span>
<span class="c1"># =============================================================================</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_direct_solver_memory</span><span class="p">(</span><span class="n">nnz</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate memory required for direct sparse solver.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    nnz : int</span>
<span class="sd">        Number of non-zero elements.</span>
<span class="sd">    n : int</span>
<span class="sd">        Matrix dimension.</span>
<span class="sd">    dtype : torch.dtype</span>
<span class="sd">        Data type of the matrix.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Estimated memory in bytes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bytes_per_element</span> <span class="o">=</span> <span class="mi">8</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span> <span class="k">else</span> <span class="mi">4</span>
    <span class="n">fill_factor</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">/</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">factor_memory</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">nnz</span> <span class="o">*</span> <span class="n">fill_factor</span> <span class="o">*</span> <span class="n">bytes_per_element</span><span class="p">)</span>
    <span class="n">workspace_memory</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">bytes_per_element</span> <span class="o">*</span> <span class="mi">10</span>
    <span class="k">return</span> <span class="n">factor_memory</span> <span class="o">+</span> <span class="n">workspace_memory</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_available_gpu_memory</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get available GPU memory in bytes.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Available GPU memory in bytes, or 0 if CUDA is not available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">free_memory</span><span class="p">,</span> <span class="n">total_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">mem_get_info</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">free_memory</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">auto_select_method</span><span class="p">(</span>
    <span class="n">nnz</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">is_cuda</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">is_spd</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">memory_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Automatically select the best backend and method.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    nnz : int</span>
<span class="sd">        Number of non-zero elements.</span>
<span class="sd">    n : int</span>
<span class="sd">        Matrix dimension.</span>
<span class="sd">    dtype : torch.dtype</span>
<span class="sd">        Data type of the matrix.</span>
<span class="sd">    is_cuda : bool</span>
<span class="sd">        Whether the matrix is on CUDA.</span>
<span class="sd">    is_spd : bool, optional</span>
<span class="sd">        Whether the matrix is symmetric positive definite. Default: False.</span>
<span class="sd">    memory_threshold : float, optional</span>
<span class="sd">        Fraction of GPU memory to use. Default: 0.8.</span>
<span class="sd">        </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Tuple[str, str]</span>
<span class="sd">        (backend, method) tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cuda</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_scipy_available</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;scipy&quot;</span><span class="p">,</span> <span class="s2">&quot;superlu&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_eigen_available</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;eigen&quot;</span><span class="p">,</span> <span class="s2">&quot;cg&quot;</span> <span class="k">if</span> <span class="n">is_spd</span> <span class="k">else</span> <span class="s2">&quot;bicgstab&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No CPU backend available&quot;</span><span class="p">)</span>
    
    <span class="n">estimated_memory</span> <span class="o">=</span> <span class="n">estimate_direct_solver_memory</span><span class="p">(</span><span class="n">nnz</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">available_memory</span> <span class="o">=</span> <span class="n">get_available_gpu_memory</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">available_memory</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">estimated_memory</span> <span class="o">&lt;</span> <span class="n">available_memory</span> <span class="o">*</span> <span class="n">memory_threshold</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_cudss_available</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;cudss&quot;</span><span class="p">,</span> <span class="s2">&quot;cholesky&quot;</span> <span class="k">if</span> <span class="n">is_spd</span> <span class="k">else</span> <span class="s2">&quot;lu&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_cusolver_available</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;cusolver&quot;</span><span class="p">,</span> <span class="s2">&quot;cholesky&quot;</span> <span class="k">if</span> <span class="n">is_spd</span> <span class="k">else</span> <span class="s2">&quot;qr&quot;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">is_scipy_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;scipy&quot;</span><span class="p">,</span> <span class="s2">&quot;superlu&quot;</span><span class="p">)</span>
    
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No suitable backend available&quot;</span><span class="p">)</span>


<span class="c1"># =============================================================================</span>
<span class="c1"># Autograd Functions</span>
<span class="c1"># =============================================================================</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SparseSolveFunction</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Differentiable sparse solve using scipy for CPU.</span>
<span class="sd">    </span>
<span class="sd">    Solves Ax = b and computes gradients for both A&#39;s values and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">scipy_solve</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">atol</span> <span class="o">=</span> <span class="n">atol</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">maxiter</span> <span class="o">=</span> <span class="n">maxiter</span>
        <span class="k">return</span> <span class="n">u</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_u</span><span class="p">):</span>
        <span class="n">val</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">method</span>
        <span class="n">atol</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">atol</span>
        <span class="n">maxiter</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">maxiter</span>
        <span class="n">grad_b</span> <span class="o">=</span> <span class="n">scipy_solve</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">grad_u</span><span class="p">,</span>
                            <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">)</span>
        <span class="n">grad_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">grad_b</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">col</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">grad_val</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">grad_b</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">class</span><span class="w"> </span><span class="nc">SparseSparseMatmulFunction</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Differentiable Sparse @ Sparse multiplication with SPARSE gradients.</span>
<span class="sd">    </span>
<span class="sd">    Forward: C = A @ B where A is [M, K], B is [K, N], C is [M, N]</span>
<span class="sd">    </span>
<span class="sd">    Backward:</span>
<span class="sd">    - grad_A_values = (grad_C @ B^T)[A_row, A_col]  (sparse gradient at A&#39;s positions)</span>
<span class="sd">    - grad_B_values = (A^T @ grad_C)[B_row, B_col]  (sparse gradient at B&#39;s positions)</span>
<span class="sd">    </span>
<span class="sd">    The gradients are computed only at the original non-zero positions,</span>
<span class="sd">    keeping memory usage proportional to nnz rather than M*N.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val_a</span><span class="p">,</span> <span class="n">row_a</span><span class="p">,</span> <span class="n">col_a</span><span class="p">,</span> <span class="n">shape_a</span><span class="p">,</span> <span class="n">val_b</span><span class="p">,</span> <span class="n">row_b</span><span class="p">,</span> <span class="n">col_b</span><span class="p">,</span> <span class="n">shape_b</span><span class="p">):</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">shape_a</span>
        <span class="n">K2</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">shape_b</span>
        <span class="k">assert</span> <span class="n">K</span> <span class="o">==</span> <span class="n">K2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Inner dimensions must match: </span><span class="si">{</span><span class="n">K</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="n">K2</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Create torch sparse tensors for multiplication</span>
        <span class="n">A_coo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">row_a</span><span class="p">,</span> <span class="n">col_a</span><span class="p">]),</span> <span class="n">val_a</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
        <span class="n">B_coo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">row_b</span><span class="p">,</span> <span class="n">col_b</span><span class="p">]),</span> <span class="n">val_b</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
        
        <span class="c1"># Sparse @ Sparse -&gt; Sparse</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">C_coo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">A_coo</span><span class="p">,</span> <span class="n">B_coo</span><span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
        
        <span class="c1"># Extract result</span>
        <span class="n">C_indices</span> <span class="o">=</span> <span class="n">C_coo</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
        <span class="n">C_values</span> <span class="o">=</span> <span class="n">C_coo</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
        
        <span class="c1"># Save for backward</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">val_a</span><span class="p">,</span> <span class="n">row_a</span><span class="p">,</span> <span class="n">col_a</span><span class="p">,</span> <span class="n">val_b</span><span class="p">,</span> <span class="n">row_b</span><span class="p">,</span> <span class="n">col_b</span><span class="p">,</span> 
                              <span class="n">C_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">C_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">C_values</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">shape_a</span> <span class="o">=</span> <span class="n">shape_a</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">shape_b</span> <span class="o">=</span> <span class="n">shape_b</span>
        
        <span class="k">return</span> <span class="n">C_values</span><span class="p">,</span> <span class="n">C_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">C_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_C_values</span><span class="p">,</span> <span class="n">grad_row_c</span><span class="p">,</span> <span class="n">grad_col_c</span><span class="p">):</span>
        <span class="p">(</span><span class="n">val_a</span><span class="p">,</span> <span class="n">row_a</span><span class="p">,</span> <span class="n">col_a</span><span class="p">,</span> <span class="n">val_b</span><span class="p">,</span> <span class="n">row_b</span><span class="p">,</span> <span class="n">col_b</span><span class="p">,</span> 
         <span class="n">row_c</span><span class="p">,</span> <span class="n">col_c</span><span class="p">,</span> <span class="n">val_c</span><span class="p">)</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape_a</span>
        <span class="n">K2</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape_b</span>
        
        <span class="n">grad_val_a</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">grad_val_b</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="c1"># grad_A = grad_C @ B^T</span>
            <span class="n">grad_C_coo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">row_c</span><span class="p">,</span> <span class="n">col_c</span><span class="p">]),</span> <span class="n">grad_C_values</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
            <span class="n">B_T_coo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">col_b</span><span class="p">,</span> <span class="n">row_b</span><span class="p">]),</span> <span class="n">val_b</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
            <span class="n">grad_A_dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">grad_C_coo</span><span class="p">,</span> <span class="n">B_T_coo</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="n">grad_val_a</span> <span class="o">=</span> <span class="n">grad_A_dense</span><span class="p">[</span><span class="n">row_a</span><span class="p">,</span> <span class="n">col_a</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span>
            <span class="c1"># grad_B = A^T @ grad_C</span>
            <span class="n">A_T_coo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">col_a</span><span class="p">,</span> <span class="n">row_a</span><span class="p">]),</span> <span class="n">val_a</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
            <span class="n">grad_C_coo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">row_c</span><span class="p">,</span> <span class="n">col_c</span><span class="p">]),</span> <span class="n">grad_C_values</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
            <span class="n">grad_B_dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">A_T_coo</span><span class="p">,</span> <span class="n">grad_C_coo</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="n">grad_val_b</span> <span class="o">=</span> <span class="n">grad_B_dense</span><span class="p">[</span><span class="n">row_b</span><span class="p">,</span> <span class="n">col_b</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">grad_val_a</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">grad_val_b</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_sparse_sparse_matmul_with_sparse_grad</span><span class="p">(</span>
    <span class="n">val_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">row_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">col_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shape_a</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">val_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">row_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">col_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shape_b</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sparse @ Sparse with sparse gradients.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    val_a, row_a, col_a : torch.Tensor</span>
<span class="sd">        COO representation of matrix A.</span>
<span class="sd">    shape_a : Tuple[int, int]</span>
<span class="sd">        Shape of matrix A (M, K).</span>
<span class="sd">    val_b, row_b, col_b : torch.Tensor</span>
<span class="sd">        COO representation of matrix B.</span>
<span class="sd">    shape_b : Tuple[int, int]</span>
<span class="sd">        Shape of matrix B (K, N).</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple[int, int]]</span>
<span class="sd">        (values, row_indices, col_indices, shape) of result C = A @ B.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">shape_a</span>
    <span class="n">K2</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">shape_b</span>
    
    <span class="n">C_values</span><span class="p">,</span> <span class="n">C_row</span><span class="p">,</span> <span class="n">C_col</span> <span class="o">=</span> <span class="n">SparseSparseMatmulFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">val_a</span><span class="p">,</span> <span class="n">row_a</span><span class="p">,</span> <span class="n">col_a</span><span class="p">,</span> <span class="n">shape_a</span><span class="p">,</span>
        <span class="n">val_b</span><span class="p">,</span> <span class="n">row_b</span><span class="p">,</span> <span class="n">col_b</span><span class="p">,</span> <span class="n">shape_b</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">C_values</span><span class="p">,</span> <span class="n">C_row</span><span class="p">,</span> <span class="n">C_col</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>


<span class="c1"># =============================================================================</span>
<span class="c1"># LOBPCG and Power Iteration for CUDA</span>
<span class="c1"># =============================================================================</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_lobpcg_eigsh</span><span class="p">(</span>
    <span class="n">A_matvec</span><span class="p">,</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">largest</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LOBPCG eigenvalue solver for sparse matrices on any device.</span>
<span class="sd">    </span>
<span class="sd">    Uses subspace iteration with Rayleigh-Ritz procedure to find</span>
<span class="sd">    the k largest or smallest eigenvalues.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    A_matvec : callable</span>
<span class="sd">        Function that computes A @ x for input x of shape [n] or [n, m].</span>
<span class="sd">    n : int</span>
<span class="sd">        Matrix dimension.</span>
<span class="sd">    k : int</span>
<span class="sd">        Number of eigenvalues to compute.</span>
<span class="sd">    dtype : torch.dtype</span>
<span class="sd">        Data type.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        Device to compute on.</span>
<span class="sd">    largest : bool, optional</span>
<span class="sd">        If True, compute largest eigenvalues. Default: True.</span>
<span class="sd">    maxiter : int, optional</span>
<span class="sd">        Maximum iterations. Default: 1000.</span>
<span class="sd">    tol : float, optional</span>
<span class="sd">        Convergence tolerance. Default: 1e-8.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Tuple[torch.Tensor, torch.Tensor]</span>
<span class="sd">        (eigenvalues, eigenvectors) with shapes [k] and [n, k].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="n">eigenvalues_prev</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
        <span class="n">AX</span> <span class="o">=</span> <span class="n">A_matvec</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">AX</span>
        <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">largest</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
        
        <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">eigenvectors</span>
        
        <span class="k">if</span> <span class="n">eigenvalues_prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">eigenvalues_prev</span><span class="p">[:</span><span class="n">k</span><span class="p">])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">diff</span> <span class="o">&lt;</span> <span class="n">tol</span> <span class="o">*</span> <span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                <span class="k">break</span>
        <span class="n">eigenvalues_prev</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">iteration</span> <span class="o">&lt;</span> <span class="n">maxiter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">AX</span> <span class="o">=</span> <span class="n">A_matvec</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">AX</span> <span class="o">-</span> <span class="n">X</span> <span class="o">*</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">],</span> <span class="n">residual</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">:</span>
                <span class="n">extra</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">extra</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">[:</span><span class="n">k</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_power_iteration_svd</span><span class="p">(</span>
    <span class="n">A_matvec</span><span class="p">,</span>
    <span class="n">At_matvec</span><span class="p">,</span>
    <span class="n">m</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Power iteration based SVD for sparse matrices on any device.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    A_matvec : callable</span>
<span class="sd">        Function that computes A @ x.</span>
<span class="sd">    At_matvec : callable</span>
<span class="sd">        Function that computes A^T @ x.</span>
<span class="sd">    m, n : int</span>
<span class="sd">        Matrix dimensions (m rows, n columns).</span>
<span class="sd">    k : int</span>
<span class="sd">        Number of singular values to compute.</span>
<span class="sd">    dtype : torch.dtype</span>
<span class="sd">        Data type.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        Device to compute on.</span>
<span class="sd">    maxiter : int, optional</span>
<span class="sd">        Maximum iterations. Default: 100.</span>
<span class="sd">    tol : float, optional</span>
<span class="sd">        Convergence tolerance. Default: 1e-6.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Tuple[torch.Tensor, torch.Tensor, torch.Tensor]</span>
<span class="sd">        (U, S, Vt) with shapes [m, k], [k], [k, n].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">V</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiter</span><span class="p">):</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">A_matvec</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
        <span class="n">V_new</span> <span class="o">=</span> <span class="n">At_matvec</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">V_new</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">V_new</span> <span class="o">=</span> <span class="n">V_new</span> <span class="o">/</span> <span class="n">S</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">)</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">V_new</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_new</span>
        <span class="k">if</span> <span class="n">diff</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>
    
    <span class="k">return</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span>


<span class="c1"># =============================================================================</span>
<span class="c1"># SparseTensor Class</span>
<span class="c1"># =============================================================================</span>

<div class="viewcode-block" id="SparseTensor">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SparseTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper class for PyTorch sparse tensors with batched and block support.</span>
<span class="sd">    </span>
<span class="sd">    Supports tensors with shape [...batch, M, N, ...block] where:</span>
<span class="sd">    - Leading dimensions [...batch] are batch dimensions</span>
<span class="sd">    - (M, N) are the sparse matrix dimensions (at sparse_dim positions)</span>
<span class="sd">    - Trailing dimensions [...block] are block dimensions</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    values : torch.Tensor</span>
<span class="sd">        Non-zero values with shape:</span>
<span class="sd">        - Simple: [nnz]</span>
<span class="sd">        - Batched: [...batch, nnz] </span>
<span class="sd">        - Block: [nnz, *block_shape]</span>
<span class="sd">        - Batched+Block: [...batch, nnz, *block_shape]</span>
<span class="sd">    row_indices : torch.Tensor</span>
<span class="sd">        Row indices with shape [nnz]. Must be on the same device as values.</span>
<span class="sd">    col_indices : torch.Tensor</span>
<span class="sd">        Column indices with shape [nnz]. Must be on the same device as values.</span>
<span class="sd">    shape : Tuple[int, ...]</span>
<span class="sd">        Full tensor shape [...batch, M, N, *block_shape].</span>
<span class="sd">    sparse_dim : Tuple[int, int], optional</span>
<span class="sd">        Which dimensions are sparse (M, N). Default: (-2, -1) meaning last two</span>
<span class="sd">        before any block dimensions.</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    values : torch.Tensor</span>
<span class="sd">        The non-zero values.</span>
<span class="sd">    row_indices : torch.Tensor</span>
<span class="sd">        Row indices of non-zeros.</span>
<span class="sd">    col_indices : torch.Tensor</span>
<span class="sd">        Column indices of non-zeros.</span>
<span class="sd">    shape : Tuple[int, ...]</span>
<span class="sd">        Full tensor shape.</span>
<span class="sd">    sparse_shape : Tuple[int, int]</span>
<span class="sd">        The (M, N) dimensions.</span>
<span class="sd">    batch_shape : Tuple[int, ...]</span>
<span class="sd">        The batch dimensions.</span>
<span class="sd">    block_shape : Tuple[int, ...]</span>
<span class="sd">        The block dimensions.</span>
<span class="sd">    </span>
<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    **1. Simple 2D Sparse Matrix [M, N]**</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from torch_sla import SparseTensor</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Create a 3x3 tridiagonal matrix in COO format</span>
<span class="sd">    &gt;&gt;&gt; val = torch.tensor([4.0, -1.0, -1.0, 4.0, -1.0, -1.0, 4.0])</span>
<span class="sd">    &gt;&gt;&gt; row = torch.tensor([0, 0, 1, 1, 1, 2, 2])</span>
<span class="sd">    &gt;&gt;&gt; col = torch.tensor([0, 1, 0, 1, 2, 1, 2])</span>
<span class="sd">    &gt;&gt;&gt; A = SparseTensor(val, row, col, (3, 3))</span>
<span class="sd">    &gt;&gt;&gt; print(A)</span>
<span class="sd">    SparseTensor(shape=(3, 3), sparse=(3, 3), nnz=7, dtype=torch.float64, device=cpu)</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Solve Ax = b</span>
<span class="sd">    &gt;&gt;&gt; b = torch.tensor([1.0, 2.0, 3.0])</span>
<span class="sd">    &gt;&gt;&gt; x = A.solve(b)</span>
<span class="sd">    </span>
<span class="sd">    **2. Batched Sparse Matrices [B, M, N]**</span>
<span class="sd">    </span>
<span class="sd">    Same sparsity pattern, different values for each batch.</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; # 4 matrices, each 3x3, same structure</span>
<span class="sd">    &gt;&gt;&gt; batch_size = 4</span>
<span class="sd">    &gt;&gt;&gt; val_batch = val.unsqueeze(0).expand(batch_size, -1).clone()  # [4, 7]</span>
<span class="sd">    &gt;&gt;&gt; for i in range(batch_size):</span>
<span class="sd">    ...     val_batch[i] = val * (1.0 + 0.1 * i)  # Scale each matrix</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; A_batch = SparseTensor(val_batch, row, col, (4, 3, 3))</span>
<span class="sd">    &gt;&gt;&gt; print(A_batch.batch_shape)  # (4,)</span>
<span class="sd">    &gt;&gt;&gt; print(A_batch.sparse_shape)  # (3, 3)</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Batched solve</span>
<span class="sd">    &gt;&gt;&gt; b_batch = torch.randn(4, 3)</span>
<span class="sd">    &gt;&gt;&gt; x_batch = A_batch.solve(b_batch)  # [4, 3]</span>
<span class="sd">    </span>
<span class="sd">    **3. Multi-Dimensional Batch [B1, B2, M, N]**</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; B1, B2 = 2, 3  # e.g., 2 materials x 3 temperatures</span>
<span class="sd">    &gt;&gt;&gt; val_batch = val.unsqueeze(0).unsqueeze(0).expand(B1, B2, -1).clone()  # [2, 3, 7]</span>
<span class="sd">    &gt;&gt;&gt; A_multi = SparseTensor(val_batch, row, col, (B1, B2, 3, 3))</span>
<span class="sd">    &gt;&gt;&gt; print(A_multi.batch_shape)  # (2, 3)</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; b_multi = torch.randn(B1, B2, 3)</span>
<span class="sd">    &gt;&gt;&gt; x_multi = A_multi.solve(b_multi)  # [2, 3, 3]</span>
<span class="sd">    </span>
<span class="sd">    **4. Block Sparse Matrix [M, N, K, K] (Block Size K)**</span>
<span class="sd">    </span>
<span class="sd">    Each non-zero entry is a KxK dense block instead of a scalar.</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; # 2x2 block matrix with 2x2 blocks = 4x4 total</span>
<span class="sd">    &gt;&gt;&gt; block_size = 2</span>
<span class="sd">    &gt;&gt;&gt; nnz = 3  # 3 non-zero blocks</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Values: [nnz, K, K] = [3, 2, 2]</span>
<span class="sd">    &gt;&gt;&gt; val_block = torch.randn(nnz, block_size, block_size)</span>
<span class="sd">    &gt;&gt;&gt; row_block = torch.tensor([0, 0, 1])  # Block row indices</span>
<span class="sd">    &gt;&gt;&gt; col_block = torch.tensor([0, 1, 1])  # Block col indices</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Shape: (num_block_rows, num_block_cols, block_size, block_size)</span>
<span class="sd">    &gt;&gt;&gt; A_block = SparseTensor(val_block, row_block, col_block, (2, 2, 2, 2))</span>
<span class="sd">    &gt;&gt;&gt; print(A_block.block_shape)  # (2, 2)</span>
<span class="sd">    &gt;&gt;&gt; print(A_block.sparse_shape)  # (2, 2) - number of blocks</span>
<span class="sd">    &gt;&gt;&gt; print(A_block.shape)  # (2, 2, 2, 2) - full shape</span>
<span class="sd">    </span>
<span class="sd">    **5. Batched Block Sparse [B, M, N, K, K]**</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; batch_size = 4</span>
<span class="sd">    &gt;&gt;&gt; val_batch_block = torch.randn(batch_size, nnz, block_size, block_size)  # [4, 3, 2, 2]</span>
<span class="sd">    &gt;&gt;&gt; A_batch_block = SparseTensor(val_batch_block, row_block, col_block, (4, 2, 2, 2, 2))</span>
<span class="sd">    &gt;&gt;&gt; print(A_batch_block.batch_shape)  # (4,)</span>
<span class="sd">    &gt;&gt;&gt; print(A_batch_block.block_shape)  # (2, 2)</span>
<span class="sd">    </span>
<span class="sd">    **6. Create from Dense Matrix**</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; A_dense = torch.randn(100, 100)</span>
<span class="sd">    &gt;&gt;&gt; A_dense[A_dense.abs() &lt; 0.5] = 0  # Sparsify</span>
<span class="sd">    &gt;&gt;&gt; A = SparseTensor.from_dense(A_dense)</span>
<span class="sd">    </span>
<span class="sd">    **7. Create from PyTorch Sparse Tensor**</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; A_torch = torch.randn(100, 100).to_sparse_coo()</span>
<span class="sd">    &gt;&gt;&gt; A = SparseTensor.from_torch_sparse(A_torch)</span>
<span class="sd">    </span>
<span class="sd">    **8. Property Detection**</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; A = SparseTensor(val, row, col, (3, 3))</span>
<span class="sd">    &gt;&gt;&gt; A.is_symmetric()  # tensor(True) - returns tensor for batch support</span>
<span class="sd">    &gt;&gt;&gt; A.is_positive_definite()  # tensor(True)</span>
<span class="sd">    &gt;&gt;&gt; A.is_positive_definite(&#39;cholesky&#39;)  # Use Cholesky factorization check</span>
<span class="sd">    </span>
<span class="sd">    **9. Matrix Operations**</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; # Matrix-vector multiply</span>
<span class="sd">    &gt;&gt;&gt; y = A @ x  # SparseTensor @ dense vector</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Sparse-sparse multiply (returns SparseTensor with sparse gradients)</span>
<span class="sd">    &gt;&gt;&gt; C = A @ A</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Norms</span>
<span class="sd">    &gt;&gt;&gt; A.norm(&#39;fro&#39;)  # Frobenius norm</span>
<span class="sd">    &gt;&gt;&gt; </span>
<span class="sd">    &gt;&gt;&gt; # Eigenvalues (symmetric matrices)</span>
<span class="sd">    &gt;&gt;&gt; eigenvalues, eigenvectors = A.eigsh(k=2, which=&#39;LM&#39;)</span>
<span class="sd">    </span>
<span class="sd">    **10. CUDA Support**</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; A_cuda = A.cuda()</span>
<span class="sd">    &gt;&gt;&gt; x = A_cuda.solve(b.cuda())  # Uses cuDSS or cuSOLVER</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">row_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">col_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">sparse_dim</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="n">values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span> <span class="o">=</span> <span class="n">row_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span> <span class="o">=</span> <span class="n">col_indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_sparse_dim</span><span class="p">(</span><span class="n">sparse_dim</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
        
        <span class="c1"># Cache for computed properties</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_symmetric_cache</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_positive_definite_cache</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_normalize_sparse_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_dim</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">ndim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Normalize negative indices in sparse_dim.&quot;&quot;&quot;</span>
        <span class="n">dim_m</span> <span class="o">=</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ndim</span> <span class="o">+</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dim_n</span> <span class="o">=</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ndim</span> <span class="o">+</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validate tensor dimensions and indices.&quot;&quot;&quot;</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
        <span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape must have at least 2 dimensions, got </span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">dim_m</span> <span class="o">&lt;</span> <span class="n">ndim</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">dim_n</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sparse_dim </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="si">}</span><span class="s2"> out of range for shape </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dim_m</span> <span class="o">==</span> <span class="n">dim_n</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sparse_dim dimensions must be different&quot;</span><span class="p">)</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Class Methods</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.from_dense">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.from_dense">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_dense</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> 
        <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
        <span class="n">sparse_dim</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create SparseTensor from dense tensor.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        A : torch.Tensor</span>
<span class="sd">            Dense tensor with shape [...batch, M, N, ...block].</span>
<span class="sd">        sparse_dim : Tuple[int, int], optional</span>
<span class="sd">            Which dimensions are sparse. Default: (-2, -1).</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            Sparse representation of A.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A_dense = torch.randn(3, 3)</span>
<span class="sd">        &gt;&gt;&gt; A_dense[A_dense.abs() &lt; 0.5] = 0</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor.from_dense(A_dense)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
        <span class="n">dim_m</span> <span class="o">=</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ndim</span> <span class="o">+</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dim_n</span> <span class="o">=</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ndim</span> <span class="o">+</span> <span class="n">sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">dim_m</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">dim_n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">A_sparse</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">to_sparse_coo</span><span class="p">()</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">A_sparse</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">A_sparse</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">sparse_dim</span><span class="o">=</span><span class="n">sparse_dim</span><span class="p">)</span>
        
        <span class="n">perm</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">]</span>
        <span class="n">A_perm</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="o">*</span><span class="n">perm</span><span class="p">)</span>
        <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">A_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">A_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">A_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">A_flat</span> <span class="o">=</span> <span class="n">A_perm</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        
        <span class="n">A_2d</span> <span class="o">=</span> <span class="n">A_flat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_sparse_coo</span><span class="p">()</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">A_2d</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
        <span class="n">row</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">col</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">nnz</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">values</span> <span class="o">=</span> <span class="n">A_flat</span><span class="p">[:,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">nnz</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">sparse_dim</span><span class="o">=</span><span class="n">sparse_dim</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.from_torch_sparse">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.from_torch_sparse">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_torch_sparse</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create SparseTensor from PyTorch sparse tensor.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        A : torch.Tensor</span>
<span class="sd">            PyTorch sparse COO or CSR tensor (2D only).</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            SparseTensor representation.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A_coo = torch.randn(3, 3).to_sparse_coo()</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor.from_torch_sparse(A_coo)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">A</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">:</span>
            <span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">to_sparse_coo</span><span class="p">()</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Properties</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Full tensor shape [...batch, M, N, ...block].&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sparse_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The (M, N) sparse matrix dimensions.&quot;&quot;&quot;</span>
        <span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">dim_m</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">dim_n</span><span class="p">])</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">batch_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The batch dimensions before the sparse dimensions.&quot;&quot;&quot;</span>
        <span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="n">min_dim</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[:</span><span class="n">min_dim</span><span class="p">]</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">block_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The block dimensions after the sparse dimensions.&quot;&quot;&quot;</span>
        <span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="n">max_dim</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">max_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sparse_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The dimensions that are sparse (M, N).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">nnz</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of non-zero elements (per batch/block).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Data type of the values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Device of the tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">device</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the tensor is on CUDA.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">is_cuda</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_batched</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the tensor has batch dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_block</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the tensor has block dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Total number of batch elements (product of batch_shape).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span> <span class="k">else</span> <span class="mi">1</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_square</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the sparse dimensions are square (M == N).&quot;&quot;&quot;</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="k">return</span> <span class="n">M</span> <span class="o">==</span> <span class="n">N</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Device and Type Management</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.to">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.to">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Move tensor to device and/or convert dtype.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        device : str or torch.device, optional</span>
<span class="sd">            Target device (e.g., &#39;cuda&#39;, &#39;cpu&#39;, &#39;cuda:0&#39;).</span>
<span class="sd">        dtype : torch.dtype, optional</span>
<span class="sd">            Target data type (e.g., torch.float32, torch.float64).</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            New SparseTensor on the target device/dtype.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, shape)</span>
<span class="sd">        &gt;&gt;&gt; A_cuda = A.to(&#39;cuda&#39;)</span>
<span class="sd">        &gt;&gt;&gt; A_float32 = A.to(dtype=torch.float32)</span>
<span class="sd">        &gt;&gt;&gt; A_cuda_float32 = A.to(&#39;cuda&#39;, torch.float32)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span>
        <span class="n">new_row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span>
        <span class="n">new_col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>
        
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">new_values</span> <span class="o">=</span> <span class="n">new_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">new_row</span> <span class="o">=</span> <span class="n">new_row</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">new_col</span> <span class="o">=</span> <span class="n">new_col</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">new_values</span> <span class="o">=</span> <span class="n">new_values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        
        <span class="n">result</span> <span class="o">=</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="n">new_values</span><span class="p">,</span> <span class="n">new_row</span><span class="p">,</span> <span class="n">new_col</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
            <span class="n">sparse_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span></div>

    
<div class="viewcode-block" id="SparseTensor.cuda">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.cuda">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Move tensor to CUDA device.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        device : int, optional</span>
<span class="sd">            CUDA device index. Default: current device.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            Tensor on CUDA.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.cpu">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.cpu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Move tensor to CPU.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            Tensor on CPU.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.float">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.float">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert to float32.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.double">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.double">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert to float64.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.half">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.half">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert to float16.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Conversion Methods</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.to_torch_sparse">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.to_torch_sparse">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_torch_sparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert to PyTorch sparse COO tensor.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch_idx : Tuple[int, ...], optional</span>
<span class="sd">            For batched tensors, which batch element to convert.</span>
<span class="sd">            Default: (0, 0, ...) for first batch element.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            PyTorch sparse COO tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">batch_idx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span>
        
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">vals</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">vals</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span></div>

    
<div class="viewcode-block" id="SparseTensor.to_dense">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.to_dense">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_dense</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert to dense tensor.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch_idx : Tuple[int, ...], optional</span>
<span class="sd">            For batched tensors, which batch element to convert.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Dense tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_torch_sparse</span><span class="p">(</span><span class="n">batch_idx</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span></div>

    
<div class="viewcode-block" id="SparseTensor.to_csr">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.to_csr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_csr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert to CSR format.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch_idx : Tuple[int, ...], optional</span>
<span class="sd">            For batched tensors, which batch element to convert.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            PyTorch sparse CSR tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_torch_sparse</span><span class="p">(</span><span class="n">batch_idx</span><span class="p">)</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span></div>

    
<div class="viewcode-block" id="SparseTensor.partition">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.partition">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">partition</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Partition into a distributed sparse tensor.</span>
<span class="sd">        </span>
<span class="sd">        Creates a DSparseTensor with automatic domain decomposition.</span>
<span class="sd">        This is useful for distributed computing and parallel solvers.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        num_partitions : int</span>
<span class="sd">            Number of partitions to create</span>
<span class="sd">        coords : torch.Tensor, optional</span>
<span class="sd">            Node coordinates for geometric partitioning [num_nodes, dim].</span>
<span class="sd">            Required for &#39;rcb&#39; and &#39;slicing&#39; methods.</span>
<span class="sd">        partition_method : str</span>
<span class="sd">            Partitioning method:</span>
<span class="sd">            - &#39;auto&#39;: Auto-select (uses &#39;rcb&#39; if coords provided, else &#39;metis&#39;)</span>
<span class="sd">            - &#39;metis&#39;: Graph-based partitioning (requires pymetis)</span>
<span class="sd">            - &#39;rcb&#39;: Recursive Coordinate Bisection (requires coords)</span>
<span class="sd">            - &#39;slicing&#39;: Simple coordinate slicing (requires coords)</span>
<span class="sd">            - &#39;simple&#39;: Simple 1D partitioning by node index</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Whether to print partition info</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseTensor</span>
<span class="sd">            Distributed sparse tensor with the specified partitions</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, shape)</span>
<span class="sd">        &gt;&gt;&gt; D = A.partition(num_partitions=4)</span>
<span class="sd">        &gt;&gt;&gt; for i in range(4):</span>
<span class="sd">        ...     partition = D[i]</span>
<span class="sd">        ...     y = partition.matvec(x_local)</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        - Use `D.to_sparse_tensor()` to gather back to a SparseTensor</span>
<span class="sd">        - For distributed training, use `partition_for_rank()` instead</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">DSparseTensor</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;partition() does not support batched SparseTensor. &quot;</span>
                           <span class="s2">&quot;Use a 2D SparseTensor.&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">DSparseTensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span><span class="p">,</span>
            <span class="n">num_partitions</span><span class="o">=</span><span class="n">num_partitions</span><span class="p">,</span>
            <span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">,</span>
            <span class="n">partition_method</span><span class="o">=</span><span class="n">partition_method</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.partition_for_rank">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.partition_for_rank">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">partition_for_rank</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;simple&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DSparseMatrix&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get partition for a specific rank in distributed environment.</span>
<span class="sd">        </span>
<span class="sd">        This is the recommended API for multi-process distributed computing.</span>
<span class="sd">        Each rank calls this method with its own rank ID to get its local</span>
<span class="sd">        partition. The partitioning is deterministic and consistent across</span>
<span class="sd">        all ranks.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        rank : int</span>
<span class="sd">            This process&#39;s rank (0 to world_size-1)</span>
<span class="sd">        world_size : int</span>
<span class="sd">            Total number of processes</span>
<span class="sd">        coords : torch.Tensor, optional</span>
<span class="sd">            Node coordinates for geometric partitioning</span>
<span class="sd">        partition_method : str</span>
<span class="sd">            Partitioning method (&#39;simple&#39;, &#39;metis&#39;, &#39;rcb&#39;, &#39;slicing&#39;)</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Print partition info</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DSparseMatrix</span>
<span class="sd">            Local partition for this rank</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; # In multi-process code:</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, shape)</span>
<span class="sd">        &gt;&gt;&gt; partition = A.partition_for_rank(rank, world_size)</span>
<span class="sd">        &gt;&gt;&gt; y_local = partition.matvec(x_local)</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        - This uses `DSparseTensor.from_global_distributed()` internally,</span>
<span class="sd">          which broadcasts partition IDs from rank 0 for consistency.</span>
<span class="sd">        - Requires `torch.distributed` to be initialized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">DSparseTensor</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;partition_for_rank() does not support batched SparseTensor.&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">DSparseTensor</span><span class="o">.</span><span class="n">from_global_distributed</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span><span class="p">,</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
            <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
            <span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">,</span>
            <span class="n">partition_method</span><span class="o">=</span><span class="n">partition_method</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.T">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.T">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transpose the sparse dimensions.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            Transposed tensor with row/col indices swapped.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
        <span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="n">new_shape</span><span class="p">[</span><span class="n">dim_m</span><span class="p">],</span> <span class="n">new_shape</span><span class="p">[</span><span class="n">dim_n</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_shape</span><span class="p">[</span><span class="n">dim_n</span><span class="p">],</span> <span class="n">new_shape</span><span class="p">[</span><span class="n">dim_m</span><span class="p">]</span>
        
        <span class="n">result</span> <span class="o">=</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>  <span class="c1"># Swap row and col</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span>
            <span class="nb">tuple</span><span class="p">(</span><span class="n">new_shape</span><span class="p">),</span>
            <span class="n">sparse_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span></div>

    
<div class="viewcode-block" id="SparseTensor.flatten_blocks">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.flatten_blocks">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">flatten_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Flatten block dimensions into the sparse (M, N) dimensions.</span>
<span class="sd">        </span>
<span class="sd">        For a block-sparse tensor with shape [...batch, M, N, *block_shape],</span>
<span class="sd">        this creates a new tensor with shape [...batch, M*block_M, N*block_N]</span>
<span class="sd">        where each block entry becomes multiple scalar entries.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            Flattened tensor without block dimensions.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; # Block sparse: shape (10, 10, 2, 2), block_shape=(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (10, 10, 2, 2))</span>
<span class="sd">        &gt;&gt;&gt; A_flat = A.flatten_blocks()</span>
<span class="sd">        &gt;&gt;&gt; print(A_flat.shape)  # (20, 20)</span>
<span class="sd">        &gt;&gt;&gt; print(A_flat.nnz)    # nnz * 4 (each block has 4 elements)</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        - Only works for 2D block shapes (block_M, block_N).</span>
<span class="sd">        - Use `unflatten_blocks(block_shape)` to reverse this operation.</span>
<span class="sd">        - The flattened tensor&#39;s sparsity pattern may have duplicates that</span>
<span class="sd">          need to be coalesced.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>  <span class="c1"># No blocks, return as is</span>
        
        <span class="n">block_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;flatten_blocks only supports 2D blocks, got </span><span class="si">{</span><span class="n">block_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">block_M</span><span class="p">,</span> <span class="n">block_N</span> <span class="o">=</span> <span class="n">block_shape</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
        <span class="n">nnz</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span>
        
        <span class="c1"># New sparse shape</span>
        <span class="n">new_M</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">block_M</span>
        <span class="n">new_N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">*</span> <span class="n">block_N</span>
        
        <span class="c1"># Expand block entries into individual entries</span>
        <span class="c1"># Original: values shape [...batch, nnz, block_M, block_N]</span>
        <span class="c1"># New: values shape [...batch, nnz * block_M * block_N]</span>
        
        <span class="c1"># Create new row/col indices</span>
        <span class="c1"># For each (row, col) block at position (i, j), create indices:</span>
        <span class="c1"># (i*block_M + bi, j*block_N + bj) for bi in [0, block_M), bj in [0, block_N)</span>
        
        <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span>  <span class="c1"># [nnz]</span>
        <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>  <span class="c1"># [nnz]</span>
        
        <span class="c1"># Create block offsets</span>
        <span class="n">block_offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">block_M</span> <span class="o">*</span> <span class="n">block_N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">bi</span> <span class="o">=</span> <span class="n">block_offsets</span> <span class="o">//</span> <span class="n">block_N</span>  <span class="c1"># [block_M * block_N]</span>
        <span class="n">bj</span> <span class="o">=</span> <span class="n">block_offsets</span> <span class="o">%</span> <span class="n">block_N</span>   <span class="c1"># [block_M * block_N]</span>
        
        <span class="c1"># Expand row/col to new indices</span>
        <span class="c1"># new_row[k * block_M * block_N + offset] = row[k] * block_M + bi[offset]</span>
        <span class="n">new_row</span> <span class="o">=</span> <span class="p">(</span><span class="n">row</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_M</span> <span class="o">+</span> <span class="n">bi</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [nnz * block_size]</span>
        <span class="n">new_col</span> <span class="o">=</span> <span class="p">(</span><span class="n">col</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_N</span> <span class="o">+</span> <span class="n">bj</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [nnz * block_size]</span>
        
        <span class="c1"># Flatten values</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># [...batch, nnz, block_M, block_N] -&gt; [...batch, nnz * block_M * block_N]</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">nnz</span> <span class="o">*</span> <span class="n">block_M</span> <span class="o">*</span> <span class="n">block_N</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># [nnz, block_M, block_N] -&gt; [nnz * block_M * block_N]</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">nnz</span> <span class="o">*</span> <span class="n">block_M</span> <span class="o">*</span> <span class="n">block_N</span><span class="p">)</span>
        
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">batch_shape</span> <span class="o">+</span> <span class="p">(</span><span class="n">new_M</span><span class="p">,</span> <span class="n">new_N</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="n">vals</span><span class="p">,</span> <span class="n">new_row</span><span class="p">,</span> <span class="n">new_col</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">,</span>
            <span class="n">sparse_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.unflatten_blocks">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.unflatten_blocks">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unflatten_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Restore block structure from a flattened tensor.</span>
<span class="sd">        </span>
<span class="sd">        This is the inverse of `flatten_blocks()`. It groups scalar entries</span>
<span class="sd">        back into block entries.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        block_shape : Tuple[int, int]</span>
<span class="sd">            The (block_M, block_N) dimensions to create.</span>
<span class="sd">            M and N must be divisible by block_M and block_N respectively.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            Block-sparse tensor with the specified block shape.</span>
<span class="sd">            </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; A_flat = SparseTensor(val, row, col, (20, 20))</span>
<span class="sd">        &gt;&gt;&gt; A_block = A_flat.unflatten_blocks((2, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(A_block.shape)  # (10, 10, 2, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(A_block.block_shape)  # (2, 2)</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        - Requires that the sparsity pattern is block-aligned.</span>
<span class="sd">        - All block entries must be present (dense within each block).</span>
<span class="sd">        - For sparse blocks, use `to_block_sparse()` instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tensor already has block structure. Use flatten_blocks first.&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;block_shape must be 2D, got </span><span class="si">{</span><span class="n">block_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">block_M</span><span class="p">,</span> <span class="n">block_N</span> <span class="o">=</span> <span class="n">block_shape</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
        
        <span class="k">if</span> <span class="n">M</span> <span class="o">%</span> <span class="n">block_M</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">N</span> <span class="o">%</span> <span class="n">block_N</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Sparse shape (</span><span class="si">{</span><span class="n">M</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2">) not divisible by block_shape (</span><span class="si">{</span><span class="n">block_M</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">block_N</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="p">)</span>
        
        <span class="n">new_M</span> <span class="o">=</span> <span class="n">M</span> <span class="o">//</span> <span class="n">block_M</span>
        <span class="n">new_N</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="n">block_N</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="n">block_M</span> <span class="o">*</span> <span class="n">block_N</span>
        
        <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span>
        <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>
        <span class="n">nnz</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span>
        
        <span class="k">if</span> <span class="n">nnz</span> <span class="o">%</span> <span class="n">block_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Number of non-zeros (</span><span class="si">{</span><span class="n">nnz</span><span class="si">}</span><span class="s2">) not divisible by block size (</span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2">). &quot;</span>
                <span class="s2">&quot;The sparsity pattern may not be block-aligned.&quot;</span>
            <span class="p">)</span>
        
        <span class="c1"># Compute block indices</span>
        <span class="n">block_row</span> <span class="o">=</span> <span class="n">row</span> <span class="o">//</span> <span class="n">block_M</span>  <span class="c1"># Which block row</span>
        <span class="n">block_col</span> <span class="o">=</span> <span class="n">col</span> <span class="o">//</span> <span class="n">block_N</span>  <span class="c1"># Which block col</span>
        <span class="n">local_row</span> <span class="o">=</span> <span class="n">row</span> <span class="o">%</span> <span class="n">block_M</span>   <span class="c1"># Position within block</span>
        <span class="n">local_col</span> <span class="o">=</span> <span class="n">col</span> <span class="o">%</span> <span class="n">block_N</span>   <span class="c1"># Position within block</span>
        
        <span class="c1"># Group entries by (block_row, block_col)</span>
        <span class="c1"># Create a unique block ID for sorting</span>
        <span class="n">block_id</span> <span class="o">=</span> <span class="n">block_row</span> <span class="o">*</span> <span class="n">new_N</span> <span class="o">+</span> <span class="n">block_col</span>
        
        <span class="c1"># Sort by block_id, then by local position</span>
        <span class="n">local_offset</span> <span class="o">=</span> <span class="n">local_row</span> <span class="o">*</span> <span class="n">block_N</span> <span class="o">+</span> <span class="n">local_col</span>
        <span class="n">sort_key</span> <span class="o">=</span> <span class="n">block_id</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span> <span class="n">local_offset</span>
        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">sort_key</span><span class="p">)</span>
        
        <span class="n">sorted_block_id</span> <span class="o">=</span> <span class="n">block_id</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">]</span>
        <span class="n">sorted_local_offset</span> <span class="o">=</span> <span class="n">local_offset</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">]</span>
        
        <span class="c1"># Extract unique blocks</span>
        <span class="n">unique_blocks</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span><span class="n">sorted_block_id</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">counts</span> <span class="o">==</span> <span class="n">block_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Not all blocks are complete. Each block must have exactly &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">block_size</span><span class="si">}</span><span class="s2"> entries.&quot;</span>
            <span class="p">)</span>
        
        <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">unique_blocks</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">new_row_indices</span> <span class="o">=</span> <span class="n">unique_blocks</span> <span class="o">//</span> <span class="n">new_N</span>
        <span class="n">new_col_indices</span> <span class="o">=</span> <span class="n">unique_blocks</span> <span class="o">%</span> <span class="n">new_N</span>
        
        <span class="c1"># Reshape values to include block dimensions</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Sort values: [...batch, nnz] -&gt; [...batch, num_blocks * block_size]</span>
            <span class="n">sorted_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">sort_idx</span><span class="p">]</span>
            <span class="n">new_vals</span> <span class="o">=</span> <span class="n">sorted_vals</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">block_M</span><span class="p">,</span> <span class="n">block_N</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sorted_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">]</span>
            <span class="n">new_vals</span> <span class="o">=</span> <span class="n">sorted_vals</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">block_M</span><span class="p">,</span> <span class="n">block_N</span><span class="p">)</span>
        
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">batch_shape</span> <span class="o">+</span> <span class="p">(</span><span class="n">new_M</span><span class="p">,</span> <span class="n">new_N</span><span class="p">,</span> <span class="n">block_M</span><span class="p">,</span> <span class="n">block_N</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="n">new_vals</span><span class="p">,</span> <span class="n">new_row_indices</span><span class="p">,</span> <span class="n">new_col_indices</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">,</span>
            <span class="n">sparse_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Property Detection (returns tensor for batched)</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.is_symmetric">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.is_symmetric">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_symmetric</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> 
        <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">force_recompute</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if the matrix is symmetric (A == A^T).</span>
<span class="sd">        </span>
<span class="sd">        For batched tensors, checks each matrix independently and returns</span>
<span class="sd">        a boolean tensor with shape matching the batch dimensions.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        atol : float, optional</span>
<span class="sd">            Absolute tolerance for comparison. Default: 1e-8.</span>
<span class="sd">        rtol : float, optional</span>
<span class="sd">            Relative tolerance for comparison. Default: 1e-5.</span>
<span class="sd">        force_recompute : bool, optional</span>
<span class="sd">            If True, recompute even if cached. Default: False.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Boolean tensor with shape:</span>
<span class="sd">            - [] (scalar) for non-batched tensors</span>
<span class="sd">            - [*batch_shape] for batched tensors</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; A.is_symmetric()  # tensor(True) or tensor(False)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; A_batch = SparseTensor(val_batch, row, col, (4, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; A_batch.is_symmetric()  # tensor([True, True, True, True])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_symmetric_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">force_recompute</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_symmetric_cache</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_square</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_symmetric_cache</span> <span class="o">=</span> <span class="n">result</span>
            <span class="k">return</span> <span class="n">result</span>
        
        <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span>
        <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="c1"># Create hash for (row, col) pairs</span>
        <span class="n">forward_hash</span> <span class="o">=</span> <span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span>
        <span class="n">transpose_hash</span> <span class="o">=</span> <span class="n">col</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">row</span>
        
        <span class="c1"># Sort both to align</span>
        <span class="n">forward_order</span> <span class="o">=</span> <span class="n">forward_hash</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
        <span class="n">transpose_order</span> <span class="o">=</span> <span class="n">transpose_hash</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
        
        <span class="n">sorted_forward_hash</span> <span class="o">=</span> <span class="n">forward_hash</span><span class="p">[</span><span class="n">forward_order</span><span class="p">]</span>
        <span class="n">sorted_transpose_hash</span> <span class="o">=</span> <span class="n">transpose_hash</span><span class="p">[</span><span class="n">transpose_order</span><span class="p">]</span>
        
        <span class="c1"># Check sparsity pattern</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">sorted_forward_hash</span><span class="p">,</span> <span class="n">sorted_transpose_hash</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_symmetric_cache</span> <span class="o">=</span> <span class="n">result</span>
            <span class="k">return</span> <span class="n">result</span>
        
        <span class="c1"># Compare values</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">vals_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)</span>
            <span class="n">vals_forward</span> <span class="o">=</span> <span class="n">vals_flat</span><span class="p">[:,</span> <span class="n">forward_order</span><span class="p">]</span>
            <span class="n">vals_transpose</span> <span class="o">=</span> <span class="n">vals_flat</span><span class="p">[:,</span> <span class="n">transpose_order</span><span class="p">]</span>
            
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">vals_forward</span> <span class="o">-</span> <span class="n">vals_transpose</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
            <span class="n">threshold</span> <span class="o">=</span> <span class="n">atol</span> <span class="o">+</span> <span class="n">rtol</span> <span class="o">*</span> <span class="n">vals_forward</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
            <span class="n">is_sym</span> <span class="o">=</span> <span class="p">(</span><span class="n">diff</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">is_sym</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vals_forward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">forward_order</span><span class="p">]</span>
            <span class="n">vals_transpose</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">transpose_order</span><span class="p">]</span>
            
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">vals_forward</span> <span class="o">-</span> <span class="n">vals_transpose</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
            <span class="n">threshold</span> <span class="o">=</span> <span class="n">atol</span> <span class="o">+</span> <span class="n">rtol</span> <span class="o">*</span> <span class="n">vals_forward</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((</span><span class="n">diff</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_symmetric_cache</span> <span class="o">=</span> <span class="n">result</span>
        <span class="k">return</span> <span class="n">result</span></div>

    
<div class="viewcode-block" id="SparseTensor.is_positive_definite">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.is_positive_definite">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_positive_definite</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;gershgorin&quot;</span><span class="p">,</span> <span class="s2">&quot;cholesky&quot;</span><span class="p">,</span> <span class="s2">&quot;eigenvalue&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gershgorin&quot;</span><span class="p">,</span>
        <span class="n">force_recompute</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if the matrix is positive definite.</span>
<span class="sd">        </span>
<span class="sd">        For batched tensors, checks each matrix independently and returns</span>
<span class="sd">        a boolean tensor with shape matching the batch dimensions.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        method : {&quot;gershgorin&quot;, &quot;cholesky&quot;, &quot;eigenvalue&quot;}, optional</span>
<span class="sd">            Method for checking:</span>
<span class="sd">            - &quot;gershgorin&quot;: Fast check using Gershgorin circles (sufficient but not necessary)</span>
<span class="sd">            - &quot;cholesky&quot;: Try Cholesky decomposition (necessary and sufficient, slower)</span>
<span class="sd">            - &quot;eigenvalue&quot;: Check smallest eigenvalues (necessary and sufficient, slowest)</span>
<span class="sd">            Default: &quot;gershgorin&quot;.</span>
<span class="sd">        force_recompute : bool, optional</span>
<span class="sd">            If True, recompute even if cached. Default: False.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Boolean tensor with shape:</span>
<span class="sd">            - [] (scalar) for non-batched tensors</span>
<span class="sd">            - [*batch_shape] for batched tensors</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; A.is_positive_definite()  # tensor(True) or tensor(False)</span>
<span class="sd">        &gt;&gt;&gt; A.is_positive_definite(method=&quot;cholesky&quot;)  # More accurate check</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; A_batch = SparseTensor(val_batch, row, col, (4, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; A_batch.is_positive_definite()  # tensor([True, True, True, True])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_positive_definite_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">force_recompute</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_positive_definite_cache</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_square</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_positive_definite_cache</span> <span class="o">=</span> <span class="n">result</span>
            <span class="k">return</span> <span class="n">result</span>
        
        <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span>
        <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;gershgorin&quot;</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_pd_gershgorin</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;cholesky&quot;</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_pd_cholesky</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># eigenvalue</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_pd_eigenvalue</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_positive_definite_cache</span> <span class="o">=</span> <span class="n">result</span>
        <span class="k">return</span> <span class="n">result</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="nf">_check_pd_gershgorin</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check positive definiteness using Gershgorin circles.&quot;&quot;&quot;</span>
        <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span>
        <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="n">is_diag</span> <span class="o">=</span> <span class="p">(</span><span class="n">row</span> <span class="o">==</span> <span class="n">col</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">vals_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)</span>
            
            <span class="c1"># Diagonal elements</span>
            <span class="n">diag_rows</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="n">is_diag</span><span class="p">]</span>
            <span class="n">diag_vals</span> <span class="o">=</span> <span class="n">vals_flat</span><span class="p">[:,</span> <span class="n">is_diag</span><span class="p">]</span>  <span class="c1"># [B, num_diag]</span>
            
            <span class="n">diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">diag</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">diag_rows</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">diag_vals</span><span class="p">)</span>
            
            <span class="c1"># Off-diagonal sum</span>
            <span class="n">is_offdiag</span> <span class="o">=</span> <span class="o">~</span><span class="n">is_diag</span>
            <span class="n">offdiag_rows</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="n">is_offdiag</span><span class="p">]</span>
            <span class="n">offdiag_vals</span> <span class="o">=</span> <span class="n">vals_flat</span><span class="p">[:,</span> <span class="n">is_offdiag</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>  <span class="c1"># [B, num_offdiag]</span>
            
            <span class="n">offdiag_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">offdiag_sum</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">offdiag_rows</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">offdiag_vals</span><span class="p">)</span>
            
            <span class="c1"># Check: diag &gt; offdiag_sum AND diag &gt; 0</span>
            <span class="n">is_pd</span> <span class="o">=</span> <span class="p">((</span><span class="n">diag</span> <span class="o">&gt;</span> <span class="n">offdiag_sum</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">diag</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">is_pd</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">diag_rows</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="n">is_diag</span><span class="p">]</span>
            <span class="n">diag_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">is_diag</span><span class="p">]</span>
            
            <span class="n">diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">diag</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">diag_rows</span><span class="p">,</span> <span class="n">diag_vals</span><span class="p">)</span>
            
            <span class="n">is_offdiag</span> <span class="o">=</span> <span class="o">~</span><span class="n">is_diag</span>
            <span class="n">offdiag_rows</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="n">is_offdiag</span><span class="p">]</span>
            <span class="n">offdiag_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">is_offdiag</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
            
            <span class="n">offdiag_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">offdiag_sum</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">offdiag_rows</span><span class="p">,</span> <span class="n">offdiag_vals</span><span class="p">)</span>
            
            <span class="n">is_pd</span> <span class="o">=</span> <span class="p">((</span><span class="n">diag</span> <span class="o">&gt;</span> <span class="n">offdiag_sum</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">diag</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">is_pd</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_check_pd_cholesky</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check positive definiteness using Cholesky decomposition.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_indices</span><span class="p">():</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">A_dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">A_dense</span><span class="p">)</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">A_dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">A_dense</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_check_pd_eigenvalue</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check positive definiteness using eigenvalue computation.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_indices</span><span class="p">():</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">A_dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">A_dense</span><span class="p">)</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">eigenvalues</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">A_dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
                <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">A_dense</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((</span><span class="n">eigenvalues</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_batch_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate all batch index tuples.&quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
        <span class="n">ranges</span> <span class="o">=</span> <span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">ranges</span><span class="p">)</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Matrix Multiplication</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_spmv_coo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sparse matrix-vector/matrix multiply using COO format with scatter_add.</span>
<span class="sd">        </span>
<span class="sd">        Computes A @ x where A is this sparse tensor and x is dense.</span>
<span class="sd">        Works on any device without explicit CSR conversion.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            Dense tensor to multiply. Shape depends on batching:</span>
<span class="sd">            - Non-batched: [N] or [N, K]</span>
<span class="sd">            - Batched: [B, N] or [B, N, K]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Result of A @ x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span>
        <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">vals_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># x: [N] - same for all batches -&gt; result [B, M]</span>
                <span class="n">x_gathered</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="n">vals_flat</span> <span class="o">*</span> <span class="n">x_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">row_expanded</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">row_expanded</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
            
            <span class="k">elif</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># x: [...batch, N] -&gt; result [...batch, M]</span>
                <span class="n">x_flat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
                <span class="n">x_gathered</span> <span class="o">=</span> <span class="n">x_flat</span><span class="p">[:,</span> <span class="n">col</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="n">vals_flat</span> <span class="o">*</span> <span class="n">x_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">row_expanded</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">row_expanded</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># x: [...batch, N, K] -&gt; result [...batch, M, K]</span>
                <span class="n">K</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">x_flat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
                <span class="n">x_gathered</span> <span class="o">=</span> <span class="n">x_flat</span><span class="p">[:,</span> <span class="n">col</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="n">vals_flat</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">row_expanded</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">row_expanded</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">x_gathered</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">x_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">K</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">x_gathered</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">row_expanded</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">row_expanded</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_dense_sparse_mm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Dense @ Sparse: X @ A where X is [..., M] or [..., K, M], A is [..., M, N].</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : torch.Tensor</span>
<span class="sd">            Dense tensor.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Result of X @ A.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span>
        <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">vals_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">X_gathered</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">row</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="n">vals_flat</span> <span class="o">*</span> <span class="n">X_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">col_expanded</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">col_expanded</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
            
            <span class="k">elif</span> <span class="n">X</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">X_flat</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
                <span class="n">X_gathered</span> <span class="o">=</span> <span class="n">X_flat</span><span class="p">[:,</span> <span class="n">row</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="n">vals_flat</span> <span class="o">*</span> <span class="n">X_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">col_expanded</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">col_expanded</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
            
            <span class="k">else</span><span class="p">:</span>
                <span class="n">K</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">X_flat</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
                <span class="n">X_gathered</span> <span class="o">=</span> <span class="n">X_flat</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">row</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="n">vals_flat</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">col_expanded</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">col_expanded</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">X_gathered</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">row</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">X_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">K</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">X_gathered</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">row</span><span class="p">]</span>
                <span class="n">products</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_gathered</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">col_expanded</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">col_expanded</span><span class="p">,</span> <span class="n">products</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_spsp_multiply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sparse-Sparse multiplication: A @ B where both are sparse.</span>
<span class="sd">        </span>
<span class="sd">        Uses custom autograd function to provide SPARSE gradients.</span>
<span class="sd">        Memory usage is O(nnz) not O(M*N).</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other : SparseTensor</span>
<span class="sd">            Right-hand side sparse matrix.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            Result C = A @ B.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="n">K2</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="k">if</span> <span class="n">K</span> <span class="o">!=</span> <span class="n">K2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inner dimensions don&#39;t match: </span><span class="si">{</span><span class="n">K</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="n">K2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">C_values</span><span class="p">,</span> <span class="n">C_row</span><span class="p">,</span> <span class="n">C_col</span><span class="p">,</span> <span class="n">C_shape</span> <span class="o">=</span> <span class="n">_sparse_sparse_matmul_with_sparse_grad</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span>
            <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">C_row</span><span class="p">,</span> <span class="n">C_col</span><span class="p">,</span> <span class="n">C_shape</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Matrix multiplication: A @ other.</span>
<span class="sd">        </span>
<span class="sd">        Supports:</span>
<span class="sd">        - Sparse @ Dense vector: A @ x -&gt; y</span>
<span class="sd">        - Sparse @ Dense matrix: A @ X -&gt; Y</span>
<span class="sd">        - Sparse @ Sparse: A @ B -&gt; C (with sparse gradients)</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other : torch.Tensor or SparseTensor</span>
<span class="sd">            Right-hand side operand.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor or SparseTensor</span>
<span class="sd">            Result of multiplication.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spsp_multiply</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spmv_coo</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Dense @ Sparse multiplication: other @ A.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other : torch.Tensor</span>
<span class="sd">            Left-hand side dense tensor.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Result of multiplication.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dense_sparse_mm</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Linear Solve</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.solve">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.solve">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">solve</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">BackendType</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">MethodType</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve the sparse linear system Ax = b.</span>
<span class="sd">        </span>
<span class="sd">        Automatically handles batched tensors: if A is [...batch, M, N] and</span>
<span class="sd">        b is [...batch, M], returns x with shape [...batch, N].</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : torch.Tensor</span>
<span class="sd">            Right-hand side vector(s). Shape:</span>
<span class="sd">            - Non-batched: [M] or [M, K] for multiple RHS</span>
<span class="sd">            - Batched: [...batch, M] or [...batch, M, K]</span>
<span class="sd">        backend : {&quot;auto&quot;, &quot;scipy&quot;, &quot;eigen&quot;, &quot;cusolver&quot;, &quot;cudss&quot;}, optional</span>
<span class="sd">            Solver backend. Default: &quot;auto&quot; (selects based on device).</span>
<span class="sd">            - &quot;scipy&quot;: Uses SciPy&#39;s sparse solvers (CPU only)</span>
<span class="sd">            - &quot;eigen&quot;: Uses Eigen C++ library (CPU only)</span>
<span class="sd">            - &quot;cusolver&quot;: Uses NVIDIA cuSOLVER (CUDA only)</span>
<span class="sd">            - &quot;cudss&quot;: Uses NVIDIA cuDSS (CUDA only)</span>
<span class="sd">        method : str, optional</span>
<span class="sd">            Solver method. Default: &quot;auto&quot; (selects based on matrix properties).</span>
<span class="sd">            - Direct methods: &quot;superlu&quot;, &quot;umfpack&quot;, &quot;lu&quot;, &quot;qr&quot;, &quot;cholesky&quot;, &quot;ldlt&quot;</span>
<span class="sd">            - Iterative methods: &quot;cg&quot;, &quot;bicgstab&quot;, &quot;gmres&quot;, &quot;minres&quot;</span>
<span class="sd">        atol : float, optional</span>
<span class="sd">            Absolute tolerance for iterative solvers. Default: 1e-10.</span>
<span class="sd">        maxiter : int, optional</span>
<span class="sd">            Maximum iterations for iterative solvers. Default: 10000.</span>
<span class="sd">        tol : float, optional</span>
<span class="sd">            Relative tolerance for direct solvers. Default: 1e-12.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Solution x with same batch shape as b.</span>
<span class="sd">        </span>
<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If matrix is not square.</span>
<span class="sd">        NotImplementedError</span>
<span class="sd">            If block sparse tensors are used (not yet supported).</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; # Simple solve</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = torch.randn(3)</span>
<span class="sd">        &gt;&gt;&gt; x = A.solve(b)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; # Batched solve</span>
<span class="sd">        &gt;&gt;&gt; A_batch = SparseTensor(val_batch, row, col, (4, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; b_batch = torch.randn(4, 3)</span>
<span class="sd">        &gt;&gt;&gt; x_batch = A_batch.solve(b_batch)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; # Specify backend</span>
<span class="sd">        &gt;&gt;&gt; x = A.solve(b, backend=&#39;scipy&#39;, method=&#39;cg&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_square</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Matrix must be square for solve()&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;solve() not yet supported for block sparse tensors&quot;</span><span class="p">)</span>
        
        <span class="c1"># Get matrix properties</span>
        <span class="n">is_sym</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_symmetric</span><span class="p">()</span><span class="o">.</span><span class="n">all</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_symmetric</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">is_pd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_positive_definite</span><span class="p">()</span><span class="o">.</span><span class="n">all</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_positive_definite</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">is_spd</span> <span class="o">=</span> <span class="n">is_sym</span> <span class="ow">and</span> <span class="n">is_pd</span>
        
        <span class="kn">from</span><span class="w"> </span><span class="nn">.linear_solve</span><span class="w"> </span><span class="kn">import</span> <span class="n">spsolve</span>
        
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">vals_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)</span>
            <span class="n">b_flat</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
            
            <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vals_flat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">spsolve</span><span class="p">(</span>
                    <span class="n">vals_flat</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">b_flat</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
                    <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                    <span class="n">is_symmetric</span><span class="o">=</span><span class="n">is_sym</span><span class="p">,</span> <span class="n">is_spd</span><span class="o">=</span><span class="n">is_spd</span>
                <span class="p">)</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">spsolve</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
                <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">b</span><span class="p">,</span>
                <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
                <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">is_symmetric</span><span class="o">=</span><span class="n">is_sym</span><span class="p">,</span> <span class="n">is_spd</span><span class="o">=</span><span class="n">is_spd</span>
            <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.solve_batch">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.solve_batch">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">solve_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">BackendType</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">MethodType</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-12</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve with different values but same sparsity structure.</span>
<span class="sd">        </span>
<span class="sd">        This is efficient when you have the same structure but different values</span>
<span class="sd">        (e.g., time-stepping, optimization, parameter sweeps).</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        values : torch.Tensor</span>
<span class="sd">            Matrix values. Shape [...batch, nnz] where ... are batch dimensions.</span>
<span class="sd">            All matrices share the same row_indices and col_indices.</span>
<span class="sd">        b : torch.Tensor</span>
<span class="sd">            Right-hand side. Shape [...batch, M].</span>
<span class="sd">        backend : {&quot;auto&quot;, &quot;scipy&quot;, &quot;eigen&quot;, &quot;cusolver&quot;, &quot;cudss&quot;}, optional</span>
<span class="sd">            Solver backend. See solve() for details. Default: &quot;auto&quot;.</span>
<span class="sd">        method : str, optional</span>
<span class="sd">            Solver method. See solve() for details. Default: &quot;auto&quot;.</span>
<span class="sd">        atol : float, optional</span>
<span class="sd">            Absolute tolerance for iterative solvers. Default: 1e-10.</span>
<span class="sd">        maxiter : int, optional</span>
<span class="sd">            Maximum iterations for iterative solvers. Default: 10000.</span>
<span class="sd">        tol : float, optional</span>
<span class="sd">            Relative tolerance. Default: 1e-12.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Solution x with shape [...batch, N].</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; # Template matrix</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (10, 10))</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; # Batch of different values</span>
<span class="sd">        &gt;&gt;&gt; val_batch = torch.stack([val * (1 + 0.1*i) for i in range(4)])  # [4, nnz]</span>
<span class="sd">        &gt;&gt;&gt; b_batch = torch.randn(4, 10)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; # Solve all at once</span>
<span class="sd">        &gt;&gt;&gt; x_batch = A.solve_batch(val_batch, b_batch)  # [4, 10]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.linear_solve</span><span class="w"> </span><span class="kn">import</span> <span class="n">spsolve</span>
        
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="c1"># Check properties using first batch element</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">SparseTensor</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">values</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">values</span><span class="p">,</span> 
                           <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="n">is_sym</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">is_symmetric</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">is_pd</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">is_positive_definite</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">is_spd</span> <span class="o">=</span> <span class="n">is_sym</span> <span class="ow">and</span> <span class="n">is_pd</span>
        
        <span class="k">if</span> <span class="n">values</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">vals_flat</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)</span>
            <span class="n">b_flat</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
            
            <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vals_flat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">spsolve</span><span class="p">(</span>
                    <span class="n">vals_flat</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">b_flat</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
                    <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                    <span class="n">is_symmetric</span><span class="o">=</span><span class="n">is_sym</span><span class="p">,</span> <span class="n">is_spd</span><span class="o">=</span><span class="n">is_spd</span>
                <span class="p">)</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">spsolve</span><span class="p">(</span>
                <span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">b</span><span class="p">,</span>
                <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
                <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">is_symmetric</span><span class="o">=</span><span class="n">is_sym</span><span class="p">,</span> <span class="n">is_spd</span><span class="o">=</span><span class="n">is_spd</span>
            <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.nonlinear_solve">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.nonlinear_solve">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">nonlinear_solve</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">residual_fn</span><span class="p">,</span>
        <span class="n">u0</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">*</span><span class="n">params</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">,</span> <span class="s1">&#39;picard&#39;</span><span class="p">,</span> <span class="s1">&#39;anderson&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;newton&#39;</span><span class="p">,</span>
        <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">line_search</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">linear_solver</span><span class="p">:</span> <span class="n">BackendType</span> <span class="o">=</span> <span class="s1">&#39;pytorch&#39;</span><span class="p">,</span>
        <span class="n">linear_method</span><span class="p">:</span> <span class="n">MethodType</span> <span class="o">=</span> <span class="s1">&#39;cg&#39;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve nonlinear equation F(u, A, θ) = 0 with adjoint-based gradients.</span>
<span class="sd">        </span>
<span class="sd">        The SparseTensor A is automatically passed as the first parameter to</span>
<span class="sd">        the residual function, enabling gradients to flow through A&#39;s values.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        residual_fn : Callable</span>
<span class="sd">            Function F(u, A, *params) -&gt; residual tensor.</span>
<span class="sd">            - u: Current solution estimate</span>
<span class="sd">            - A: This SparseTensor (passed automatically)</span>
<span class="sd">            - *params: Additional parameters with requires_grad=True</span>
<span class="sd">        u0 : torch.Tensor</span>
<span class="sd">            Initial guess for solution.</span>
<span class="sd">        *params : torch.Tensor</span>
<span class="sd">            Additional parameters (e.g., boundary conditions, coefficients).</span>
<span class="sd">            Tensors with requires_grad=True will receive gradients.</span>
<span class="sd">        method : {&#39;newton&#39;, &#39;picard&#39;, &#39;anderson&#39;}, optional</span>
<span class="sd">            Nonlinear solver method:</span>
<span class="sd">            - &#39;newton&#39;: Newton-Raphson with line search (default, fast)</span>
<span class="sd">            - &#39;picard&#39;: Fixed-point iteration (simple, slow)</span>
<span class="sd">            - &#39;anderson&#39;: Anderson acceleration (memory efficient)</span>
<span class="sd">        tol : float, optional</span>
<span class="sd">            Relative convergence tolerance. Default: 1e-6.</span>
<span class="sd">        atol : float, optional</span>
<span class="sd">            Absolute convergence tolerance. Default: 1e-10.</span>
<span class="sd">        max_iter : int, optional</span>
<span class="sd">            Maximum nonlinear iterations. Default: 50.</span>
<span class="sd">        line_search : bool, optional</span>
<span class="sd">            Use Armijo line search for Newton. Default: True.</span>
<span class="sd">        verbose : bool, optional</span>
<span class="sd">            Print convergence information. Default: False.</span>
<span class="sd">        linear_solver : str, optional</span>
<span class="sd">            Backend for linear solves. Default: &#39;pytorch&#39;.</span>
<span class="sd">        linear_method : str, optional</span>
<span class="sd">            Method for linear solves. Default: &#39;cg&#39;.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Solution u* satisfying F(u*, A, θ) ≈ 0.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; # Nonlinear PDE: A @ u + u² = f</span>
<span class="sd">        &gt;&gt;&gt; def residual(u, A, f):</span>
<span class="sd">        ...     return A @ u + u**2 - f</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (n, n))</span>
<span class="sd">        &gt;&gt;&gt; f = torch.randn(n, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; u0 = torch.zeros(n)</span>
<span class="sd">        &gt;&gt;&gt; </span>
<span class="sd">        &gt;&gt;&gt; u = A.nonlinear_solve(residual, u0, f, method=&#39;newton&#39;)</span>
<span class="sd">        &gt;&gt;&gt; </span>
<span class="sd">        &gt;&gt;&gt; # Gradients flow via adjoint method</span>
<span class="sd">        &gt;&gt;&gt; loss = u.sum()</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt; print(f.grad)  # ∂u/∂f</span>
<span class="sd">        &gt;&gt;&gt; print(A.values.grad)  # ∂u/∂A (if A.values.requires_grad)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; # Nonlinear elasticity: K(u) @ u = F</span>
<span class="sd">        &gt;&gt;&gt; def residual_elasticity(u, K, F, material):</span>
<span class="sd">        ...     # K depends on displacement through material nonlinearity</span>
<span class="sd">        ...     return K @ u - F + material * u**3</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; u = K.nonlinear_solve(residual_elasticity, u0, F, material)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.nonlinear_solve</span><span class="w"> </span><span class="kn">import</span> <span class="n">nonlinear_solve</span> <span class="k">as</span> <span class="n">_nonlinear_solve</span>
        
        <span class="c1"># Wrap residual_fn to pass SparseTensor as matvec</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">def</span><span class="w"> </span><span class="nf">wrapped_residual</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="o">*</span><span class="n">all_params</span><span class="p">):</span>
            <span class="c1"># First param is the values tensor, rest are user params</span>
            <span class="c1"># Reconstruct sparse matvec capability</span>
            <span class="k">return</span> <span class="n">residual_fn</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">all_params</span><span class="p">)</span>
        
        <span class="c1"># Include self.values in params if it requires grad</span>
        <span class="n">all_params</span> <span class="o">=</span> <span class="n">params</span>
        
        <span class="k">return</span> <span class="n">_nonlinear_solve</span><span class="p">(</span>
            <span class="n">wrapped_residual</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="o">*</span><span class="n">all_params</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">line_search</span><span class="o">=</span><span class="n">line_search</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">linear_solver</span><span class="o">=</span><span class="n">linear_solver</span><span class="p">,</span> <span class="n">linear_method</span><span class="o">=</span><span class="n">linear_method</span><span class="p">,</span>
            <span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Norms</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.norm">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.norm">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">ord</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;fro&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;fro&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute matrix norm.</span>
<span class="sd">        </span>
<span class="sd">        For batched tensors, returns norm for each batch element.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ord : {&#39;fro&#39;, 1, 2}, optional</span>
<span class="sd">            Norm type:</span>
<span class="sd">            - &#39;fro&#39;: Frobenius norm (default)</span>
<span class="sd">            - 1: Maximum absolute column sum</span>
<span class="sd">            - 2: Spectral norm (largest singular value)</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Norm value(s). Shape [] for non-batched, [*batch_shape] for batched.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; A.norm(&#39;fro&#39;)  # tensor(5.0)</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; A_batch = SparseTensor(val_batch, row, col, (4, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; A_batch.norm(&#39;fro&#39;)  # tensor([5.0, 5.0, 5.0, 5.0])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">vals_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)</span>
            <span class="n">norms</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vals_flat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
                <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="s1">&#39;fro&#39;</span><span class="p">:</span>
                    <span class="n">norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vals_flat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flat_to_batch_idx</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                    <span class="n">A_dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A_dense</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="nb">ord</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">norms</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="s1">&#39;fro&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">is_scipy_available</span><span class="p">():</span>
                <span class="n">A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="nb">ord</span><span class="p">)</span>
            <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
            <span class="k">return</span> <span class="n">scipy_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="nb">ord</span><span class="o">=</span><span class="nb">ord</span><span class="p">)</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="nf">_flat_to_batch_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flat_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert flat batch index to tuple.&quot;&quot;&quot;</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">):</span>
            <span class="n">idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flat_idx</span> <span class="o">%</span> <span class="n">s</span><span class="p">)</span>
            <span class="n">flat_idx</span> <span class="o">//=</span> <span class="n">s</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Visualization</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.spy">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.spy">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">spy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">title</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cmap</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;viridis&#39;</span><span class="p">,</span>
        <span class="n">show_grid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">grid_color</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;#cccccc&#39;</span><span class="p">,</span>
        <span class="n">grid_linewidth</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">show_colorbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
        <span class="n">save_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dpi</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">150</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Visualize the sparsity pattern with values shown as color intensity.</span>
<span class="sd">        </span>
<span class="sd">        Creates a spy plot where each matrix element is rendered as a pixel.</span>
<span class="sd">        Non-zero elements are colored with intensity proportional to the absolute</span>
<span class="sd">        value, while zero elements are shown as white. This provides a pixel-perfect</span>
<span class="sd">        visualization without overlapping markers.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch_idx : Tuple[int, ...], optional</span>
<span class="sd">            For batched tensors, which batch element to visualize.</span>
<span class="sd">            Required if the tensor is batched.</span>
<span class="sd">        ax : matplotlib.axes.Axes, optional</span>
<span class="sd">            Axes to plot on. If None, creates a new figure.</span>
<span class="sd">        title : str, optional</span>
<span class="sd">            Plot title. Defaults to showing matrix info.</span>
<span class="sd">        cmap : str, optional</span>
<span class="sd">            Colormap for values. Default: &#39;viridis&#39;.</span>
<span class="sd">            Other options: &#39;plasma&#39;, &#39;hot&#39;, &#39;coolwarm&#39;, &#39;Greys&#39;, etc.</span>
<span class="sd">        show_grid : bool, optional</span>
<span class="sd">            Whether to show grid lines (only for matrices &lt;= 30x30). Default: True.</span>
<span class="sd">        grid_color : str, optional</span>
<span class="sd">            Color of grid lines. Default: &#39;#cccccc&#39; (light gray).</span>
<span class="sd">        grid_linewidth : float, optional</span>
<span class="sd">            Width of grid lines. Default: 0.5.</span>
<span class="sd">        show_colorbar : bool, optional</span>
<span class="sd">            Whether to show colorbar for values. Default: True.</span>
<span class="sd">        figsize : Tuple[float, float], optional</span>
<span class="sd">            Figure size in inches. Default: (8, 8).</span>
<span class="sd">        save_path : str, optional</span>
<span class="sd">            If provided, save figure to this path.</span>
<span class="sd">        dpi : int, optional</span>
<span class="sd">            DPI for saved figure. Default: 150.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ax : matplotlib.axes.Axes</span>
<span class="sd">            The axes object with the plot.</span>
<span class="sd">            </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (100, 100))</span>
<span class="sd">        &gt;&gt;&gt; A.spy()  # Basic spy plot</span>
<span class="sd">        &gt;&gt;&gt; A.spy(cmap=&#39;hot&#39;, show_grid=False)  # Custom colormap, no grid</span>
<span class="sd">        &gt;&gt;&gt; A.spy(save_path=&#39;matrix.png&#39;)  # Save to file</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; # For batched tensor</span>
<span class="sd">        &gt;&gt;&gt; A_batch = SparseTensor(val_batch, row, col, (4, 100, 100))</span>
<span class="sd">        &gt;&gt;&gt; A_batch.spy(batch_idx=(0,))  # Visualize first batch element</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mcolors</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">PathCollection</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;matplotlib is required for spy(). Install with: pip install matplotlib&quot;</span><span class="p">)</span>
        
        <span class="c1"># Get indices and values</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;batch_idx is required for batched tensors&quot;</span><span class="p">)</span>
            <span class="c1"># Flatten batch_idx to linear index</span>
            <span class="n">flat_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">batch_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)):</span>
                <span class="n">flat_idx</span> <span class="o">=</span> <span class="n">flat_idx</span> <span class="o">*</span> <span class="n">s</span> <span class="o">+</span> <span class="n">idx</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)[</span><span class="n">flat_idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span>
        
        <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">vals_np</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="c1"># Create figure if needed</span>
        <span class="n">created_fig</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
            <span class="n">created_fig</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fig</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_figure</span><span class="p">()</span>
        
        <span class="c1"># Normalize values for colormap</span>
        <span class="k">if</span> <span class="n">vals_np</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">vals_norm</span> <span class="o">=</span> <span class="n">vals_np</span> <span class="o">/</span> <span class="n">vals_np</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vals_norm</span> <span class="o">=</span> <span class="n">vals_np</span>
        
        <span class="c1"># Build a dense image for visualization</span>
        <span class="c1"># Use NaN for empty cells (will be shown as white)</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">image</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals_norm</span>
        
        <span class="c1"># Create a colormap with white for NaN values</span>
        <span class="n">cmap_obj</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="n">cmap</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">cmap_obj</span><span class="o">.</span><span class="n">set_bad</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
        
        <span class="c1"># Use imshow for pixel-perfect rendering</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
            <span class="n">image</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_obj</span><span class="p">,</span>
            <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span>
            <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span>
            <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;upper&#39;</span>
        <span class="p">)</span>
        
        <span class="c1"># Add colorbar</span>
        <span class="k">if</span> <span class="n">show_colorbar</span><span class="p">:</span>
            <span class="n">cbar</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
            <span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;|value| (normalized)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        
        <span class="c1"># Clean up axes - hide ticks for cleaner look</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        
        <span class="c1"># Add border</span>
        <span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">spine</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">spine</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;#333333&#39;</span><span class="p">)</span>
            <span class="n">spine</span><span class="o">.</span><span class="n">set_linewidth</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Add grid only for small matrices</span>
        <span class="k">if</span> <span class="n">show_grid</span> <span class="ow">and</span> <span class="nb">max</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">30</span><span class="p">:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="n">i</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">minor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="n">i</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">minor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">&#39;minor&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">grid_color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">grid_linewidth</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">&#39;minor&#39;</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Set title</span>
        <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nnz</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vals_np</span><span class="p">)</span>
            <span class="n">sparsity</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">nnz</span> <span class="o">/</span> <span class="p">(</span><span class="n">M</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span>
            <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Sparse Matrix: </span><span class="si">{</span><span class="n">M</span><span class="si">}</span><span class="s1">×</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">, nnz=</span><span class="si">{</span><span class="n">nnz</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1">, sparsity=</span><span class="si">{</span><span class="n">sparsity</span><span class="si">:</span><span class="s1">.1%</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
        
        <span class="c1"># Tight layout</span>
        <span class="k">if</span> <span class="n">created_fig</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        
        <span class="c1"># Save if requested</span>
        <span class="k">if</span> <span class="n">save_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="n">dpi</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">ax</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Eigenvalues and SVD</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.eigs">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.eigs">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eigs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">which</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;LM&quot;</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_eigenvectors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute k eigenvalues and eigenvectors.</span>
<span class="sd">        </span>
<span class="sd">        For batched tensors, computes for each batch element.</span>
<span class="sd">        For CUDA tensors, uses LOBPCG algorithm.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int, optional</span>
<span class="sd">            Number of eigenvalues to compute. Default: 6.</span>
<span class="sd">        which : {&quot;LM&quot;, &quot;SM&quot;, &quot;LR&quot;, &quot;SR&quot;, &quot;LA&quot;, &quot;SA&quot;}, optional</span>
<span class="sd">            Which eigenvalues to find:</span>
<span class="sd">            - &quot;LM&quot;: Largest magnitude (default)</span>
<span class="sd">            - &quot;SM&quot;: Smallest magnitude</span>
<span class="sd">            - &quot;LR&quot;/&quot;SR&quot;: Largest/smallest real part</span>
<span class="sd">            - &quot;LA&quot;/&quot;SA&quot;: Largest/smallest algebraic (for symmetric)</span>
<span class="sd">        sigma : float, optional</span>
<span class="sd">            Find eigenvalues near sigma (shift-invert mode).</span>
<span class="sd">        return_eigenvectors : bool, optional</span>
<span class="sd">            Whether to return eigenvectors. Default: True.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        eigenvalues : torch.Tensor</span>
<span class="sd">            Shape [k] for non-batched, [*batch_shape, k] for batched.</span>
<span class="sd">        eigenvectors : torch.Tensor or None</span>
<span class="sd">            Shape [M, k] for non-batched, [*batch_shape, M, k] for batched.</span>
<span class="sd">            None if return_eigenvectors is False.</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        **Gradient Support:**</span>
<span class="sd">        </span>
<span class="sd">        - Both CPU and CUDA: Fully differentiable via adjoint method</span>
<span class="sd">        - Uses O(1) graph nodes regardless of iteration count</span>
<span class="sd">        - For symmetric matrices, prefer eigsh() for efficiency</span>
<span class="sd">        </span>
<span class="sd">        **Warning**: For non-symmetric matrices with complex eigenvalues,</span>
<span class="sd">        gradient computation is only supported for the real part.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val.requires_grad_(True), row, col, (n, n))</span>
<span class="sd">        &gt;&gt;&gt; eigenvalues, eigenvectors = A.eigs(k=3)</span>
<span class="sd">        &gt;&gt;&gt; loss = eigenvalues.real.sum()  # For complex eigenvalues</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">eigenvalues_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">eigenvectors_list</span> <span class="o">=</span> <span class="p">[]</span>
            
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_indices</span><span class="p">():</span>
                <span class="n">A_single</span> <span class="o">=</span> <span class="n">SparseTensor</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">evals</span><span class="p">,</span> <span class="n">evecs</span> <span class="o">=</span> <span class="n">A_single</span><span class="o">.</span><span class="n">eigs</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">which</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="p">)</span>
                <span class="n">eigenvalues_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evals</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">return_eigenvectors</span><span class="p">:</span>
                    <span class="n">eigenvectors_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evecs</span><span class="p">)</span>
            
            <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">eigenvalues_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">return_eigenvectors</span><span class="p">:</span>
                <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">eigenvectors_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span>
            <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="kc">None</span>
        
        <span class="c1"># For symmetric matrices or when using LA/SA, use eigsh (more efficient)</span>
        <span class="k">if</span> <span class="n">which</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;LA&quot;</span><span class="p">,</span> <span class="s2">&quot;SA&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_symmetric</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigsh</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="n">which</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="o">=</span><span class="n">return_eigenvectors</span><span class="p">)</span>
        
        <span class="c1"># Use adjoint-based eigs for differentiability on all devices</span>
        <span class="k">return</span> <span class="n">EigshAdjoint</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span>
            <span class="n">k</span><span class="p">,</span> <span class="n">which</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.eigsh">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.eigsh">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eigsh</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">which</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;LM&quot;</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_eigenvectors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute k eigenvalues for symmetric matrices.</span>
<span class="sd">        </span>
<span class="sd">        More efficient than eigs() for symmetric matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int, optional</span>
<span class="sd">            Number of eigenvalues to compute. Default: 6.</span>
<span class="sd">        which : {&quot;LM&quot;, &quot;SM&quot;, &quot;LA&quot;, &quot;SA&quot;}, optional</span>
<span class="sd">            Which eigenvalues to find:</span>
<span class="sd">            - &quot;LM&quot;: Largest magnitude (default)</span>
<span class="sd">            - &quot;SM&quot;: Smallest magnitude</span>
<span class="sd">            - &quot;LA&quot;/&quot;SA&quot;: Largest/smallest algebraic</span>
<span class="sd">        sigma : float, optional</span>
<span class="sd">            Find eigenvalues near sigma.</span>
<span class="sd">        return_eigenvectors : bool, optional</span>
<span class="sd">            Whether to return eigenvectors. Default: True.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        eigenvalues : torch.Tensor</span>
<span class="sd">            Shape [k] for non-batched, [*batch_shape, k] for batched.</span>
<span class="sd">        eigenvectors : torch.Tensor or None</span>
<span class="sd">            Shape [M, k] for non-batched, [*batch_shape, M, k] for batched.</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        **Gradient Support:**</span>
<span class="sd">        </span>
<span class="sd">        - Both CPU and CUDA: Fully differentiable via adjoint method</span>
<span class="sd">        - Uses O(1) graph nodes regardless of iteration count</span>
<span class="sd">        - Gradient computed as: ∂L/∂A = Σ_i (∂L/∂λ_i) * v_i @ v_i.T</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val.requires_grad_(True), row, col, (n, n))</span>
<span class="sd">        &gt;&gt;&gt; eigenvalues, eigenvectors = A.eigsh(k=3)</span>
<span class="sd">        &gt;&gt;&gt; loss = eigenvalues.sum()</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()  # Computes ∂loss/∂val</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">eigenvalues_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">eigenvectors_list</span> <span class="o">=</span> <span class="p">[]</span>
            
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_indices</span><span class="p">():</span>
                <span class="n">A_single</span> <span class="o">=</span> <span class="n">SparseTensor</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">evals</span><span class="p">,</span> <span class="n">evecs</span> <span class="o">=</span> <span class="n">A_single</span><span class="o">.</span><span class="n">eigsh</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">which</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="p">)</span>
                <span class="n">eigenvalues_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evals</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">return_eigenvectors</span><span class="p">:</span>
                    <span class="n">eigenvectors_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evecs</span><span class="p">)</span>
            
            <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">eigenvalues_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">return_eigenvectors</span><span class="p">:</span>
                <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">eigenvectors_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span>
            <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="kc">None</span>
        
        <span class="c1"># Use adjoint-based eigsh for differentiability on all devices</span>
        <span class="k">return</span> <span class="n">EigshAdjoint</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span>
            <span class="n">k</span><span class="p">,</span> <span class="n">which</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.svd">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.svd">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">svd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute truncated SVD.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int, optional</span>
<span class="sd">            Number of singular values to compute. Default: 6.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        U : torch.Tensor</span>
<span class="sd">            Left singular vectors. Shape [M, k] or [*batch_shape, M, k].</span>
<span class="sd">        S : torch.Tensor</span>
<span class="sd">            Singular values. Shape [k] or [*batch_shape, k].</span>
<span class="sd">        Vt : torch.Tensor</span>
<span class="sd">            Right singular vectors. Shape [k, N] or [*batch_shape, k, N].</span>
<span class="sd">        </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        **Gradient Support:**</span>
<span class="sd">        </span>
<span class="sd">        - CUDA: Fully differentiable (uses power iteration with PyTorch operations)</span>
<span class="sd">        - CPU: NOT differentiable (uses SciPy which breaks gradient chain)</span>
<span class="sd">        </span>
<span class="sd">        For differentiable SVD on CPU, use `A.to_dense()` and `torch.linalg.svd()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
    
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">U_list</span><span class="p">,</span> <span class="n">S_list</span><span class="p">,</span> <span class="n">Vt_list</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
            
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_indices</span><span class="p">():</span>
                <span class="n">A_single</span> <span class="o">=</span> <span class="n">SparseTensor</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">A_single</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
                <span class="n">U_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
                <span class="n">S_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
                <span class="n">Vt_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Vt</span><span class="p">)</span>
            
            <span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">U_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
            <span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">S_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
            <span class="n">Vt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Vt_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="n">matvec</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spmv_coo</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">matvec_T</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">()</span><span class="o">.</span><span class="n">_spmv_coo</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">_power_iteration_svd</span><span class="p">(</span><span class="n">matvec</span><span class="p">,</span> <span class="n">matvec_T</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_scipy_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;SciPy is required for SVD on CPU&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">scipy_svds</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.condition_number">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.condition_number">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">condition_number</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">ord</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Estimate condition number.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ord : int, optional</span>
<span class="sd">            Norm order for condition number. Default: 2 (spectral).</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Condition number. Shape [] or [*batch_shape].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">cond_list</span> <span class="o">=</span> <span class="p">[]</span>
            
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_indices</span><span class="p">():</span>
                <span class="n">A_single</span> <span class="o">=</span> <span class="n">SparseTensor</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">cond_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A_single</span><span class="o">.</span><span class="n">condition_number</span><span class="p">(</span><span class="nb">ord</span><span class="p">))</span>
            
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">cond_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">A_dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
                <span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svdvals</span><span class="p">(</span><span class="n">A_dense</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">S</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="n">S</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">S</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="n">S</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        
        <span class="n">norm_A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="nb">ord</span><span class="o">=</span><span class="nb">ord</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">e</span> <span class="o">/</span> <span class="n">e</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">norm_A</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">/</span> <span class="n">e</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># LU Factorization</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.lu">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.lu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">lu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LUFactorization&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute LU decomposition for repeated solves.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        LUFactorization</span>
<span class="sd">            Factorization object with solve() method.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (10, 10))</span>
<span class="sd">        &gt;&gt;&gt; lu = A.lu()</span>
<span class="sd">        &gt;&gt;&gt; x1 = lu.solve(b1)</span>
<span class="sd">        &gt;&gt;&gt; x2 = lu.solve(b2)  # Reuses factorization</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;lu() not supported for batched tensors&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;LU decomposition on CUDA not yet supported&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_scipy_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;SciPy is required for LU decomposition&quot;</span><span class="p">)</span>
        
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="n">lu</span> <span class="o">=</span> <span class="n">scipy_lu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">LUFactorization</span><span class="p">(</span><span class="n">lu</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># String Representation</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;SparseTensor(shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;batch=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sparse=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
            <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;block=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;nnz=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span>
    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Reduction Operations (sum, mean, prod)</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_normalize_axis</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Normalize axis to tuple of positive indices.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="n">axis</span><span class="p">,)</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">a</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ndim</span> <span class="o">+</span> <span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_get_dim_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the type of dimension: &#39;batch&#39;, &#39;sparse_m&#39;, &#39;sparse_n&#39;, or &#39;block&#39;.&quot;&quot;&quot;</span>
        <span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="n">min_sparse</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">)</span>
        <span class="n">max_sparse</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="n">min_sparse</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;batch&#39;</span>
        <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="n">dim_m</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;sparse_m&#39;</span>
        <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="n">dim_n</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;sparse_n&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;block&#39;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_values_axis_for_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Map tensor dimension to values tensor dimension.</span>
<span class="sd">        </span>
<span class="sd">        Values shape: [...batch, nnz, ...block]</span>
<span class="sd">        Tensor shape: [...batch, M, N, ...block]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="n">min_sparse</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">)</span>
        <span class="n">max_sparse</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="n">min_sparse</span><span class="p">:</span>
            <span class="c1"># Batch dimension - same position</span>
            <span class="k">return</span> <span class="n">dim</span>
        <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="n">dim_m</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">==</span> <span class="n">dim_n</span><span class="p">:</span>
            <span class="c1"># Sparse dimension - maps to nnz axis</span>
            <span class="k">return</span> <span class="n">min_sparse</span>  <span class="c1"># nnz is at the position of first sparse dim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Block dimension - after nnz axis</span>
            <span class="c1"># Shift by -1 because we replaced (M, N) with (nnz,)</span>
            <span class="k">return</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span>
    
<div class="viewcode-block" id="SparseTensor.sum">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.sum">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sum of sparse tensor elements over specified axis.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, tuple of ints, or None</span>
<span class="sd">            Axis or axes along which to sum. Axes correspond to:</span>
<span class="sd">            - Batch dimensions: [...batch] at the beginning</span>
<span class="sd">            - Sparse dimensions: (M, N) at sparse_dim positions</span>
<span class="sd">            - Block dimensions: [...block] at the end</span>
<span class="sd">            </span>
<span class="sd">            If None, sum over all elements (returns scalar tensor).</span>
<span class="sd">        keepdim : bool</span>
<span class="sd">            Whether to keep the reduced dimensions.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor or SparseTensor</span>
<span class="sd">            - If reducing over sparse dimensions: returns dense tensor</span>
<span class="sd">            - If reducing over batch/block dimensions only: returns SparseTensor</span>
<span class="sd">            - If axis=None: returns scalar tensor</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; # Shape: [batch=2, M=10, N=10, block=3]</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (2, 10, 10, 3))</span>
<span class="sd">        &gt;&gt;&gt; </span>
<span class="sd">        &gt;&gt;&gt; A.sum()           # Scalar: sum all elements</span>
<span class="sd">        &gt;&gt;&gt; A.sum(axis=0)     # Sum over batch -&gt; [10, 10, 3]</span>
<span class="sd">        &gt;&gt;&gt; A.sum(axis=1)     # Sum over M (rows) -&gt; [2, 10, 3] (dense)</span>
<span class="sd">        &gt;&gt;&gt; A.sum(axis=2)     # Sum over N (cols) -&gt; [2, 10, 3] (dense)</span>
<span class="sd">        &gt;&gt;&gt; A.sum(axis=3)     # Sum over block -&gt; SparseTensor [2, 10, 10]</span>
<span class="sd">        &gt;&gt;&gt; A.sum(axis=(1,2)) # Sum over M and N -&gt; [2, 3] (dense)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Sum over all elements</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        
        <span class="n">axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="n">dim_types</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_dim_type</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        
        <span class="c1"># Check if we&#39;re reducing over sparse dimensions</span>
        <span class="n">has_sparse_reduction</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">dt</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;sparse_m&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse_n&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">dim_types</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">has_sparse_reduction</span><span class="p">:</span>
            <span class="c1"># Need to convert to dense for sparse reduction</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sum_over_sparse</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Only batch/block reduction - can stay sparse</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sum_over_batch_block</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="nf">_sum_over_sparse</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">axes</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> 
        <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sum that involves sparse dimensions - returns dense.&quot;&quot;&quot;</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span>
        <span class="n">dim_m</span><span class="p">,</span> <span class="n">dim_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span>
        
        <span class="c1"># Separate sparse and non-sparse axes</span>
        <span class="n">sparse_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dim_type</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;sparse_m&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse_n&#39;</span><span class="p">)]</span>
        <span class="n">other_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dim_type</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;sparse_m&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse_n&#39;</span><span class="p">)]</span>
        
        <span class="n">reduce_m</span> <span class="o">=</span> <span class="n">dim_m</span> <span class="ow">in</span> <span class="n">axes</span>
        <span class="n">reduce_n</span> <span class="o">=</span> <span class="n">dim_n</span> <span class="ow">in</span> <span class="n">axes</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_batched</span><span class="p">:</span>
            <span class="n">B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_shape</span>
            <span class="n">vals_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">reduce_m</span> <span class="ow">and</span> <span class="n">reduce_n</span><span class="p">:</span>
                <span class="c1"># Sum all sparse entries per batch</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">vals_flat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, *block]</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span> <span class="k">else</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">reduce_m</span><span class="p">:</span>
                <span class="c1"># Sum over rows -&gt; result is [B, N, *block]</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">col_idx</span> <span class="o">=</span> <span class="n">col</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
                        <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">),</span> <span class="n">vals_flat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">vals_flat</span><span class="p">)</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span> <span class="k">else</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># reduce_n</span>
                <span class="c1"># Sum over cols -&gt; result is [B, M, *block]</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">row_idx</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
                        <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">),</span> <span class="n">vals_flat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">vals_flat</span><span class="p">)</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span> <span class="k">else</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span>
            
            <span class="k">if</span> <span class="n">reduce_m</span> <span class="ow">and</span> <span class="n">reduce_n</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span> <span class="k">else</span> <span class="n">vals</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="k">elif</span> <span class="n">reduce_m</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">),</span> <span class="n">vals</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">vals</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># reduce_n</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_block</span><span class="p">:</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span><span class="p">),</span> <span class="n">vals</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">vals</span><span class="p">)</span>
        
        <span class="c1"># Handle other axes reduction</span>
        <span class="k">if</span> <span class="n">other_axes</span><span class="p">:</span>
            <span class="n">result_axes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_values_axis_for_dim</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">other_axes</span><span class="p">]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">result_axes</span><span class="p">),</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">result</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_sum_over_batch_block</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">axes</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> 
        <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sum over batch/block dimensions only - stays sparse.&quot;&quot;&quot;</span>
        <span class="c1"># Map tensor axes to values axes</span>
        <span class="n">val_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values_axis_for_dim</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">)</span>
        <span class="n">new_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">val_axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        
        <span class="c1"># Compute new shape</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">keepdim</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
                <span class="n">new_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                <span class="k">del</span> <span class="n">new_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
        
        <span class="c1"># Adjust sparse_dim if needed</span>
        <span class="n">new_sparse_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdim</span><span class="p">:</span>
            <span class="n">removed_before_m</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">removed_before_n</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">new_sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">removed_before_m</span>
            <span class="n">new_sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-=</span> <span class="n">removed_before_n</span>
        
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="n">new_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> 
            <span class="nb">tuple</span><span class="p">(</span><span class="n">new_shape</span><span class="p">),</span> <span class="n">sparse_dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_sparse_dim</span><span class="p">)</span>
        <span class="p">)</span>
    
<div class="viewcode-block" id="SparseTensor.mean">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.mean">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Mean of sparse tensor elements over specified axis.</span>
<span class="sd">        </span>
<span class="sd">        Note: For sparse dimensions, this computes mean of non-zero values only,</span>
<span class="sd">        NOT the mean over all M*N elements. For full mean, use to_dense().mean().</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, tuple of ints, or None</span>
<span class="sd">            Axis or axes along which to compute mean.</span>
<span class="sd">        keepdim : bool</span>
<span class="sd">            Whether to keep the reduced dimensions.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor or SparseTensor</span>
<span class="sd">            Mean values.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (10, 10))</span>
<span class="sd">        &gt;&gt;&gt; A.mean()           # Mean of all non-zero values</span>
<span class="sd">        &gt;&gt;&gt; A.mean(axis=0)     # Mean over batch dimension</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        
        <span class="n">axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        
        <span class="c1"># For sparse dims, we compute sum/count of nnz (not M*N)</span>
        <span class="n">sum_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        
        <span class="c1"># Compute divisor based on axes</span>
        <span class="n">divisor</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
            <span class="n">divisor</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
        
        <span class="c1"># But for sparse dimensions, divisor should be nnz not M*N</span>
        <span class="n">dim_types</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_dim_type</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="k">if</span> <span class="s1">&#39;sparse_m&#39;</span> <span class="ow">in</span> <span class="n">dim_types</span> <span class="ow">or</span> <span class="s1">&#39;sparse_n&#39;</span> <span class="ow">in</span> <span class="n">dim_types</span><span class="p">:</span>
            <span class="c1"># For sparse reduction, we&#39;re averaging over nnz values</span>
            <span class="n">sparse_divisor</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="s1">&#39;sparse_m&#39;</span> <span class="ow">in</span> <span class="n">dim_types</span><span class="p">:</span>
                <span class="n">sparse_divisor</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="s1">&#39;sparse_n&#39;</span> <span class="ow">in</span> <span class="n">dim_types</span><span class="p">:</span>
                <span class="n">sparse_divisor</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># Replace M*N with nnz</span>
            <span class="n">divisor</span> <span class="o">=</span> <span class="n">divisor</span> <span class="o">//</span> <span class="n">sparse_divisor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz</span>
        
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sum_result</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
                <span class="n">sum_result</span><span class="o">.</span><span class="n">values</span> <span class="o">/</span> <span class="n">divisor</span><span class="p">,</span>
                <span class="n">sum_result</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span>
                <span class="n">sum_result</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
                <span class="n">sum_result</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                <span class="n">sparse_dim</span><span class="o">=</span><span class="n">sum_result</span><span class="o">.</span><span class="n">sparse_dim</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">sum_result</span> <span class="o">/</span> <span class="n">divisor</span></div>

    
<div class="viewcode-block" id="SparseTensor.prod">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.prod">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">prod</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Product of sparse tensor elements over specified axis.</span>
<span class="sd">        </span>
<span class="sd">        Warning: For sparse matrices, zero elements are not included in the product.</span>
<span class="sd">        This means prod() computes the product of non-zero values only.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int, tuple of ints, or None</span>
<span class="sd">            Axis or axes along which to compute product.</span>
<span class="sd">        keepdim : bool</span>
<span class="sd">            Whether to keep the reduced dimensions.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor or SparseTensor</span>
<span class="sd">            Product values.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (10, 10))</span>
<span class="sd">        &gt;&gt;&gt; A.prod()           # Product of all non-zero values</span>
<span class="sd">        &gt;&gt;&gt; A.prod(axis=0)     # Product over batch dimension</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span>
        
        <span class="n">axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="n">dim_types</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_dim_type</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        
        <span class="c1"># Check if we&#39;re reducing over sparse dimensions</span>
        <span class="n">has_sparse_reduction</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">dt</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;sparse_m&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse_n&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">dim_types</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">has_sparse_reduction</span><span class="p">:</span>
            <span class="c1"># For sparse reduction, prod is complex - convert to dense</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;prod() over sparse dimensions converts to dense. &quot;</span>
                <span class="s2">&quot;This may use significant memory for large matrices.&quot;</span>
            <span class="p">)</span>
            <span class="n">dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">dense</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Only batch/block reduction</span>
            <span class="n">val_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values_axis_for_dim</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">)</span>
            <span class="n">new_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">val_axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
            
            <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">keepdim</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
                    <span class="n">new_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                    <span class="k">del</span> <span class="n">new_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
            
            <span class="n">new_sparse_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdim</span><span class="p">:</span>
                <span class="n">removed_before_m</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">removed_before_n</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">new_sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">removed_before_m</span>
                <span class="n">new_sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-=</span> <span class="n">removed_before_n</span>
            
            <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
                <span class="n">new_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">new_shape</span><span class="p">),</span> <span class="n">sparse_dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_sparse_dim</span><span class="p">)</span>
            <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.max">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.max">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Max of non-zero values over specified axis.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        
        <span class="n">axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="n">dim_types</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_dim_type</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="n">has_sparse_reduction</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">dt</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;sparse_m&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse_n&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">dim_types</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">has_sparse_reduction</span><span class="p">:</span>
            <span class="n">dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">dense</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dense</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">val_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values_axis_for_dim</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">)</span>
            <span class="n">new_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">val_axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
            
            <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">keepdim</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
                    <span class="n">new_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                    <span class="k">del</span> <span class="n">new_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
            
            <span class="n">new_sparse_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdim</span><span class="p">:</span>
                <span class="n">removed_before_m</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">removed_before_n</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">new_sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">removed_before_m</span>
                <span class="n">new_sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-=</span> <span class="n">removed_before_n</span>
            
            <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
                <span class="n">new_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">new_shape</span><span class="p">),</span> <span class="n">sparse_dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_sparse_dim</span><span class="p">)</span>
            <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.min">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.min">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Min of non-zero values over specified axis.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        
        <span class="n">axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="n">dim_types</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_dim_type</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>
        <span class="n">has_sparse_reduction</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">dt</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;sparse_m&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse_n&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">dim_types</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">has_sparse_reduction</span><span class="p">:</span>
            <span class="n">dense</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">dense</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span><span class="o">.</span><span class="n">values</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dense</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">val_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_values_axis_for_dim</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">)</span>
            <span class="n">new_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">val_axes</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
            
            <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">keepdim</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
                    <span class="n">new_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                    <span class="k">del</span> <span class="n">new_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
            
            <span class="n">new_sparse_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdim</span><span class="p">:</span>
                <span class="n">removed_before_m</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">removed_before_n</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">new_sparse_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">removed_before_m</span>
                <span class="n">new_sparse_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-=</span> <span class="n">removed_before_n</span>
            
            <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
                <span class="n">new_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">new_shape</span><span class="p">),</span> <span class="n">sparse_dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_sparse_dim</span><span class="p">)</span>
            <span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Element-wise Operations</span>
    <span class="c1"># =========================================================================</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_elementwise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply element-wise function to values.&quot;&quot;&quot;</span>
        <span class="n">new_values</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="n">new_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span> <span class="n">sparse_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="p">)</span>
    
    <span class="c1"># Arithmetic operations</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise addition. For SparseTensor + SparseTensor, patterns must match.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">row_indices</span><span class="p">)</span> <span class="ow">or</span> \
               <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">col_indices</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;SparseTensor addition requires matching sparsity patterns&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">+</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__add__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">row_indices</span><span class="p">)</span> <span class="ow">or</span> \
               <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">col_indices</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;SparseTensor subtraction requires matching sparsity patterns&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">-</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">-</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">other</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise multiplication (Hadamard product for sparse tensors).&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">row_indices</span><span class="p">)</span> <span class="ow">or</span> \
               <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">col_indices</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;SparseTensor multiplication requires matching sparsity patterns&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">*</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__mul__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">row_indices</span><span class="p">)</span> <span class="ow">or</span> \
               <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">col_indices</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;SparseTensor division requires matching sparsity patterns&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">/</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">/</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">other</span> <span class="o">/</span> <span class="n">v</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">//</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">//</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exponent</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">**</span> <span class="n">exponent</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="o">-</span><span class="n">v</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__pos__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__abs__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">)</span>
    
    <span class="c1"># Math functions - directly delegate to values</span>
<div class="viewcode-block" id="SparseTensor.abs">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.abs">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise absolute value.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.sqrt">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.sqrt">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise square root.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.square">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.square">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">square</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise square.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.exp">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.exp">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise exponential.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.log">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.log">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise natural logarithm.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.log10">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.log10">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">log10</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise base-10 logarithm.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.log2">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.log2">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">log2</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise base-2 logarithm.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.sin">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.sin">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise sine.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.cos">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.cos">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cos</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise cosine.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.tan">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.tan">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">tan</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise tangent.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tan</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.sinh">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.sinh">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">sinh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise hyperbolic sine.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sinh</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.cosh">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.cosh">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cosh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise hyperbolic cosine.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.tanh">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.tanh">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise hyperbolic tangent.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.sigmoid">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.sigmoid">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise sigmoid.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.relu">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.relu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise ReLU.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.clamp">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.clamp">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">clamp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise clamp.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="nb">max</span><span class="p">))</span></div>

    
<div class="viewcode-block" id="SparseTensor.sign">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.sign">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">sign</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise sign.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.floor">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.floor">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">floor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise floor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.ceil">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.ceil">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">ceil</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise ceil.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ceil</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.round">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.round">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise round.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.reciprocal">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.reciprocal">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">reciprocal</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise reciprocal (1/x).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.pow">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.pow">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">pow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exponent</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise power.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">exponent</span><span class="p">))</span></div>

    
    <span class="c1"># Comparison operations (return SparseTensor with bool values)</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">==</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__ne__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">!=</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">&lt;=</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">&lt;=</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">&gt;</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">&gt;</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">SparseTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">&gt;=</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="o">&gt;=</span> <span class="n">other</span><span class="p">)</span>
    
    <span class="c1"># Boolean operations</span>
<div class="viewcode-block" id="SparseTensor.logical_not">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.logical_not">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">logical_not</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise logical NOT.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_not</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.logical_and">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.logical_and">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">logical_and</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise logical AND.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">))</span></div>

    
<div class="viewcode-block" id="SparseTensor.logical_or">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.logical_or">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">logical_or</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise logical OR.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">))</span></div>

    
<div class="viewcode-block" id="SparseTensor.logical_xor">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.logical_xor">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">logical_xor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise logical XOR.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">logical_xor</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="p">))</span></div>

    
    <span class="c1"># Type checking</span>
<div class="viewcode-block" id="SparseTensor.isnan">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.isnan">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">isnan</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise isnan check.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.isinf">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.isinf">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">isinf</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise isinf check.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.isfinite">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.isfinite">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">isfinite</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Element-wise isfinite check.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_elementwise</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">)</span></div>

    
    <span class="c1"># Gradient-related</span>
<div class="viewcode-block" id="SparseTensor.detach">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.detach">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">detach</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Detach from computation graph.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
            <span class="n">sparse_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.requires_grad_">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.requires_grad_">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Enable/disable gradient tracking.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">requires_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether gradient tracking is enabled.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">requires_grad</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Gradient of values if available.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">grad</span>
    
<div class="viewcode-block" id="SparseTensor.clone">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.clone">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a copy of this SparseTensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
            <span class="n">sparse_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.contiguous">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.contiguous">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">contiguous</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Make values contiguous in memory.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">SparseTensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
            <span class="n">sparse_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sparse_dim</span>
        <span class="p">)</span></div>

    
    <span class="c1"># =========================================================================</span>
    <span class="c1"># Persistence (I/O)</span>
    <span class="c1"># =========================================================================</span>
    
<div class="viewcode-block" id="SparseTensor.save">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.save">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;os.PathLike&quot;</span><span class="p">],</span>
        <span class="n">metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save SparseTensor to safetensors format.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str or PathLike</span>
<span class="sd">            Output file path (should end with .safetensors).</span>
<span class="sd">        metadata : dict, optional</span>
<span class="sd">            Additional metadata to store.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor(val, row, col, (100, 100))</span>
<span class="sd">        &gt;&gt;&gt; A.save(&quot;matrix.safetensors&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">save_sparse</span>
        <span class="n">save_sparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.load">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.load">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;os.PathLike&quot;</span><span class="p">],</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load SparseTensor from safetensors format.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str or PathLike</span>
<span class="sd">            Input file path.</span>
<span class="sd">        device : str or torch.device</span>
<span class="sd">            Device to load tensors to.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            The loaded sparse tensor.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; A = SparseTensor.load(&quot;matrix.safetensors&quot;, device=&quot;cuda&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_sparse</span>
        <span class="k">return</span> <span class="n">load_sparse</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensor.save_distributed">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensor.save_distributed">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_distributed</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;os.PathLike&quot;</span><span class="p">],</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">partition_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;simple&quot;</span><span class="p">,</span>
        <span class="n">coords</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save as partitioned files for distributed loading.</span>
<span class="sd">        </span>
<span class="sd">        Creates a directory with metadata and per-partition files.</span>
<span class="sd">        Each rank can then load only its own partition.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        directory : str or PathLike</span>
<span class="sd">            Output directory path.</span>
<span class="sd">        num_partitions : int</span>
<span class="sd">            Number of partitions to create.</span>
<span class="sd">        partition_method : str</span>
<span class="sd">            &#39;simple&#39;, &#39;metis&#39;, or &#39;geometric&#39;.</span>
<span class="sd">        coords : torch.Tensor, optional</span>
<span class="sd">            Node coordinates for geometric partitioning.</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            Print progress.</span>
<span class="sd">        </span>
<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; A.save_distributed(&quot;matrix_dist&quot;, num_partitions=4)</span>
<span class="sd">        # Each rank loads its partition:</span>
<span class="sd">        &gt;&gt;&gt; partition = DSparseMatrix.load(&quot;matrix_dist&quot;, rank)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">save_distributed</span>
        <span class="n">save_distributed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">directory</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">,</span> <span class="n">partition_method</span><span class="p">,</span> <span class="n">coords</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span></div>
</div>



<span class="c1"># =============================================================================</span>
<span class="c1"># LUFactorization Class</span>
<span class="c1"># =============================================================================</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LUFactorization</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LU factorization wrapper for efficient repeated solves.</span>
<span class="sd">    </span>
<span class="sd">    Created by SparseTensor.lu().</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lu_factor : scipy.sparse.linalg.SuperLU</span>
<span class="sd">        The SciPy LU factorization object.</span>
<span class="sd">    shape : Tuple[int, int]</span>
<span class="sd">        Matrix shape.</span>
<span class="sd">    dtype : torch.dtype</span>
<span class="sd">        Data type.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        Device.</span>
<span class="sd">    </span>
<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; A = SparseTensor(val, row, col, (10, 10))</span>
<span class="sd">    &gt;&gt;&gt; lu = A.lu()</span>
<span class="sd">    &gt;&gt;&gt; x1 = lu.solve(b1)  # First solve</span>
<span class="sd">    &gt;&gt;&gt; x2 = lu.solve(b2)  # Much faster - reuses factorization</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lu_factor</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lu</span> <span class="o">=</span> <span class="n">lu_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve Ax = b using the cached factorization.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : torch.Tensor</span>
<span class="sd">            Right-hand side vector.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Solution x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
        <span class="n">b_np</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">x_np</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lu</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">b_np</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_np</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;LUFactorization(shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="si">}</span><span class="s2">)&quot;</span>


<span class="c1"># =============================================================================</span>
<span class="c1"># SparseTensorList Class</span>
<span class="c1"># =============================================================================</span>

<div class="viewcode-block" id="SparseTensorList">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SparseTensorList</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A list of SparseTensors with different structures.</span>
<span class="sd">    </span>
<span class="sd">    Provides a unified interface for batch operations on matrices</span>
<span class="sd">    with different sparsity patterns. Unlike batched SparseTensor</span>
<span class="sd">    (which requires same structure), SparseTensorList allows</span>
<span class="sd">    each matrix to have different shape and sparsity pattern.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">    tensors : List[SparseTensor]</span>
<span class="sd">        List of SparseTensor objects.</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    shapes : List[Tuple[int, ...]]</span>
<span class="sd">        List of shapes for each tensor.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        Device (from first tensor).</span>
<span class="sd">    dtype : torch.dtype</span>
<span class="sd">        Data type (from first tensor).</span>
<span class="sd">    </span>
<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; # Create matrices with different sizes</span>
<span class="sd">    &gt;&gt;&gt; A1 = SparseTensor(val1, row1, col1, (10, 10))</span>
<span class="sd">    &gt;&gt;&gt; A2 = SparseTensor(val2, row2, col2, (20, 20))</span>
<span class="sd">    &gt;&gt;&gt; A3 = SparseTensor(val3, row3, col3, (30, 30))</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; # Create list</span>
<span class="sd">    &gt;&gt;&gt; matrices = SparseTensorList([A1, A2, A3])</span>
<span class="sd">    &gt;&gt;&gt; print(matrices.shapes)  # [(10, 10), (20, 20), (30, 30)]</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; # Batch solve</span>
<span class="sd">    &gt;&gt;&gt; x_list = matrices.solve([b1, b2, b3])</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; # Check properties for all</span>
<span class="sd">    &gt;&gt;&gt; is_sym = matrices.is_symmetric()  # [tensor(True), tensor(True), tensor(True)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;SparseTensor&quot;</span><span class="p">]):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">tensors</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;SparseTensorList cannot be empty&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
    
<div class="viewcode-block" id="SparseTensorList.from_coo_list">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.from_coo_list">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_coo_list</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">matrices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensorList&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create from list of COO data tuples.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        matrices : List[Tuple]</span>
<span class="sd">            List of (values, row_indices, col_indices, shape) tuples.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensorList</span>
<span class="sd">            List of SparseTensors.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; data = [</span>
<span class="sd">        ...     (val1, row1, col1, (10, 10)),</span>
<span class="sd">        ...     (val2, row2, col2, (20, 20)),</span>
<span class="sd">        ... ]</span>
<span class="sd">        &gt;&gt;&gt; matrices = SparseTensorList.from_coo_list(data)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">SparseTensor</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">val</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">matrices</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensorList.from_torch_sparse_list">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.from_torch_sparse_list">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_torch_sparse_list</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">A_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensorList&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create from list of PyTorch sparse tensors.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        A_list : List[torch.Tensor]</span>
<span class="sd">            List of PyTorch sparse COO tensors.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensorList</span>
<span class="sd">            List of SparseTensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">SparseTensor</span><span class="o">.</span><span class="n">from_torch_sparse</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="k">for</span> <span class="n">A</span> <span class="ow">in</span> <span class="n">A_list</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></div>

    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of shapes for each tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">]</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Device of the first tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Data type of the first tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of tensors in the list.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get tensor by index.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        idx : int</span>
<span class="sd">            Index (supports negative indexing).</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensor</span>
<span class="sd">            The tensor at that index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">)</span> <span class="o">+</span> <span class="n">idx</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Iterate over tensors.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">)</span>
    
<div class="viewcode-block" id="SparseTensorList.to">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.to">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensorList&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Move all tensors to device.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        device : str or torch.device</span>
<span class="sd">            Target device.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SparseTensorList</span>
<span class="sd">            New list with tensors on target device.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">SparseTensorList</span><span class="p">([</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">])</span></div>

    
<div class="viewcode-block" id="SparseTensorList.cuda">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.cuda">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensorList&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move all tensors to CUDA.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensorList.cpu">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.cpu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SparseTensorList&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move all tensors to CPU.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="SparseTensorList.solve">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.solve">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Solve linear systems for all matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b_list : List[torch.Tensor]</span>
<span class="sd">            List of right-hand side vectors, one per matrix.</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Additional arguments passed to SparseTensor.solve().</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of solutions.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; matrices = SparseTensorList([A1, A2, A3])</span>
<span class="sd">        &gt;&gt;&gt; x_list = matrices.solve([b1, b2, b3])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_list</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">)</span><span class="si">}</span><span class="s2"> RHS vectors, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">b_list</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">,</span> <span class="n">b_list</span><span class="p">)]</span></div>

    
<div class="viewcode-block" id="SparseTensorList.is_symmetric">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.is_symmetric">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_symmetric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check symmetry for all matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Arguments passed to SparseTensor.is_symmetric().</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of boolean tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">is_symmetric</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">]</span></div>

    
<div class="viewcode-block" id="SparseTensorList.is_positive_definite">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.is_positive_definite">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_positive_definite</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check positive definiteness for all matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Arguments passed to SparseTensor.is_positive_definite().</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of boolean tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">is_positive_definite</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">]</span></div>

    
<div class="viewcode-block" id="SparseTensorList.norm">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.norm">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">ord</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;fro&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;fro&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute norms for all matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ord : {&#39;fro&#39;, 1, 2}</span>
<span class="sd">            Norm type.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of norm values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="nb">ord</span><span class="o">=</span><span class="nb">ord</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">]</span></div>

    
<div class="viewcode-block" id="SparseTensorList.eigs">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.eigs">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eigs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute eigenvalues for all matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int</span>
<span class="sd">            Number of eigenvalues.</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Additional arguments.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[Tuple[torch.Tensor, Optional[torch.Tensor]]]</span>
<span class="sd">            List of (eigenvalues, eigenvectors) tuples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">eigs</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">]</span></div>

    
<div class="viewcode-block" id="SparseTensorList.eigsh">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.eigsh">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eigsh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute eigenvalues for symmetric matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int</span>
<span class="sd">            Number of eigenvalues.</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Additional arguments.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[Tuple[torch.Tensor, Optional[torch.Tensor]]]</span>
<span class="sd">            List of (eigenvalues, eigenvectors) tuples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">eigsh</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">]</span></div>

    
<div class="viewcode-block" id="SparseTensorList.svd">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.svd">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">svd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute SVD for all matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        k : int</span>
<span class="sd">            Number of singular values.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]</span>
<span class="sd">            List of (U, S, Vt) tuples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">]</span></div>

    
<div class="viewcode-block" id="SparseTensorList.condition_number">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.condition_number">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">condition_number</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">ord</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute condition numbers for all matrices.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ord : int</span>
<span class="sd">            Norm order.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[torch.Tensor]</span>
<span class="sd">            List of condition numbers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">condition_number</span><span class="p">(</span><span class="nb">ord</span><span class="o">=</span><span class="nb">ord</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">]</span></div>

    
<div class="viewcode-block" id="SparseTensorList.spy">
<a class="viewcode-back" href="../../torch_sla.html#torch_sla.SparseTensorList.spy">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">spy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ncols</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">figsize</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Visualize sparsity patterns for multiple matrices in a grid.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        indices : List[int], optional</span>
<span class="sd">            Which matrices to visualize. Default: all.</span>
<span class="sd">        ncols : int, optional</span>
<span class="sd">            Number of columns in subplot grid. Default: 3.</span>
<span class="sd">        figsize : Tuple[float, float], optional</span>
<span class="sd">            Figure size. Auto-computed if None.</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Additional arguments passed to SparseTensor.spy().</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fig : matplotlib.figure.Figure</span>
<span class="sd">            The figure object.</span>
<span class="sd">            </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; matrices = SparseTensorList([A1, A2, A3, A4])</span>
<span class="sd">        &gt;&gt;&gt; matrices.spy()  # Visualize all in grid</span>
<span class="sd">        &gt;&gt;&gt; matrices.spy(indices=[0, 2])  # Visualize specific ones</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;matplotlib is required for spy(). Install with: pip install matplotlib&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">)))</span>
        
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
        <span class="n">nrows</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">ncols</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">ncols</span>
        
        <span class="k">if</span> <span class="n">figsize</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">ncols</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">nrows</span><span class="p">)</span>
        
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">,</span> <span class="n">squeeze</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
            <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="n">ncols</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">spy</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">show_colorbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">sparse_shape</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1">] </span><span class="si">{</span><span class="n">M</span><span class="si">}</span><span class="s1">×</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">, nnz=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">nnz</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        
        <span class="c1"># Hide unused subplots</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">nrows</span> <span class="o">*</span> <span class="n">ncols</span><span class="p">):</span>
            <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="n">ncols</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">fig</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;SparseTensorList(n=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensors</span><span class="p">)</span><span class="si">}</span><span class="s2">, device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">)&quot;</span></div>

</pre></div>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, walker chi
            </div>
            Made with 
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/walkerchi/torch-sla" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=b14783f5"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>