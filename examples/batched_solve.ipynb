{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batched Solve\n",
        "\n",
        "Batched Solve Examples for torch-sla\n",
        "\n",
        "This example demonstrates:\n",
        "1. Batched SparseTensor with same layout\n",
        "2. solve_batch for different values, same structure\n",
        "3. SparseTensorList for different layouts\n",
        "4. CUDA batched operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import torch\n",
        "from torch_sla import SparseTensor, SparseTensorList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tridiagonal(n: int, dtype=torch.float64, device='cpu'):\n",
        "    \"\"\"Create tridiagonal SPD matrix.\"\"\"\n",
        "    idx = torch.arange(n, device=device)\n",
        "    \n",
        "    # Diagonal entries: all n indices with value 4.0\n",
        "    diag_row = idx\n",
        "    diag_col = idx\n",
        "    diag_val = torch.full((n,), 4.0, dtype=dtype, device=device)\n",
        "    \n",
        "    # Sub-diagonal entries: indices 1 to n-1 with value -1.0\n",
        "    sub_row = idx[1:]\n",
        "    sub_col = idx[:-1]\n",
        "    sub_val = torch.full((n - 1,), -1.0, dtype=dtype, device=device)\n",
        "    \n",
        "    # Super-diagonal entries: indices 0 to n-2 with value -1.0\n",
        "    sup_row = idx[:-1]\n",
        "    sup_col = idx[1:]\n",
        "    sup_val = torch.full((n - 1,), -1.0, dtype=dtype, device=device)\n",
        "    \n",
        "    # Concatenate all entries\n",
        "    row = torch.cat([diag_row, sub_row, sup_row])\n",
        "    col = torch.cat([diag_col, sub_col, sup_col])\n",
        "    val = torch.cat([diag_val, sub_val, sup_val])\n",
        "    \n",
        "    return val, row, col, (n, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 Batched Tensor\n",
        "\n",
        "Batched SparseTensor with same layout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 50\n",
        "val, row, col, shape = create_tridiagonal(n)\n",
        "\n",
        "# Create batch by repeating values with variations\n",
        "batch_size = 8\n",
        "val_batch = val.unsqueeze(0).expand(batch_size, -1).clone()\n",
        "\n",
        "# Vary the diagonal values slightly for each batch\n",
        "for i in range(batch_size):\n",
        "    scale = 1.0 + 0.1 * i\n",
        "    val_batch[i] = val * scale\n",
        "\n",
        "# Create batched SparseTensor\n",
        "A_batch = SparseTensor(\n",
        "    val_batch, row, col, (batch_size, n, n)\n",
        ")\n",
        "print(f\"Batched SparseTensor: {A_batch}\")\n",
        "print(f\"  Shape: {A_batch.shape}\")\n",
        "print(f\"  Batch shape: {A_batch.batch_shape}\")\n",
        "print(f\"  Sparse shape: {A_batch.sparse_shape}\")\n",
        "print(f\"  NNZ: {A_batch.nnz}\")\n",
        "\n",
        "# Batched solve\n",
        "b_batch = torch.randn(batch_size, n, dtype=torch.float64)\n",
        "x_batch = A_batch.solve(b_batch)\n",
        "\n",
        "print(f\"\\nBatched solve:\")\n",
        "print(f\"  Input b shape: {b_batch.shape}\")\n",
        "print(f\"  Output x shape: {x_batch.shape}\")\n",
        "\n",
        "# Verify each solution\n",
        "max_residual = 0\n",
        "for i in range(batch_size):\n",
        "    A_i = SparseTensor(val_batch[i], row, col, (n, n))\n",
        "    residual = (A_i @ x_batch[i] - b_batch[i]).norm() / b_batch[i].norm()\n",
        "    max_residual = max(max_residual, residual.item())\n",
        "print(f\"  Max relative residual: {max_residual:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 Solve Batch\n",
        "\n",
        "solve_batch for different values with same structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 50\n",
        "val, row, col, shape = create_tridiagonal(n)\n",
        "\n",
        "# Create template SparseTensor\n",
        "A = SparseTensor(val, row, col, shape)\n",
        "print(f\"Template matrix: {A}\")\n",
        "\n",
        "# Create batch of values (same structure, different values)\n",
        "batch_size = 16\n",
        "val_batch = val.unsqueeze(0).expand(batch_size, -1).clone()\n",
        "for i in range(batch_size):\n",
        "    val_batch[i] = val * (1.0 + 0.05 * i)\n",
        "\n",
        "b_batch = torch.randn(batch_size, n, dtype=torch.float64)\n",
        "\n",
        "# Use solve_batch - efficient for same structure, different values\n",
        "x_batch = A.solve_batch(val_batch, b_batch)\n",
        "\n",
        "print(f\"\\nsolve_batch:\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Values batch shape: {val_batch.shape}\")\n",
        "print(f\"  RHS batch shape: {b_batch.shape}\")\n",
        "print(f\"  Solution shape: {x_batch.shape}\")\n",
        "\n",
        "# Verify\n",
        "max_residual = 0\n",
        "for i in range(batch_size):\n",
        "    A_i = SparseTensor(val_batch[i], row, col, (n, n))\n",
        "    residual = (A_i @ x_batch[i] - b_batch[i]).norm() / b_batch[i].norm()\n",
        "    max_residual = max(max_residual, residual.item())\n",
        "print(f\"  Max relative residual: {max_residual:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 Sparse Tensor List\n",
        "\n",
        "SparseTensorList for different layouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create matrices with different sizes\n",
        "sizes = [20, 50, 100, 200]\n",
        "tensors = []\n",
        "b_list = []\n",
        "\n",
        "for n in sizes:\n",
        "    val, row, col, shape = create_tridiagonal(n)\n",
        "    A = SparseTensor(val, row, col, shape)\n",
        "    tensors.append(A)\n",
        "    b_list.append(torch.randn(n, dtype=torch.float64))\n",
        "\n",
        "# Create SparseTensorList\n",
        "matrices = SparseTensorList(tensors)\n",
        "print(f\"SparseTensorList: {matrices}\")\n",
        "print(f\"  Length: {len(matrices)}\")\n",
        "print(f\"  Shapes: {matrices.shapes}\")\n",
        "\n",
        "# Batch solve with different layouts\n",
        "x_list = matrices.solve(b_list)\n",
        "\n",
        "print(f\"\\nBatch solve results:\")\n",
        "for i, (A, x, b) in enumerate(zip(matrices, x_list, b_list)):\n",
        "    residual = (A @ x - b).norm() / b.norm()\n",
        "    print(f\"  Matrix {i} ({sizes[i]}x{sizes[i]}): residual = {residual:.2e}\")\n",
        "\n",
        "# Other batch operations\n",
        "norms = matrices.norm('fro')\n",
        "print(f\"\\nFrobenius norms: {[f'{n:.2f}' for n in norms]}\")\n",
        "\n",
        "# Property detection (auto-computed)\n",
        "is_sym_list = matrices.is_symmetric()\n",
        "is_pd_list = matrices.is_positive_definite()\n",
        "print(f\"All symmetric: {all(r.item() for r in is_sym_list)}\")\n",
        "print(f\"All positive definite: {all(r.item() for r in is_pd_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 Batched Eigenvalues\n",
        "\n",
        "Batched eigenvalue computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 30\n",
        "val, row, col, shape = create_tridiagonal(n)\n",
        "\n",
        "batch_size = 4\n",
        "val_batch = val.unsqueeze(0).expand(batch_size, -1).clone()\n",
        "for i in range(batch_size):\n",
        "    val_batch[i] = val * (1.0 + 0.2 * i)\n",
        "\n",
        "A_batch = SparseTensor(val_batch, row, col, (batch_size, n, n))\n",
        "\n",
        "# Batched eigenvalue computation\n",
        "k = 5\n",
        "eigenvalues, eigenvectors = A_batch.eigsh(k=k, which='LM')\n",
        "\n",
        "print(f\"Batched eigsh:\")\n",
        "print(f\"  Eigenvalues shape: {eigenvalues.shape}\")\n",
        "print(f\"  Eigenvectors shape: {eigenvectors.shape}\")\n",
        "print(f\"\\nLargest eigenvalues per batch:\")\n",
        "for i in range(batch_size):\n",
        "    print(f\"  Batch {i}: {eigenvalues[i].tolist()}\")\n",
        "\n",
        "# Batched SVD\n",
        "U, S, Vt = A_batch.svd(k=k)\n",
        "print(f\"\\nBatched SVD:\")\n",
        "print(f\"  U shape: {U.shape}\")\n",
        "print(f\"  S shape: {S.shape}\")\n",
        "print(f\"  Vt shape: {Vt.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 Cuda Batched\n",
        "\n",
        "CUDA batched operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not torch.cuda.is_available():\n",
        "    print(\"CUDA not available, skipping...\")\n",
        "    pass  # skipped in notebook\n",
        "\n",
        "n = 100\n",
        "val, row, col, shape = create_tridiagonal(n, device='cuda')\n",
        "\n",
        "batch_size = 8\n",
        "val_batch = val.unsqueeze(0).expand(batch_size, -1).clone()\n",
        "\n",
        "A_batch = SparseTensor(\n",
        "    val_batch, row, col, (batch_size, n, n)\n",
        ")\n",
        "print(f\"CUDA Batched SparseTensor: {A_batch}\")\n",
        "\n",
        "# Batched solve on CUDA\n",
        "b_batch = torch.randn(batch_size, n, dtype=torch.float64, device='cuda')\n",
        "x_batch = A_batch.solve(b_batch)\n",
        "\n",
        "print(f\"\\nCUDA batched solve:\")\n",
        "print(f\"  Device: {x_batch.device}\")\n",
        "print(f\"  Shape: {x_batch.shape}\")\n",
        "\n",
        "# Batched matmul on CUDA\n",
        "y_batch = A_batch @ b_batch\n",
        "print(f\"  Matmul device: {y_batch.device}\")\n",
        "\n",
        "# Batched eigenvalues on CUDA (uses LOBPCG)\n",
        "eigenvalues, _ = A_batch.eigsh(k=5, which='LM')\n",
        "print(f\"  Eigenvalues device: {eigenvalues.device}\")\n",
        "print(f\"  Largest eigenvalues (batch 0): {eigenvalues[0].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 Multi Batch\n",
        "\n",
        "Multi-dimensional batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 20\n",
        "val, row, col, shape = create_tridiagonal(n)\n",
        "\n",
        "# Create 4D batched tensor [B1, B2, M, N]\n",
        "B1, B2 = 2, 3\n",
        "val_batch = val.unsqueeze(0).unsqueeze(0).expand(B1, B2, -1).clone()\n",
        "\n",
        "A_batch = SparseTensor(val_batch, row, col, (B1, B2, n, n))\n",
        "print(f\"4D Batched SparseTensor: {A_batch}\")\n",
        "print(f\"  Shape: {A_batch.shape}\")\n",
        "print(f\"  Batch shape: {A_batch.batch_shape}\")\n",
        "print(f\"  Batch size: {A_batch.batch_size}\")\n",
        "\n",
        "# Batched operations\n",
        "norms = A_batch.norm('fro')\n",
        "print(f\"\\nBatched norms shape: {norms.shape}\")\n",
        "print(f\"Norms:\\n{norms}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}