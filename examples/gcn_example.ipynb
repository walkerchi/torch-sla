{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gcn Example\n",
        "\n",
        "Graph Convolutional Network (GCN) Example using torch-sla SparseTensor\n",
        "\n",
        "Demonstrates full use of SparseTensor API for graph neural networks:\n",
        "- SparseTensor @ Dense for message passing\n",
        "- Element-wise operations (*, +, -, clamp, abs, etc.)\n",
        "- Reductions (sum, mean, max, min)\n",
        "- Property detection (is_symmetric, is_positive_definite)\n",
        "- solve() for graph regularization\n",
        "\n",
        "GCN layer: H' = σ(A_norm @ H @ W + b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys.path.insert(0, '..')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "\n",
        "from torch_sla import SparseTensor\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Graph Construction using SparseTensor\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sbm_graph(\n",
        "    num_nodes: int = 300,\n",
        "    num_communities: int = 3,\n",
        "    p_intra: float = 0.1,\n",
        "    p_inter: float = 0.01,\n",
        "    seed: int = 42\n",
        ") -> Tuple[SparseTensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Create Stochastic Block Model graph as SparseTensor.\n",
        "    \n",
        "    Returns:\n",
        "        adj: SparseTensor adjacency matrix (with self-loops)\n",
        "        labels: [num_nodes] community labels\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    # Assign community labels\n",
        "    nodes_per_comm = num_nodes // num_communities\n",
        "    labels = torch.arange(num_communities, dtype=torch.long).repeat_interleave(nodes_per_comm)\n",
        "    if len(labels) < num_nodes:\n",
        "        labels = torch.cat([labels, torch.full((num_nodes - len(labels),), num_communities - 1, dtype=torch.long)])\n",
        "    \n",
        "    # Generate edges based on community structure using vectorized operations\n",
        "    # Create upper triangular indices (i < j pairs)\n",
        "    idx = torch.triu_indices(num_nodes, num_nodes, offset=1)\n",
        "    i_idx, j_idx = idx[0], idx[1]\n",
        "    \n",
        "    # Determine probability for each edge based on community membership\n",
        "    same_community = labels[i_idx] == labels[j_idx]\n",
        "    probs = torch.where(same_community, p_intra, p_inter)\n",
        "    \n",
        "    # Sample edges\n",
        "    edge_mask = torch.rand(probs.shape) < probs\n",
        "    src_upper = i_idx[edge_mask]\n",
        "    dst_upper = j_idx[edge_mask]\n",
        "    \n",
        "    # Create undirected edges (both directions)\n",
        "    src_edges = torch.cat([src_upper, dst_upper])\n",
        "    dst_edges = torch.cat([dst_upper, src_upper])\n",
        "    \n",
        "    # Add self-loops\n",
        "    self_loops = torch.arange(num_nodes, dtype=torch.long)\n",
        "    row = torch.cat([src_edges, self_loops])\n",
        "    col = torch.cat([dst_edges, self_loops])\n",
        "    val = torch.ones(row.shape[0], dtype=torch.float32)\n",
        "    \n",
        "    adj = SparseTensor(val, row, col, (num_nodes, num_nodes))\n",
        "    return adj, labels\n",
        "\n",
        "def normalize_adjacency(adj: SparseTensor, mode: str = 'sym') -> SparseTensor:\n",
        "    \"\"\"\n",
        "    Normalize adjacency matrix using SparseTensor operations.\n",
        "    \n",
        "    Args:\n",
        "        adj: SparseTensor adjacency matrix\n",
        "        mode: 'sym' for D^{-1/2}AD^{-1/2}, 'row' for D^{-1}A\n",
        "    \n",
        "    Returns:\n",
        "        Normalized SparseTensor\n",
        "    \"\"\"\n",
        "    N = adj.sparse_shape[0]\n",
        "    row, col = adj.row_indices, adj.col_indices\n",
        "    \n",
        "    # Compute degree using SparseTensor.sum(axis=1) -> row sums\n",
        "    # But we need to handle this manually since sum over sparse dim returns dense\n",
        "    deg = adj.sum(axis=1)  # [N] - sum over columns for each row\n",
        "    \n",
        "    if mode == 'sym':\n",
        "        # D^{-1/2} A D^{-1/2}\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        \n",
        "        # Scale values: val * d_i^{-1/2} * d_j^{-1/2}\n",
        "        scale = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "        new_values = adj.values * scale\n",
        "    else:\n",
        "        # D^{-1} A\n",
        "        deg_inv = deg.pow(-1)\n",
        "        deg_inv[deg_inv == float('inf')] = 0\n",
        "        new_values = adj.values * deg_inv[row]\n",
        "    \n",
        "    return SparseTensor(new_values, row, col, adj.shape)\n",
        "\n",
        "def compute_laplacian(adj: SparseTensor, normalize: bool = True) -> SparseTensor:\n",
        "    \"\"\"\n",
        "    Compute graph Laplacian: L = I - A_norm (normalized) or L = D - A.\n",
        "    \n",
        "    Uses SparseTensor arithmetic operations.\n",
        "    \"\"\"\n",
        "    N = adj.sparse_shape[0]\n",
        "    \n",
        "    if normalize:\n",
        "        # L = I - D^{-1/2}AD^{-1/2}\n",
        "        A_norm = normalize_adjacency(adj, mode='sym')\n",
        "        \n",
        "        # Create identity SparseTensor\n",
        "        I = SparseTensor(\n",
        "            torch.ones(N, dtype=adj.dtype),\n",
        "            torch.arange(N),\n",
        "            torch.arange(N),\n",
        "            (N, N)\n",
        "        )\n",
        "        \n",
        "        # L = I - A_norm: combine indices\n",
        "        L_row = torch.cat([I.row_indices, A_norm.row_indices])\n",
        "        L_col = torch.cat([I.col_indices, A_norm.col_indices])\n",
        "        L_val = torch.cat([I.values, -A_norm.values])\n",
        "        \n",
        "        return SparseTensor(L_val, L_row, L_col, (N, N))\n",
        "    else:\n",
        "        # L = D - A\n",
        "        deg = adj.sum(axis=1)\n",
        "        \n",
        "        # Diagonal entries\n",
        "        diag_row = torch.arange(N)\n",
        "        diag_val = deg\n",
        "        \n",
        "        # Off-diagonal: -A (excluding self-loops)\n",
        "        mask = adj.row_indices != adj.col_indices\n",
        "        off_row = adj.row_indices[mask]\n",
        "        off_col = adj.col_indices[mask]\n",
        "        off_val = -adj.values[mask]\n",
        "        \n",
        "        L_row = torch.cat([diag_row, off_row])\n",
        "        L_col = torch.cat([diag_row, off_col])\n",
        "        L_val = torch.cat([diag_val, off_val])\n",
        "        \n",
        "        return SparseTensor(L_val, L_row, L_col, (N, N))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj_norm: SparseTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [N, in_channels] node features\n",
        "            adj_norm: Normalized adjacency SparseTensor\n",
        "        \n",
        "        Returns:\n",
        "            [N, out_channels] updated features\n",
        "        \"\"\"\n",
        "        # Transform: H @ W\n",
        "        h = x @ self.weight  # [N, out_channels]\n",
        "        \n",
        "        # Message passing using SparseTensor @ Dense\n",
        "        # SparseTensor.__matmul__ handles this natively\n",
        "        out = adj_norm @ h  # Uses SparseTensor._spmv_coo internally\n",
        "        \n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        nn.init.xavier_uniform_(self.att_src)\n",
        "        nn.init.xavier_uniform_(self.att_dst)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj: SparseTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [N, in_channels]\n",
        "            adj: SparseTensor adjacency (edge structure)\n",
        "        \"\"\"\n",
        "        N = x.size(0)\n",
        "        H, C = self.heads, self.out_channels\n",
        "        \n",
        "        # Get edge indices from SparseTensor\n",
        "        row, col = adj.row_indices, adj.col_indices\n",
        "        \n",
        "        # Linear transformation\n",
        "        x = (x @ self.weight).view(N, H, C)\n",
        "        \n",
        "        # Attention scores\n",
        "        alpha_src = (x * self.att_src).sum(dim=-1)  # [N, H]\n",
        "        alpha_dst = (x * self.att_dst).sum(dim=-1)\n",
        "        alpha = F.leaky_relu(alpha_src[row] + alpha_dst[col], 0.2)  # [E, H]\n",
        "        \n",
        "        # Sparse softmax\n",
        "        alpha_max = torch.zeros(N, H, device=x.device, dtype=x.dtype)\n",
        "        alpha_max.scatter_reduce_(0, row.unsqueeze(1).expand(-1, H), alpha, reduce='amax')\n",
        "        alpha = (alpha - alpha_max[row]).exp()\n",
        "        \n",
        "        alpha_sum = torch.zeros(N, H, device=x.device, dtype=x.dtype)\n",
        "        alpha_sum.scatter_add_(0, row.unsqueeze(1).expand(-1, H), alpha)\n",
        "        alpha = alpha / (alpha_sum[row] + 1e-8)\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "        \n",
        "        # Weighted aggregation\n",
        "        out = torch.zeros(N, H, C, device=x.device, dtype=x.dtype)\n",
        "        msg = alpha.unsqueeze(-1) * x[col]\n",
        "        out.scatter_add_(0, row.view(-1, 1, 1).expand(-1, H, C), msg)\n",
        "        \n",
        "        out = out.view(N, H * C) if self.concat else out.mean(dim=1)\n",
        "        \n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj: SparseTensor) -> torch.Tensor:\n",
        "        # Cache normalized adjacency\n",
        "        if self._cached_adj_norm is None:\n",
        "            self._cached_adj_norm = normalize_adjacency(adj, mode='sym')\n",
        "        \n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, self._cached_adj_norm)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        \n",
        "        return self.convs[-1](x, self._cached_adj_norm)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj: SparseTensor) -> torch.Tensor:\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.elu(self.conv1(x, adj))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv2(x, adj)\n",
        "\n",
        "def create_dataset(num_nodes: int = 300, num_features: int = 32, num_classes: int = 3, seed: int = 42):\n",
        "    \"\"\"Create synthetic node classification dataset.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    adj, labels = create_sbm_graph(num_nodes, num_classes, seed=seed)\n",
        "    \n",
        "    # Features with class signal\n",
        "    x = torch.randn(num_nodes, num_features)\n",
        "    for c in range(num_classes):\n",
        "        mask = (labels == c)\n",
        "        x[mask, c * (num_features // num_classes):(c + 1) * (num_features // num_classes)] += 1.5\n",
        "    \n",
        "    # Split\n",
        "    perm = torch.randperm(num_nodes)\n",
        "    n_train, n_val = int(0.6 * num_nodes), int(0.2 * num_nodes)\n",
        "    \n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    \n",
        "    train_mask[perm[:n_train]] = True\n",
        "    val_mask[perm[n_train:n_train + n_val]] = True\n",
        "    test_mask[perm[n_train + n_val:]] = True\n",
        "    \n",
        "    return x, adj, labels, train_mask, val_mask, test_mask\n",
        "\n",
        "def train(model, x, adj, y, masks, epochs=200, lr=0.01, wd=5e-4):\n",
        "    \"\"\"Train and evaluate model.\"\"\"\n",
        "    train_mask, val_mask, test_mask = masks\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    \n",
        "    best_val, best_test = 0, 0\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x, adj)\n",
        "        F.cross_entropy(out[train_mask], y[train_mask]).backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if epoch % 20 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                pred = model(x, adj).argmax(dim=1)\n",
        "                train_acc = (pred[train_mask] == y[train_mask]).float().mean().item()\n",
        "                val_acc = (pred[val_mask] == y[val_mask]).float().mean().item()\n",
        "                test_acc = (pred[test_mask] == y[test_mask]).float().mean().item()\n",
        "                \n",
        "                if val_acc > best_val:\n",
        "                    best_val, best_test = val_acc, test_acc\n",
        "                \n",
        "                print(f\"Epoch {epoch:3d}: Train={train_acc:.3f} Val={val_acc:.3f} Test={test_acc:.3f}\")\n",
        "    \n",
        "    return best_val, best_test\n",
        "\n",
        "class GCNConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Convolution using SparseTensor @ operator.\n",
        "    \n",
        "    H' = A_norm @ H @ W + b\n",
        "    \n",
        "    Uses SparseTensor's native __matmul__ for message passing.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels: int, out_channels: int, bias: bool = True):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(in_channels, out_channels))\n",
        "        self.bias = nn.Parameter(torch.empty(out_channels)) if bias else None\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, adj_norm: SparseTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [N, in_channels] node features\n",
        "            adj_norm: Normalized adjacency SparseTensor\n",
        "        \n",
        "        Returns:\n",
        "            [N, out_channels] updated features\n",
        "        \"\"\"\n",
        "        # Transform: H @ W\n",
        "        h = x @ self.weight  # [N, out_channels]\n",
        "        \n",
        "        # Message passing using SparseTensor @ Dense\n",
        "        # SparseTensor.__matmul__ handles this natively\n",
        "        out = adj_norm @ h  # Uses SparseTensor._spmv_coo internally\n",
        "        \n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias\n",
        "        \n",
        "        return out\n",
        "\n",
        "class GATConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention using SparseTensor for edge indexing.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        heads: int = 1,\n",
        "        concat: bool = True,\n",
        "        dropout: float = 0.0,\n",
        "        bias: bool = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.concat = concat\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.weight = nn.Parameter(torch.empty(in_channels, heads * out_channels))\n",
        "        self.att_src = nn.Parameter(torch.empty(1, heads, out_channels))\n",
        "        self.att_dst = nn.Parameter(torch.empty(1, heads, out_channels))\n",
        "        \n",
        "        out_dim = heads * out_channels if concat else out_channels\n",
        "        self.bias = nn.Parameter(torch.empty(out_dim)) if bias else None\n",
        "        \n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        nn.init.xavier_uniform_(self.att_src)\n",
        "        nn.init.xavier_uniform_(self.att_dst)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, adj: SparseTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [N, in_channels]\n",
        "            adj: SparseTensor adjacency (edge structure)\n",
        "        \"\"\"\n",
        "        N = x.size(0)\n",
        "        H, C = self.heads, self.out_channels\n",
        "        \n",
        "        # Get edge indices from SparseTensor\n",
        "        row, col = adj.row_indices, adj.col_indices\n",
        "        \n",
        "        # Linear transformation\n",
        "        x = (x @ self.weight).view(N, H, C)\n",
        "        \n",
        "        # Attention scores\n",
        "        alpha_src = (x * self.att_src).sum(dim=-1)  # [N, H]\n",
        "        alpha_dst = (x * self.att_dst).sum(dim=-1)\n",
        "        alpha = F.leaky_relu(alpha_src[row] + alpha_dst[col], 0.2)  # [E, H]\n",
        "        \n",
        "        # Sparse softmax\n",
        "        alpha_max = torch.zeros(N, H, device=x.device, dtype=x.dtype)\n",
        "        alpha_max.scatter_reduce_(0, row.unsqueeze(1).expand(-1, H), alpha, reduce='amax')\n",
        "        alpha = (alpha - alpha_max[row]).exp()\n",
        "        \n",
        "        alpha_sum = torch.zeros(N, H, device=x.device, dtype=x.dtype)\n",
        "        alpha_sum.scatter_add_(0, row.unsqueeze(1).expand(-1, H), alpha)\n",
        "        alpha = alpha / (alpha_sum[row] + 1e-8)\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "        \n",
        "        # Weighted aggregation\n",
        "        out = torch.zeros(N, H, C, device=x.device, dtype=x.dtype)\n",
        "        msg = alpha.unsqueeze(-1) * x[col]\n",
        "        out.scatter_add_(0, row.view(-1, 1, 1).expand(-1, H, C), msg)\n",
        "        \n",
        "        out = out.view(N, H * C) if self.concat else out.mean(dim=1)\n",
        "        \n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias\n",
        "        \n",
        "        return out\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    \"\"\"Multi-layer GCN.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
        "        \n",
        "        self._cached_adj_norm = None\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, adj: SparseTensor) -> torch.Tensor:\n",
        "        # Cache normalized adjacency\n",
        "        if self._cached_adj_norm is None:\n",
        "            self._cached_adj_norm = normalize_adjacency(adj, mode='sym')\n",
        "        \n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, self._cached_adj_norm)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        \n",
        "        return self.convs[-1](x, self._cached_adj_norm)\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    \"\"\"Multi-layer GAT.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        heads: int = 8,\n",
        "        dropout: float = 0.6\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, adj: SparseTensor) -> torch.Tensor:\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.elu(self.conv1(x, adj))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv2(x, adj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sparse Tensor Api\n",
        "\n",
        "Demonstrate SparseTensor API for graph learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"SparseTensor API for Graph Learning\")\n",
        "\n",
        "# Create graph as SparseTensor\n",
        "adj, labels = create_sbm_graph(100, 3)\n",
        "print(f\"\\n1. Graph as SparseTensor: {adj}\")\n",
        "\n",
        "# Property detection\n",
        "print(f\"\\n2. Properties:\")\n",
        "print(f\"   adj.is_symmetric() = {adj.is_symmetric().item()}\")\n",
        "print(f\"   adj.nnz = {adj.nnz}\")\n",
        "\n",
        "# Element-wise operations\n",
        "print(f\"\\n3. Element-wise operations:\")\n",
        "adj_scaled = adj * 0.5\n",
        "print(f\"   adj * 0.5 → values: [{adj_scaled.values.min():.2f}, {adj_scaled.values.max():.2f}]\")\n",
        "\n",
        "adj_normalized = normalize_adjacency(adj)\n",
        "print(f\"   Normalized → values: [{adj_normalized.values.min():.3f}, {adj_normalized.values.max():.3f}]\")\n",
        "\n",
        "adj_clipped = adj_normalized.clamp(min=0.01, max=0.5)\n",
        "print(f\"   Clipped → values: [{adj_clipped.values.min():.3f}, {adj_clipped.values.max():.3f}]\")\n",
        "\n",
        "# Reductions\n",
        "print(f\"\\n4. Reductions:\")\n",
        "print(f\"   adj.sum() = {adj.sum().item():.2f}\")\n",
        "print(f\"   adj.mean() = {adj.mean().item():.4f}\")\n",
        "print(f\"   adj.max() = {adj.max().item():.2f}\")\n",
        "\n",
        "# Degree computation via sum\n",
        "degrees = adj.sum(axis=1)\n",
        "print(f\"   Degrees (adj.sum(axis=1)): mean={degrees.mean():.2f}, min={degrees.min():.0f}, max={degrees.max():.0f}\")\n",
        "\n",
        "# SparseTensor @ Dense\n",
        "print(f\"\\n5. Message Passing (SparseTensor @ Dense):\")\n",
        "x = torch.randn(100, 16)\n",
        "h = adj_normalized @ x\n",
        "print(f\"   adj_norm @ x: {x.shape} → {h.shape}\")\n",
        "\n",
        "# Multiple features (matrix)\n",
        "X = torch.randn(100, 32)\n",
        "H = adj_normalized @ X\n",
        "print(f\"   adj_norm @ X: {X.shape} → {H.shape}\")\n",
        "\n",
        "# Laplacian\n",
        "print(f\"\\n6. Graph Laplacian:\")\n",
        "L = compute_laplacian(adj, normalize=True)\n",
        "print(f\"   L = I - A_norm: {L}\")\n",
        "\n",
        "# Graph smoothing with solve\n",
        "print(f\"\\n7. Graph Smoothing (SparseTensor.solve):\")\n",
        "# Create regularized system: (I + αL)\n",
        "alpha = 0.1\n",
        "I = SparseTensor(\n",
        "    torch.ones(100, dtype=torch.float64),\n",
        "    torch.arange(100), torch.arange(100), (100, 100)\n",
        ")\n",
        "L_double = SparseTensor(L.values.double(), L.row_indices, L.col_indices, L.shape)\n",
        "\n",
        "# Combine: I + αL\n",
        "A_row = torch.cat([I.row_indices, L_double.row_indices])\n",
        "A_col = torch.cat([I.col_indices, L_double.col_indices])\n",
        "A_val = torch.cat([I.values, alpha * L_double.values])\n",
        "A = SparseTensor(A_val, A_row, A_col, (100, 100))\n",
        "\n",
        "# Solve\n",
        "b = torch.randn(100, dtype=torch.float64)\n",
        "x_smooth = A.solve(b)\n",
        "residual = (A @ x_smooth - b).norm() / b.norm()\n",
        "print(f\"   Solve (I + {alpha}L)x = b: residual = {residual:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gcn\n",
        "\n",
        "Train GCN using SparseTensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"GCN Training with SparseTensor\")\n",
        "\n",
        "x, adj, y, train_mask, val_mask, test_mask = create_dataset(300, 32, 3)\n",
        "\n",
        "print(f\"\\nDataset:\")\n",
        "print(f\"   Adjacency: {adj}\")\n",
        "print(f\"   Features: {x.shape}\")\n",
        "print(f\"   Classes: {y.max().item() + 1}\")\n",
        "\n",
        "model = GCN(x.size(1), 64, y.max().item() + 1, num_layers=2)\n",
        "print(f\"\\nModel: {sum(p.numel() for p in model.parameters())} params\")\n",
        "\n",
        "print(\"\\nTraining:\")\n",
        "best_val, best_test = train(model, x, adj, y, (train_mask, val_mask, test_mask))\n",
        "print(f\"\\nBest: Val={best_val:.4f}, Test={best_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gat\n",
        "\n",
        "Train GAT using SparseTensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"GAT Training with SparseTensor\")\n",
        "\n",
        "x, adj, y, train_mask, val_mask, test_mask = create_dataset(300, 32, 3)\n",
        "\n",
        "model = GAT(x.size(1), 8, y.max().item() + 1, heads=8)\n",
        "print(f\"Model: {sum(p.numel() for p in model.parameters())} params\")\n",
        "\n",
        "print(\"\\nTraining:\")\n",
        "best_val, best_test = train(model, x, adj, y, (train_mask, val_mask, test_mask), lr=0.005)\n",
        "print(f\"\\nBest: Val={best_val:.4f}, Test={best_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Flow\n",
        "\n",
        "Verify gradients flow through SparseTensor operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Gradient Flow Through SparseTensor\")\n",
        "\n",
        "adj, _ = create_sbm_graph(50, 3)\n",
        "adj_norm = normalize_adjacency(adj)\n",
        "\n",
        "# Features with gradients\n",
        "x = torch.randn(50, 16, requires_grad=True)\n",
        "\n",
        "# Forward: adj @ x\n",
        "h = adj_norm @ x\n",
        "loss = h.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\n   Input: {x.shape}\")\n",
        "print(f\"   Output: {h.shape}\")\n",
        "print(f\"   Gradient: {x.grad.shape}, norm={x.grad.norm():.4f}\")\n",
        "print(\"   ✓ Gradients flow correctly!\")\n",
        "\n",
        "# Element-wise grad\n",
        "val = adj.values.clone().requires_grad_(True)\n",
        "adj_grad = SparseTensor(val, adj.row_indices, adj.col_indices, adj.shape)\n",
        "\n",
        "scaled = adj_grad * 2\n",
        "loss = scaled.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\n   SparseTensor.values gradient: {val.grad.shape}, sum={val.grad.sum():.1f}\")\n",
        "print(\"   ✓ Element-wise gradients work!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
