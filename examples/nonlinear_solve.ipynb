{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nonlinear Solve\n",
        "\n",
        "Nonlinear Solve Examples for torch-sla\n",
        "\n",
        "This example demonstrates:\n",
        "1. Basic nonlinear equation solving with Newton's method\n",
        "2. Nonlinear PDE: heat equation with temperature-dependent conductivity\n",
        "3. Multiple parameters with gradient computation\n",
        "4. Different solvers: Newton, Picard, Anderson\n",
        "5. Using adjoint gradients for optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import torch\n",
        "from torch_sla import SparseTensor, nonlinear_solve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def residual(u, theta):\n",
        "        return u**3 - u - theta\n",
        "\n",
        "    def residual(u, K, alpha, f):\n",
        "        return K @ u + alpha * u**2 - f\n",
        "\n",
        "    def residual(u, a, b, c):\n",
        "        return a * u**2 + b * u + c\n",
        "\n",
        "    def residual(u, alpha):\n",
        "        return u - torch.tanh(alpha * u + 0.5)\n",
        "\n",
        "    def residual(u, alpha, b):\n",
        "        return u**3 + alpha * u - b\n",
        "\n",
        "    def idx(i, j):\n",
        "        return i * ny + j\n",
        "\n",
        "    def residual(u, A, f):\n",
        "        return A @ u + 0.1 * u**3 - f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 Scalar Equation\n",
        "\n",
        "Solve a simple scalar nonlinear equation: u\u00b3 - u - \u03b8 = 0\n",
        "\n",
        "This demonstrates:\n",
        "- Basic nonlinear_solve usage\n",
        "- Adjoint gradient computation\n",
        "- Comparison with analytical gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Solving: u\u00b3 - u - \u03b8 = 0\")\n",
        "\n",
        "def residual(u, theta):\n",
        "    return u**3 - u - theta\n",
        "\n",
        "# Parameter with gradient tracking\n",
        "theta = torch.tensor([2.0], dtype=torch.float64, requires_grad=True)\n",
        "u0 = torch.tensor([1.5], dtype=torch.float64)\n",
        "\n",
        "# Solve\n",
        "u = nonlinear_solve(residual, u0, theta, method='newton', verbose=True)\n",
        "\n",
        "print(f\"\\nSolution: u = {u.item():.10f}\")\n",
        "print(f\"Residual: F(u) = {residual(u, theta).item():.2e}\")\n",
        "\n",
        "# Compute gradient via adjoint method\n",
        "loss = u.sum()\n",
        "loss.backward()\n",
        "\n",
        "# Analytical gradient: du/d\u03b8 = 1 / (3u\u00b2 - 1)\n",
        "analytical_grad = 1.0 / (3 * u.item()**2 - 1)\n",
        "\n",
        "print(f\"\\n\u2202u/\u2202\u03b8 (adjoint):    {theta.grad.item():.10f}\")\n",
        "print(f\"\u2202u/\u2202\u03b8 (analytical): {analytical_grad:.10f}\")\n",
        "print(f\"Error: {abs(theta.grad.item() - analytical_grad):.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 Nonlinear Pde\n",
        "\n",
        "Solve nonlinear 1D heat equation with temperature-dependent conductivity.\n",
        "\n",
        "PDE: -d/dx(k(u) * du/dx) = f\n",
        "where k(u) = 1 + \u03b1*u (conductivity depends on temperature)\n",
        "\n",
        "Discretized: K @ u + \u03b1 * u\u00b2 = f\n",
        "\n",
        "This demonstrates:\n",
        "- SparseTensor.nonlinear_solve interface\n",
        "- Physical problem setup\n",
        "- Gradient sensitivity analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"PDE: -d/dx(k(u) * du/dx) = f, where k(u) = 1 + \u03b1*u\")\n",
        "\n",
        "# Grid setup\n",
        "n = 50\n",
        "h = 1.0 / (n + 1)\n",
        "\n",
        "# Create 1D Laplacian matrix (tridiagonal)\n",
        "diag = 2.0 * torch.ones(n, dtype=torch.float64) / h**2\n",
        "off = -1.0 * torch.ones(n-1, dtype=torch.float64) / h**2\n",
        "\n",
        "row = torch.cat([torch.arange(n), torch.arange(n-1), torch.arange(1, n)])\n",
        "col = torch.cat([torch.arange(n), torch.arange(1, n), torch.arange(n-1)])\n",
        "val = torch.cat([diag, off, off])\n",
        "\n",
        "K = SparseTensor(val, row, col, (n, n))\n",
        "print(f\"Stiffness matrix: {K}\")\n",
        "\n",
        "# Parameters\n",
        "alpha = torch.tensor([0.5], dtype=torch.float64, requires_grad=True)\n",
        "f = torch.ones(n, dtype=torch.float64, requires_grad=True)  # Uniform heat source\n",
        "\n",
        "# Nonlinear residual\n",
        "def residual(u, K, alpha, f):\n",
        "    return K @ u + alpha * u**2 - f\n",
        "\n",
        "# Solve\n",
        "u0 = torch.zeros(n, dtype=torch.float64)\n",
        "u = K.nonlinear_solve(residual, u0, alpha, f, method='newton', verbose=True)\n",
        "\n",
        "# Results\n",
        "print(f\"\\nSolution range: [{u.min().item():.4f}, {u.max().item():.4f}]\")\n",
        "print(f\"Residual norm: {torch.norm(residual(u, K, alpha, f)).item():.2e}\")\n",
        "\n",
        "# Sensitivity analysis\n",
        "total_temp = u.sum()\n",
        "total_temp.backward()\n",
        "\n",
        "print(f\"\\n--- Sensitivity Analysis ---\")\n",
        "print(f\"Total temperature: {total_temp.item():.4f}\")\n",
        "print(f\"\u2202(\u03a3u)/\u2202\u03b1 = {alpha.grad.item():.6f}\")\n",
        "print(f\"  \u2192 Increasing \u03b1 (nonlinearity) {'decreases' if alpha.grad.item() < 0 else 'increases'} temperature\")\n",
        "print(f\"Average \u2202(\u03a3u)/\u2202f = {f.grad.mean().item():.6f}\")\n",
        "print(f\"  \u2192 Each unit increase in source adds ~{f.grad.mean().item():.4f} to total temp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 Multiple Parameters\n",
        "\n",
        "Solve with multiple parameters and verify all gradients.\n",
        "\n",
        "Equation: a*u\u00b2 + b*u + c = 0 (quadratic)\n",
        "\n",
        "This demonstrates:\n",
        "- Multiple parameter gradients\n",
        "- Comparison with analytical gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Solving: a*u\u00b2 + b*u + c = 0\")\n",
        "\n",
        "def residual(u, a, b, c):\n",
        "    return a * u**2 + b * u + c\n",
        "\n",
        "# Parameters (will give u = 2.0 as solution)\n",
        "a = torch.tensor([1.0], dtype=torch.float64, requires_grad=True)\n",
        "b = torch.tensor([-3.0], dtype=torch.float64, requires_grad=True)\n",
        "c = torch.tensor([2.0], dtype=torch.float64, requires_grad=True)\n",
        "u0 = torch.tensor([2.5], dtype=torch.float64)  # Start near solution\n",
        "\n",
        "# Solve\n",
        "u = nonlinear_solve(residual, u0, a, b, c, method='newton', tol=1e-12)\n",
        "\n",
        "print(f\"Solution: u = {u.item():.10f} (expected: 2.0)\")\n",
        "print(f\"Residual: {abs(residual(u, a, b, c).item()):.2e}\")\n",
        "\n",
        "# Compute gradients\n",
        "loss = u.sum()\n",
        "loss.backward()\n",
        "\n",
        "# Analytical gradients via implicit differentiation\n",
        "# F(u, a, b, c) = au\u00b2 + bu + c = 0\n",
        "# dF/du = 2au + b\n",
        "# \u2202u/\u2202a = -u\u00b2 / (2au + b)\n",
        "# \u2202u/\u2202b = -u / (2au + b)\n",
        "# \u2202u/\u2202c = -1 / (2au + b)\n",
        "u_val = u.item()\n",
        "denom = 2 * a.item() * u_val + b.item()\n",
        "\n",
        "analytical = {\n",
        "    'a': -u_val**2 / denom,\n",
        "    'b': -u_val / denom,\n",
        "    'c': -1 / denom\n",
        "}\n",
        "\n",
        "print(f\"\\n--- Gradient Verification ---\")\n",
        "print(f\"{'Param':<6} {'Adjoint':<15} {'Analytical':<15} {'Error':<10}\")\n",
        "for name, (grad, ana) in zip(['a', 'b', 'c'], \n",
        "                               [(a.grad.item(), analytical['a']),\n",
        "                                (b.grad.item(), analytical['b']),\n",
        "                                (c.grad.item(), analytical['c'])]):\n",
        "    print(f\"{name:<6} {grad:<15.10f} {ana:<15.10f} {abs(grad-ana):<10.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 Solver Comparison\n",
        "\n",
        "Compare different nonlinear solvers: Newton, Picard, Anderson.\n",
        "\n",
        "This demonstrates:\n",
        "- Different solver methods\n",
        "- Convergence characteristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Solving: u - tanh(1.5*u + 0.5) = 0\")\n",
        "\n",
        "def residual(u, alpha):\n",
        "    return u - torch.tanh(alpha * u + 0.5)\n",
        "\n",
        "alpha = torch.tensor([1.5], dtype=torch.float64)\n",
        "u0 = torch.tensor([0.0], dtype=torch.float64)\n",
        "\n",
        "methods = ['newton', 'picard', 'anderson']\n",
        "results = {}\n",
        "\n",
        "for method in methods:\n",
        "    print(f\"\\n--- {method.upper()} ---\")\n",
        "    u = nonlinear_solve(\n",
        "        residual, u0, alpha, \n",
        "        method=method, \n",
        "        verbose=True, \n",
        "        max_iter=100,\n",
        "        tol=1e-10\n",
        "    )\n",
        "    F = residual(u, alpha)\n",
        "    results[method] = (u.item(), abs(F.item()))\n",
        "    print(f\"Solution: u = {u.item():.6f}, Residual: {abs(F.item()):.2e}\")\n",
        "\n",
        "print(f\"\\n--- Summary ---\")\n",
        "print(f\"{'Method':<10} {'Solution':<15} {'Residual':<15}\")\n",
        "for method, (sol, res) in results.items():\n",
        "    print(f\"{method:<10} {sol:<15.6f} {res:<15.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 Optimization\n",
        "\n",
        "Use adjoint gradients for parameter optimization.\n",
        "\n",
        "Goal: Find parameter \u03b1 such that the solution u matches a target.\n",
        "\n",
        "This demonstrates:\n",
        "- Integration with PyTorch optimizers\n",
        "- Inverse problem solving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Goal: Find \u03b1 such that u(\u03b1) \u2248 target\")\n",
        "\n",
        "# Setup: simple nonlinear equation u\u00b3 + \u03b1*u = b\n",
        "def residual(u, alpha, b):\n",
        "    return u**3 + alpha * u - b\n",
        "\n",
        "# True parameter and target\n",
        "true_alpha = torch.tensor([2.0], dtype=torch.float64)\n",
        "b = torch.tensor([3.0], dtype=torch.float64)\n",
        "u0 = torch.tensor([1.0], dtype=torch.float64)\n",
        "\n",
        "target = nonlinear_solve(residual, u0, true_alpha, b, verbose=False)\n",
        "print(f\"True \u03b1 = {true_alpha.item():.4f}\")\n",
        "print(f\"Target u = {target.item():.6f}\")\n",
        "\n",
        "# Optimization: find \u03b1 from observations\n",
        "alpha_guess = torch.tensor([0.5], dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "print(f\"\\n--- Gradient Descent ---\")\n",
        "print(f\"Initial guess: \u03b1 = {alpha_guess.item():.4f}\")\n",
        "\n",
        "lr = 2.0  # Larger learning rate for faster convergence\n",
        "for epoch in range(30):\n",
        "    # Forward solve\n",
        "    u = nonlinear_solve(residual, u0, alpha_guess, b, verbose=False)\n",
        "    \n",
        "    # Loss: match target\n",
        "    loss = (u - target)**2\n",
        "    \n",
        "    # Backward via adjoint\n",
        "    loss.backward()\n",
        "    \n",
        "    # Manual gradient descent (to avoid optimizer state issues)\n",
        "    with torch.no_grad():\n",
        "        alpha_guess -= lr * alpha_guess.grad\n",
        "        alpha_guess.grad.zero_()\n",
        "    \n",
        "    if epoch % 5 == 0 or epoch == 29:\n",
        "        print(f\"Epoch {epoch:3d}: \u03b1 = {alpha_guess.item():.6f}, Loss = {loss.item():.6e}\")\n",
        "\n",
        "print(f\"\\n--- Result ---\")\n",
        "print(f\"Recovered \u03b1 = {alpha_guess.item():.6f}\")\n",
        "print(f\"True \u03b1      = {true_alpha.item():.6f}\")\n",
        "print(f\"Error       = {abs(alpha_guess.item() - true_alpha.item()):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 Sparse Nonlinear\n",
        "\n",
        "Solve a larger nonlinear system using SparseTensor.\n",
        "\n",
        "This demonstrates:\n",
        "- Scaling to larger problems\n",
        "- Practical FEM-like setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2D Poisson-like problem on a grid\n",
        "nx, ny = 20, 20\n",
        "n = nx * ny\n",
        "print(f\"Grid: {nx} x {ny} = {n} DOF\")\n",
        "\n",
        "# Create 2D Laplacian (5-point stencil)\n",
        "def idx(i, j):\n",
        "    return i * ny + j\n",
        "\n",
        "rows, cols, vals = [], [], []\n",
        "\n",
        "for i in range(nx):\n",
        "    for j in range(ny):\n",
        "        k = idx(i, j)\n",
        "        # Diagonal\n",
        "        rows.append(k)\n",
        "        cols.append(k)\n",
        "        vals.append(4.0)\n",
        "        \n",
        "        # Off-diagonals\n",
        "        if i > 0:\n",
        "            rows.append(k)\n",
        "            cols.append(idx(i-1, j))\n",
        "            vals.append(-1.0)\n",
        "        if i < nx - 1:\n",
        "            rows.append(k)\n",
        "            cols.append(idx(i+1, j))\n",
        "            vals.append(-1.0)\n",
        "        if j > 0:\n",
        "            rows.append(k)\n",
        "            cols.append(idx(i, j-1))\n",
        "            vals.append(-1.0)\n",
        "        if j < ny - 1:\n",
        "            rows.append(k)\n",
        "            cols.append(idx(i, j+1))\n",
        "            vals.append(-1.0)\n",
        "\n",
        "row = torch.tensor(rows)\n",
        "col = torch.tensor(cols)\n",
        "val = torch.tensor(vals, dtype=torch.float64)\n",
        "\n",
        "A = SparseTensor(val, row, col, (n, n))\n",
        "print(f\"Matrix: {A}\")\n",
        "print(f\"Sparsity: {100 * (1 - A.nnz / n**2):.1f}%\")\n",
        "\n",
        "# Nonlinear problem: A @ u + 0.1*u\u00b3 = f\n",
        "f = torch.randn(n, dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "def residual(u, A, f):\n",
        "    return A @ u + 0.1 * u**3 - f\n",
        "\n",
        "u0 = torch.zeros(n, dtype=torch.float64)\n",
        "\n",
        "print(\"\\nSolving nonlinear system...\")\n",
        "u = A.nonlinear_solve(residual, u0, f, method='newton', verbose=True)\n",
        "\n",
        "F = residual(u, A, f)\n",
        "print(f\"\\nResidual norm: {torch.norm(F).item():.2e}\")\n",
        "print(f\"Solution range: [{u.min().item():.4f}, {u.max().item():.4f}]\")\n",
        "\n",
        "# Gradient\n",
        "loss = u.sum()\n",
        "loss.backward()\n",
        "print(f\"||\u2202L/\u2202f||: {torch.norm(f.grad).item():.6f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}