{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spsolve\n",
        "\n",
        "Example: Sparse Linear Solve with torch-sla\n",
        "\n",
        "This example demonstrates how to use different backends for solving\n",
        "sparse linear equations Ax = b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import torch\n",
        "import torch_sla as sla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_spd_matrix(n: int, density: float = 0.3, device: str = 'cpu'):\n",
        "    \"\"\"Create a sparse symmetric positive definite matrix\"\"\"\n",
        "    A = torch.rand(n, n, dtype=torch.float64, device=device)\n",
        "    A = A @ A.T + torch.eye(n, dtype=torch.float64, device=device) * n\n",
        "    A[A.abs() < (1 - density)] = 0\n",
        "    return A.to_sparse_coo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cpu Solvers\n",
        "\n",
        "Example using CPU iterative solvers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"CPU Iterative Solvers\")\n",
        "\n",
        "n = 100\n",
        "A = create_spd_matrix(n, density=0.3, device='cpu')\n",
        "b = torch.randn(n, dtype=torch.float64)\n",
        "\n",
        "# Conjugate Gradient (for SPD matrices)\n",
        "x_cg = sla.spsolve(\n",
        "    A.values(), A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='cg', atol=1e-10, maxiter=10000\n",
        ")\n",
        "\n",
        "# BiCGStab (for general matrices)\n",
        "x_bicg = sla.spsolve(\n",
        "    A.values(), A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='bicgstab', atol=1e-10, maxiter=10000\n",
        ")\n",
        "\n",
        "# Verify solutions\n",
        "A_dense = A.to_dense()\n",
        "residual_cg = (torch.mv(A_dense, x_cg) - b).norm() / b.norm()\n",
        "residual_bicg = (torch.mv(A_dense, x_bicg) - b).norm() / b.norm()\n",
        "\n",
        "print(f\"Matrix size: {n}x{n}, NNZ: {A._nnz()}\")\n",
        "print(f\"CG relative residual: {residual_cg:.2e}\")\n",
        "print(f\"BiCGStab relative residual: {residual_bicg:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cusolver\n",
        "\n",
        "Example using cuSOLVER direct solvers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"cuSOLVER Direct Solvers\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"CUDA not available, skipping cuSOLVER example\")\n",
        "    pass  # skipped in notebook\n",
        "\n",
        "if not sla.is_cusolver_available():\n",
        "    print(\"cuSOLVER backend not available, skipping\")\n",
        "    pass  # skipped in notebook\n",
        "\n",
        "n = 100\n",
        "A = create_spd_matrix(n, density=0.3, device='cuda')\n",
        "b = torch.randn(n, dtype=torch.float64, device='cuda')\n",
        "\n",
        "# QR decomposition\n",
        "x_qr = sla.spsolve(\n",
        "    A.values(), A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='cusolver_qr', tol=1e-12\n",
        ")\n",
        "\n",
        "# Cholesky decomposition (for SPD matrices)\n",
        "x_chol = sla.spsolve(\n",
        "    A.values(), A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='cusolver_cholesky', tol=1e-12\n",
        ")\n",
        "\n",
        "# LU decomposition\n",
        "x_lu = sla.spsolve(\n",
        "    A.values(), A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='cusolver_lu', tol=1e-12\n",
        ")\n",
        "\n",
        "# Verify solutions\n",
        "A_dense = A.to_dense()\n",
        "residual_qr = (torch.mv(A_dense, x_qr) - b).norm() / b.norm()\n",
        "residual_chol = (torch.mv(A_dense, x_chol) - b).norm() / b.norm()\n",
        "residual_lu = (torch.mv(A_dense, x_lu) - b).norm() / b.norm()\n",
        "\n",
        "print(f\"Matrix size: {n}x{n}, NNZ: {A._nnz()}\")\n",
        "print(f\"QR relative residual: {residual_qr:.2e}\")\n",
        "print(f\"Cholesky relative residual: {residual_chol:.2e}\")\n",
        "print(f\"LU relative residual: {residual_lu:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cudss\n",
        "\n",
        "Example using cuDSS direct solvers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"cuDSS Direct Solvers\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"CUDA not available, skipping cuDSS example\")\n",
        "    pass  # skipped in notebook\n",
        "\n",
        "if not sla.is_cudss_available():\n",
        "    print(\"cuDSS backend not available, skipping\")\n",
        "    pass  # skipped in notebook\n",
        "\n",
        "n = 100\n",
        "A = create_spd_matrix(n, density=0.3, device='cuda')\n",
        "b = torch.randn(n, dtype=torch.float64, device='cuda')\n",
        "\n",
        "# LU factorization\n",
        "x_lu = sla.spsolve(\n",
        "    A.values(), A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='cudss_lu'\n",
        ")\n",
        "\n",
        "# Cholesky factorization (for SPD matrices)\n",
        "x_chol = sla.spsolve(\n",
        "    A.values(), A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='cudss_cholesky'\n",
        ")\n",
        "\n",
        "# LDLT factorization (for symmetric matrices)\n",
        "x_ldlt = sla.spsolve(\n",
        "    A.values(), A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='cudss_ldlt'\n",
        ")\n",
        "\n",
        "# Verify solutions\n",
        "A_dense = A.to_dense()\n",
        "residual_lu = (torch.mv(A_dense, x_lu) - b).norm() / b.norm()\n",
        "residual_chol = (torch.mv(A_dense, x_chol) - b).norm() / b.norm()\n",
        "residual_ldlt = (torch.mv(A_dense, x_ldlt) - b).norm() / b.norm()\n",
        "\n",
        "print(f\"Matrix size: {n}x{n}, NNZ: {A._nnz()}\")\n",
        "print(f\"LU relative residual: {residual_lu:.2e}\")\n",
        "print(f\"Cholesky relative residual: {residual_chol:.2e}\")\n",
        "print(f\"LDLT relative residual: {residual_ldlt:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient\n",
        "\n",
        "Example showing gradient computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Gradient Computation Example\")\n",
        "\n",
        "n = 50\n",
        "\n",
        "# Create sparse matrix with gradient tracking\n",
        "A_dense = torch.rand(n, n, dtype=torch.float64)\n",
        "A_dense = A_dense @ A_dense.T + torch.eye(n, dtype=torch.float64) * n\n",
        "A_dense[A_dense.abs() < 0.7] = 0\n",
        "A = A_dense.to_sparse_coo()\n",
        "\n",
        "val = A.values().clone().requires_grad_(True)\n",
        "b = torch.randn(n, dtype=torch.float64).requires_grad_(True)\n",
        "\n",
        "# Solve with gradient\n",
        "x = sla.spsolve(\n",
        "    val, A.indices()[0], A.indices()[1], A.shape, b,\n",
        "    method='bicgstab', atol=1e-10\n",
        ")\n",
        "\n",
        "# Compute loss and backpropagate\n",
        "loss = (x ** 2).sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"Matrix size: {n}x{n}, NNZ: {A._nnz()}\")\n",
        "print(f\"Solution norm: {x.norm():.4f}\")\n",
        "print(f\"Gradient w.r.t. values: shape={val.grad.shape}, norm={val.grad.norm():.4f}\")\n",
        "print(f\"Gradient w.r.t. b: shape={b.grad.shape}, norm={b.grad.norm():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convenience Functions\n",
        "\n",
        "Example using convenience functions with PyTorch sparse tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Convenience Functions\")\n",
        "\n",
        "n = 50\n",
        "A = create_spd_matrix(n, density=0.3, device='cpu')\n",
        "b = torch.randn(n, dtype=torch.float64)\n",
        "\n",
        "# Using spsolve_coo with sparse COO tensor\n",
        "x1 = sla.spsolve_coo(A, b, method='bicgstab')\n",
        "\n",
        "# Using spsolve_csr with sparse CSR tensor\n",
        "A_csr = A.to_sparse_csr()\n",
        "x2 = sla.spsolve_csr(A_csr, b, method='bicgstab')\n",
        "\n",
        "print(f\"Matrix size: {n}x{n}\")\n",
        "print(f\"Solution via spsolve_coo: norm={x1.norm():.4f}\")\n",
        "print(f\"Solution via spsolve_csr: norm={x2.norm():.4f}\")\n",
        "print(f\"Solutions match: {torch.allclose(x1, x2, rtol=1e-5)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}