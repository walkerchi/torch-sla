{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SuiteSparse Matrix Collection with torch-sla\n",
        "\n",
        "This notebook demonstrates how to use **torch-sla** to solve linear systems from the [SuiteSparse Matrix Collection](https://sparse.tamu.edu/) (formerly University of Florida Sparse Matrix Collection).\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Download matrices directly from SuiteSparse\n",
        "- Load Matrix Market (.mtx) format files\n",
        "- Solve `Ax = b` with multiple backends\n",
        "- Compare iterative vs direct solvers\n",
        "- Full gradient support via autograd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "sys.path.insert(0, '../benchmarks')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "from torch_sla import SparseTensor\n",
        "from torch_sla.io import load_mtx, load_mtx_info\n",
        "\n",
        "# Check CUDA availability\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Downloading SuiteSparse Matrices\n",
        "\n",
        "We provide a utility to download matrices from the SuiteSparse Matrix Collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark_suitesparse import download_matrix, list_available_matrices, POPULAR_MATRICES\n",
        "\n",
        "# List some popular matrices\n",
        "print(\"Some popular matrices for benchmarking:\")\n",
        "print(\"=\"*50)\n",
        "for name, (group, _) in list(POPULAR_MATRICES.items())[:10]:\n",
        "    print(f\"  {name:<15} (group: {group})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download a matrix\n",
        "mtx_path = download_matrix(\"bcsstk01\")\n",
        "\n",
        "# View matrix info without loading\n",
        "info = load_mtx_info(mtx_path)\n",
        "print(f\"\\nMatrix info:\")\n",
        "for key, val in info.items():\n",
        "    print(f\"  {key}: {val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading and Solving with torch-sla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load matrix\n",
        "A = load_mtx(mtx_path, dtype=torch.float64, device=device)\n",
        "\n",
        "print(f\"Loaded SparseTensor:\")\n",
        "print(f\"  Shape: {A.shape}\")\n",
        "print(f\"  NNZ: {A.nnz}\")\n",
        "print(f\"  Device: {A.device}\")\n",
        "print(f\"  Dtype: {A.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create RHS vector (b = A @ x_true, so we know the exact solution)\n",
        "n = A.shape[0]\n",
        "x_true = torch.ones(n, dtype=torch.float64, device=device)\n",
        "b = A @ x_true\n",
        "\n",
        "# Solve Ax = b (auto-selects best backend)\n",
        "x = A.solve(b)\n",
        "\n",
        "# Check error\n",
        "error = torch.norm(x - x_true) / torch.norm(x_true)\n",
        "print(f\"Relative error: {error.item():.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comparing Backends and Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_solver(A, b, backend, method, num_runs=3):\n",
        "    \"\"\"Benchmark a solver configuration.\"\"\"\n",
        "    # Warmup\n",
        "    x = A.solve(b, backend=backend, method=method)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    # Benchmark\n",
        "    times = []\n",
        "    for _ in range(num_runs):\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        start = time.perf_counter()\n",
        "        x = A.solve(b, backend=backend, method=method)\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        times.append((time.perf_counter() - start) * 1000)\n",
        "    \n",
        "    # Compute error\n",
        "    residual = A @ x - b\n",
        "    rel_error = (torch.norm(residual) / torch.norm(b)).item()\n",
        "    \n",
        "    return {\n",
        "        'backend': backend,\n",
        "        'method': method,\n",
        "        'time_ms': np.mean(times),\n",
        "        'rel_error': rel_error,\n",
        "    }\n",
        "\n",
        "# Test different configurations\n",
        "if device == 'cuda':\n",
        "    configs = [('cudss', 'cholesky'), ('cudss', 'lu'), ('pytorch', 'cg'), ('pytorch', 'bicgstab')]\n",
        "else:\n",
        "    configs = [('scipy', 'superlu'), ('pytorch', 'cg'), ('pytorch', 'bicgstab')]\n",
        "\n",
        "print(f\"{'Backend':<12} {'Method':<12} {'Time (ms)':<12} {'Rel Error':<12}\")\n",
        "print(\"-\" * 48)\n",
        "\n",
        "results = []\n",
        "for backend, method in configs:\n",
        "    try:\n",
        "        r = benchmark_solver(A, b, backend, method)\n",
        "        results.append(r)\n",
        "        print(f\"{r['backend']:<12} {r['method']:<12} {r['time_ms']:<12.2f} {r['rel_error']:<12.2e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{backend:<12} {method:<12} {'FAILED':<12} {str(e)[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Larger Matrix: apache1 (80,800 Ã— 80,800)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and load a larger matrix\n",
        "apache_path = download_matrix(\"apache1\")\n",
        "apache_info = load_mtx_info(apache_path)\n",
        "\n",
        "print(f\"Matrix: apache1\")\n",
        "print(f\"  Shape: {apache_info['shape']}\")\n",
        "print(f\"  NNZ: {apache_info['nnz']}\")\n",
        "print(f\"  Symmetry: {apache_info['symmetry']}\")\n",
        "\n",
        "# Load to device\n",
        "A_large = load_mtx(apache_path, dtype=torch.float64, device=device)\n",
        "print(f\"\\nLoaded: shape={A_large.shape}, nnz={A_large.nnz}\")\n",
        "\n",
        "# Create problem\n",
        "n = A_large.shape[0]\n",
        "x_true = torch.ones(n, dtype=torch.float64, device=device)\n",
        "b_large = A_large @ x_true\n",
        "\n",
        "# Benchmark on large matrix\n",
        "print(f\"\\nBenchmark on apache1 ({n:,} DOF, {A_large.nnz:,} NNZ):\")\n",
        "print(f\"{'Backend':<12} {'Method':<12} {'Time (ms)':<12} {'Rel Error':<12}\")\n",
        "print(\"-\" * 48)\n",
        "\n",
        "for backend, method in configs:\n",
        "    try:\n",
        "        r = benchmark_solver(A_large, b_large, backend, method, num_runs=3)\n",
        "        print(f\"{r['backend']:<12} {r['method']:<12} {r['time_ms']:<12.2f} {r['rel_error']:<12.2e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{backend:<12} {method:<12} {'FAILED':<12} {str(e)[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gradient Support (Differentiable Solving)\n",
        "\n",
        "torch-sla supports automatic differentiation through the solve operation using the adjoint method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Small matrix for gradient demo\n",
        "A_small = load_mtx(download_matrix(\"bcsstk01\"), dtype=torch.float64, device=device)\n",
        "n = A_small.shape[0]\n",
        "\n",
        "# Create b with gradient tracking\n",
        "b_grad = torch.randn(n, dtype=torch.float64, device=device, requires_grad=True)\n",
        "\n",
        "# Solve with autograd\n",
        "x = A_small.solve(b_grad)\n",
        "\n",
        "# Compute loss and backprop\n",
        "loss = x.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"Input b shape: {b_grad.shape}\")\n",
        "print(f\"Solution x shape: {x.shape}\")\n",
        "print(f\"Gradient db shape: {b_grad.grad.shape}\")\n",
        "print(f\"\\nGradient norm: {b_grad.grad.norm().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization\n",
        "\n",
        "Visualize the sparsity pattern and benchmark results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sparsity pattern\n",
        "def plot_spy(A, title=\"Sparsity Pattern\"):\n",
        "    \"\"\"Plot the sparsity pattern of a SparseTensor.\"\"\"\n",
        "    row = A.row_indices.cpu().numpy()\n",
        "    col = A.col_indices.cpu().numpy()\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.scatter(col, row, s=2, c='navy', marker='s')\n",
        "    ax.set_xlim(-0.5, A.shape[1] - 0.5)\n",
        "    ax.set_ylim(A.shape[0] - 0.5, -0.5)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('Column')\n",
        "    ax.set_ylabel('Row')\n",
        "    ax.set_title(f\"{title}\\nShape: {A.shape}, NNZ: {A.nnz}\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "fig = plot_spy(A_small, \"bcsstk01 - Stiffness Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "1. **torch-sla** provides a unified interface for solving sparse linear systems\n",
        "2. **Multiple backends**: scipy (CPU), cudss (CUDA direct), pytorch (CUDA iterative)\n",
        "3. **For small-medium problems (<2M DOF)**: Use `cudss+cholesky` for SPD matrices\n",
        "4. **For large problems (>2M DOF)**: Use `pytorch+cg` - scales to 100M+ DOF\n",
        "5. **Full gradient support**: Differentiable solving via adjoint method\n",
        "6. **Trade-off**: Direct solvers = machine precision (~1e-15), Iterative = ~1e-6 but much faster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "rix"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
