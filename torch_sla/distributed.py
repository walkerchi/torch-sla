"""
Distributed Sparse Matrix for large-scale CFD/FEM computations.

Provides domain decomposition with halo exchange, following the standard
approach used in Ansys, OpenFOAM, and other industrial CFD/FEM solvers.

Key Features:
- Graph-based partitioning (METIS or simple geometric methods)
- Halo/ghost node exchange for parallel computations
- Support for both CPU and CUDA devices
- Same API as SparseTensor for easy migration

Example
-------
>>> from torch_sla import DSparseMatrix
>>> 
>>> # Create from global matrix
>>> A_global = SparseTensor(val, row, col, shape)
>>> A_dist = DSparseMatrix.from_global(A_global, num_partitions=4)
>>> 
>>> # Distributed solve
>>> x_dist = A_dist.solve(b_dist)
>>> 
>>> # Halo exchange for iterative methods
>>> A_dist.halo_exchange(local_x)
"""

import os
import torch
from typing import Tuple, List, Dict, Optional, Union, Literal
from dataclasses import dataclass
import warnings

from .backends import (
    is_scipy_available,
    is_eigen_available,
    is_cusolver_available,
    is_cudss_available,
    select_backend,
    select_method,
    BackendType,
    MethodType,
)

try:
    import torch.distributed as dist
    DIST_AVAILABLE = True
except ImportError:
    DIST_AVAILABLE = False

# DTensor support (PyTorch 2.0+)
try:
    from torch.distributed.tensor import DTensor
    from torch.distributed.tensor.placement_types import Shard, Replicate
    DTENSOR_AVAILABLE = True
except ImportError:
    try:
        # Older import path (PyTorch 2.0-2.1)
        from torch.distributed._tensor import DTensor
        from torch.distributed._tensor.placement_types import Shard, Replicate
        DTENSOR_AVAILABLE = True
    except ImportError:
        DTENSOR_AVAILABLE = False
        DTensor = None
        Shard = None
        Replicate = None


def _is_dtensor(x) -> bool:
    """Check if x is a DTensor instance."""
    if not DTENSOR_AVAILABLE or DTensor is None:
        return False
    return isinstance(x, DTensor)


@dataclass
class Partition:
    """Represents a single partition/subdomain"""
    partition_id: int
    local_nodes: torch.Tensor      # Global indices of local nodes
    owned_nodes: torch.Tensor      # Nodes owned by this partition (not halo)
    halo_nodes: torch.Tensor       # Ghost/halo nodes from neighbors
    neighbor_partitions: List[int] # Neighboring partition IDs
    send_indices: Dict[int, torch.Tensor]  # Nodes to send to each neighbor
    recv_indices: Dict[int, torch.Tensor]  # Where to place received data
    global_to_local: torch.Tensor  # Mapping from global to local indices
    local_to_global: torch.Tensor  # Mapping from local to global indices
    

def partition_graph_metis(
    row: torch.Tensor,
    col: torch.Tensor,
    num_nodes: int,
    num_parts: int
) -> torch.Tensor:
    """
    Partition graph using METIS (if available) or fallback to simple method.
    
    Returns
    -------
    partition_ids : torch.Tensor
        Partition ID for each node [num_nodes]
    """
    try:
        import pymetis
        # Build adjacency list
        adjacency = [[] for _ in range(num_nodes)]
        row_cpu = row.cpu().numpy()
        col_cpu = col.cpu().numpy()
        
        for r, c in zip(row_cpu, col_cpu):
            if r != c:  # Skip diagonal
                adjacency[r].append(c)
        
        # Run METIS
        _, membership = pymetis.part_graph(num_parts, adjacency=adjacency)
        return torch.tensor(membership, dtype=torch.int64)
    
    except ImportError:
        warnings.warn("pymetis not available, using simple geometric partitioning")
        return partition_simple(num_nodes, num_parts)


def partition_simple(num_nodes: int, num_parts: int) -> torch.Tensor:
    """Simple 1D partitioning (fallback when METIS not available) - vectorized."""
    nodes_per_part = (num_nodes + num_parts - 1) // num_parts
    idx = torch.arange(num_nodes, dtype=torch.int64)
    partition_ids = torch.clamp(idx // nodes_per_part, max=num_parts - 1)
    return partition_ids


def partition_coordinates(
    coords: torch.Tensor,
    num_parts: int,
    method: str = 'rcb'
) -> torch.Tensor:
    """
    Partition based on node coordinates using Recursive Coordinate Bisection (RCB).
    
    This is common in CFD/FEM for mesh partitioning.
    
    Parameters
    ----------
    coords : torch.Tensor
        Node coordinates [num_nodes, dim]
    num_parts : int
        Number of partitions (should be power of 2 for RCB)
    method : str
        'rcb': Recursive Coordinate Bisection
        'slicing': Simple slicing along longest axis
        
    Returns
    -------
    partition_ids : torch.Tensor
        Partition ID for each node
    """
    num_nodes = coords.size(0)
    partition_ids = torch.zeros(num_nodes, dtype=torch.int64)
    
    if method == 'rcb':
        _rcb_partition(coords, partition_ids, torch.arange(num_nodes), 0, num_parts)
    else:  # slicing
        # Find longest axis
        ranges = coords.max(0).values - coords.min(0).values
        axis = ranges.argmax().item()
        
        # Sort by that axis
        sorted_idx = coords[:, axis].argsort()
        nodes_per_part = (num_nodes + num_parts - 1) // num_parts
        
        for i, idx in enumerate(sorted_idx):
            partition_ids[idx] = min(i // nodes_per_part, num_parts - 1)
    
    return partition_ids


def _rcb_partition(
    coords: torch.Tensor,
    partition_ids: torch.Tensor,
    node_indices: torch.Tensor,
    part_offset: int,
    num_parts: int
):
    """Recursive Coordinate Bisection helper"""
    if num_parts == 1 or len(node_indices) == 0:
        partition_ids[node_indices] = part_offset
        return
    
    # Find longest axis
    local_coords = coords[node_indices]
    ranges = local_coords.max(0).values - local_coords.min(0).values
    axis = ranges.argmax().item()
    
    # Find median
    axis_vals = local_coords[:, axis]
    median = axis_vals.median()
    
    # Split
    left_mask = axis_vals <= median
    right_mask = ~left_mask
    
    left_nodes = node_indices[left_mask]
    right_nodes = node_indices[right_mask]
    
    # Handle uneven splits
    left_parts = num_parts // 2
    right_parts = num_parts - left_parts
    
    _rcb_partition(coords, partition_ids, left_nodes, part_offset, left_parts)
    _rcb_partition(coords, partition_ids, right_nodes, part_offset + left_parts, right_parts)


def find_halo_nodes(
    row: torch.Tensor,
    col: torch.Tensor,
    partition_ids: torch.Tensor,
    partition_id: int
) -> Tuple[torch.Tensor, Dict[int, torch.Tensor]]:
    """
    Find halo/ghost nodes for a partition (vectorized version).
    
    Halo nodes are nodes owned by other partitions but connected to this partition's nodes.
    
    Returns
    -------
    halo_nodes : torch.Tensor
        Global indices of halo nodes
    send_map : Dict[int, torch.Tensor]
        For each neighbor, which of our owned nodes to send
    """
    # Vectorized ownership check
    owned_mask = partition_ids == partition_id
    
    row_cpu = row.cpu()
    col_cpu = col.cpu()
    
    row_owned = owned_mask[row_cpu]
    col_owned = owned_mask[col_cpu]
    
    # Case 1: row owned, col not owned -> col is halo
    mask1 = row_owned & ~col_owned
    halo_from_col = col_cpu[mask1]
    send_to_neighbor_col = row_cpu[mask1]  # owned nodes to send
    neighbor_ids_col = partition_ids[halo_from_col]
    
    # Case 2: col owned, row not owned -> row is halo
    mask2 = col_owned & ~row_owned
    halo_from_row = row_cpu[mask2]
    send_to_neighbor_row = col_cpu[mask2]  # owned nodes to send
    neighbor_ids_row = partition_ids[halo_from_row]
    
    # Combine halo nodes
    all_halo = torch.cat([halo_from_col, halo_from_row])
    halo_nodes = torch.unique(all_halo, sorted=True)
    
    # Build send_map: for each neighbor, which owned nodes to send
    all_neighbors = torch.cat([neighbor_ids_col, neighbor_ids_row])
    all_send_nodes = torch.cat([send_to_neighbor_col, send_to_neighbor_row])
    
    send_map = {}
    unique_neighbors = torch.unique(all_neighbors)
    for neighbor_id in unique_neighbors.tolist():
        mask = all_neighbors == neighbor_id
        nodes_to_send = torch.unique(all_send_nodes[mask], sorted=True)
        send_map[neighbor_id] = nodes_to_send
    
    return halo_nodes, send_map


class DSparseMatrix:
    """
    Distributed Sparse Matrix with halo exchange support.
    
    Designed for large-scale CFD/FEM computations following industrial
    practices from Ansys, OpenFOAM, etc.
    
    The matrix is partitioned across multiple processes/GPUs, with automatic
    halo (ghost) node management for parallel iterative solvers.
    
    Supports both CPU and CUDA devices.
    
    Attributes
    ----------
    partition : Partition
        Local partition information
    local_values : torch.Tensor
        Non-zero values for local portion of matrix
    local_row : torch.Tensor
        Local row indices
    local_col : torch.Tensor
        Local column indices
    local_shape : Tuple[int, int]
        Shape of local matrix (including halo)
    global_shape : Tuple[int, int]
        Shape of global matrix
    device : torch.device
        Device where the matrix data resides (cpu or cuda)
    
    Example
    -------
    >>> # Create distributed matrix on CPU
    >>> A = DSparseMatrix.from_global(val, row, col, shape, num_parts=4, my_part=0, device='cpu')
    >>> 
    >>> # Create distributed matrix on CUDA
    >>> A_cuda = DSparseMatrix.from_global(val, row, col, shape, num_parts=4, my_part=0, device='cuda')
    >>> 
    >>> # Distributed matrix-vector product with halo exchange
    >>> y = A.matvec(x)  # Automatically handles halo exchange
    >>> 
    >>> # Explicit halo exchange
    >>> A.halo_exchange(x)  # Update halo values in x
    """
    
    def __init__(
        self,
        partition: Partition,
        local_values: torch.Tensor,
        local_row: torch.Tensor,
        local_col: torch.Tensor,
        local_shape: Tuple[int, int],
        global_shape: Tuple[int, int],
        num_partitions: int,
        device: Union[str, torch.device] = 'cpu',
        verbose: bool = True
    ):
        # Convert device to torch.device
        if isinstance(device, str):
            device = torch.device(device)
        
        self.partition = partition
        self.local_values = local_values.to(device)
        self.local_row = local_row.to(device)
        self.local_col = local_col.to(device)
        self.local_shape = local_shape
        self.global_shape = global_shape
        self.num_partitions = num_partitions
        self.device = device
        self._verbose = verbose
        
        # Move partition tensors to device
        self._partition_to_device()
        
        # For display
        if verbose:
            self._print_partition_info()
    
    def _partition_to_device(self):
        """Move partition tensors to the target device"""
        # Note: We keep some partition info on CPU for indexing
        # Only move what's needed for computation
        pass
    
    def _print_partition_info(self):
        """Print partition info for user awareness"""
        owned = len(self.partition.owned_nodes)
        halo = len(self.partition.halo_nodes)
        total = self.local_shape[0]
        neighbors = len(self.partition.neighbor_partitions)
        
        print(f"[Partition {self.partition.partition_id}/{self.num_partitions}] "
              f"Nodes: {owned} owned + {halo} halo = {total} local | "
              f"Neighbors: {neighbors} | "
              f"Global: {self.global_shape[0]}x{self.global_shape[1]} | "
              f"Device: {self.device}")
    
    def to(self, device: Union[str, torch.device]) -> "DSparseMatrix":
        """
        Move the distributed matrix to a different device.
        
        Parameters
        ----------
        device : str or torch.device
            Target device ('cpu', 'cuda', 'cuda:0', etc.)
            
        Returns
        -------
        DSparseMatrix
            New distributed matrix on the target device
        """
        if isinstance(device, str):
            device = torch.device(device)
        
        return DSparseMatrix(
            partition=self.partition,
            local_values=self.local_values.to(device),
            local_row=self.local_row.to(device),
            local_col=self.local_col.to(device),
            local_shape=self.local_shape,
            global_shape=self.global_shape,
            num_partitions=self.num_partitions,
            device=device,
            verbose=False  # Don't print again when moving
        )
    
    def cuda(self, device: Optional[int] = None) -> "DSparseMatrix":
        """Move to CUDA device"""
        if device is not None:
            return self.to(f'cuda:{device}')
        return self.to('cuda')
    
    def cpu(self) -> "DSparseMatrix":
        """Move to CPU"""
        return self.to('cpu')
    
    @property
    def is_cuda(self) -> bool:
        """Check if matrix is on CUDA"""
        return self.device.type == 'cuda'
    
    @classmethod
    def from_global(
        cls,
        values: torch.Tensor,
        row: torch.Tensor,
        col: torch.Tensor,
        shape: Tuple[int, int],
        num_partitions: int,
        my_partition: int,
        partition_ids: Optional[torch.Tensor] = None,
        coords: Optional[torch.Tensor] = None,
        device: Union[str, torch.device] = 'cpu',
        verbose: bool = True
    ) -> "DSparseMatrix":
        """
        Create distributed matrix from global COO data.
        
        Parameters
        ----------
        values, row, col : torch.Tensor
            Global COO sparse matrix data
        shape : Tuple[int, int]
            Global matrix shape
        num_partitions : int
            Number of partitions
        my_partition : int
            This process's partition ID (0 to num_partitions-1)
        partition_ids : torch.Tensor, optional
            Pre-computed partition assignments. If None, computed automatically.
        coords : torch.Tensor, optional
            Node coordinates for geometric partitioning [num_nodes, dim]
        device : str or torch.device
            Device for local data ('cpu', 'cuda', 'cuda:0', etc.)
        verbose : bool
            Whether to print partition info
            
        Returns
        -------
        DSparseMatrix
            Local portion of the distributed matrix
        """
        num_nodes = shape[0]
        
        # Compute partitioning if not provided
        if partition_ids is None:
            if coords is not None:
                partition_ids = partition_coordinates(coords, num_partitions)
            else:
                partition_ids = partition_graph_metis(row, col, num_nodes, num_partitions)
        
        # Find owned and halo nodes
        owned_mask = partition_ids == my_partition
        owned_nodes = owned_mask.nonzero().squeeze(-1)
        halo_nodes, send_map = find_halo_nodes(row, col, partition_ids, my_partition)
        
        # All local nodes (owned + halo)
        local_nodes = torch.cat([owned_nodes, halo_nodes])
        num_local = len(local_nodes)
        
        # Build global-to-local mapping (vectorized)
        global_to_local = torch.full((num_nodes,), -1, dtype=torch.int64)
        global_to_local[local_nodes] = torch.arange(num_local, dtype=torch.int64)
        
        # Extract local matrix entries (vectorized)
        row_cpu = row.cpu()
        col_cpu = col.cpu()
        val_cpu = values.cpu()
        
        # Map global indices to local
        local_row_mapped = global_to_local[row_cpu]
        local_col_mapped = global_to_local[col_cpu]
        
        # Filter to entries where both row and col are local
        valid_mask = (local_row_mapped >= 0) & (local_col_mapped >= 0)
        local_row = local_row_mapped[valid_mask]
        local_col = local_col_mapped[valid_mask]
        local_values = val_cpu[valid_mask]
        
        # Build recv_indices (vectorized)
        recv_indices = {}
        halo_offset = len(owned_nodes)
        
        # Create halo node to local index mapping
        halo_to_local = torch.full((num_nodes,), -1, dtype=torch.int64)
        halo_to_local[halo_nodes] = torch.arange(len(halo_nodes), dtype=torch.int64) + halo_offset
        
        for neighbor_id in send_map.keys():
            neighbor_owned = (partition_ids == neighbor_id).nonzero().squeeze(-1)
            # Find which of neighbor's owned nodes are in our halo
            local_idx = halo_to_local[neighbor_owned]
            recv_indices[neighbor_id] = local_idx[local_idx >= 0]
        
        # Convert send_map from global node IDs to local indices
        # send_map currently contains global node IDs, but halo_exchange needs local indices
        send_indices_local = {}
        for neighbor_id, global_nodes in send_map.items():
            local_idx = global_to_local[global_nodes]
            send_indices_local[neighbor_id] = local_idx
        
        partition = Partition(
            partition_id=my_partition,
            local_nodes=local_nodes,
            owned_nodes=owned_nodes,
            halo_nodes=halo_nodes,
            neighbor_partitions=list(send_map.keys()),
            send_indices=send_indices_local,  # Use local indices instead of global
            recv_indices=recv_indices,
            global_to_local=global_to_local,
            local_to_global=local_nodes.clone()
        )
        
        return cls(
            partition=partition,
            local_values=local_values,
            local_row=local_row,
            local_col=local_col,
            local_shape=(num_local, num_local),
            global_shape=shape,
            num_partitions=num_partitions,
            device=device,
            verbose=verbose
        )
    
    @property
    def num_owned(self) -> int:
        """Number of owned (non-halo) nodes"""
        return len(self.partition.owned_nodes)
    
    @property
    def num_halo(self) -> int:
        """Number of halo/ghost nodes"""
        return len(self.partition.halo_nodes)
    
    @property
    def num_local(self) -> int:
        """Total local nodes (owned + halo)"""
        return self.local_shape[0]
    
    @property
    def nnz(self) -> int:
        """Number of non-zeros in local matrix"""
        return len(self.local_values)
    
    @property
    def dtype(self) -> torch.dtype:
        """Data type of matrix values"""
        return self.local_values.dtype
    
    def halo_exchange(
        self,
        x: torch.Tensor,
        async_op: bool = False
    ) -> Optional[torch.Tensor]:
        """
        Exchange halo/ghost values with neighbors.
        
        This is the core operation for parallel iterative methods.
        Updates the halo portion of x with values from neighboring partitions.
        
        Parameters
        ----------
        x : torch.Tensor
            Local vector [num_local] with owned values filled in.
            Halo values will be updated.
        async_op : bool
            If True, return immediately and return a future.
            
        Returns
        -------
        x : torch.Tensor
            Vector with updated halo values (same tensor, modified in-place)
            
        Example
        -------
        >>> # During iterative solve
        >>> for iteration in range(max_iter):
        >>>     # Compute local update
        >>>     x_new = local_gauss_seidel_step(A_local, x, b)
        >>>     
        >>>     # Exchange boundary values
        >>>     A.halo_exchange(x_new)
        >>>     
        >>>     # Check convergence using owned nodes only
        >>>     residual = compute_residual(A_local, x_new, b)
        """
        if not DIST_AVAILABLE or not dist.is_initialized():
            # Single-process fallback: just return (no exchange needed)
            return x
        
        # Use cached send/recv indices and buffers for efficiency
        send_buffers = self._get_send_buffers(x.dtype)
        recv_buffers = self._get_recv_buffers(x.dtype)
        
        # Fill send buffers (vectorized gather)
        for neighbor_id in self.partition.neighbor_partitions:
            send_idx = self._send_indices_cached.get(neighbor_id)
            if send_idx is None:
                send_idx = self.partition.send_indices[neighbor_id].to(self.device)
                self._send_indices_cached[neighbor_id] = send_idx
            send_buffers[neighbor_id].copy_(x[send_idx])
        
        # Use send/recv for p2p communication
        # Note: For NCCL, we use synchronous send/recv
        backend = dist.get_backend() if dist.is_initialized() else 'gloo'
        
        if backend == 'nccl':
            # NCCL: use synchronous send/recv pairs
            for neighbor_id in sorted(self.partition.neighbor_partitions):
                if self.partition.partition_id < neighbor_id:
                    # Lower rank sends first, then receives
                    dist.send(send_buffers[neighbor_id], dst=neighbor_id)
                    dist.recv(recv_buffers[neighbor_id], src=neighbor_id)
                else:
                    # Higher rank receives first, then sends
                    dist.recv(recv_buffers[neighbor_id], src=neighbor_id)
                    dist.send(send_buffers[neighbor_id], dst=neighbor_id)
        else:
            # Gloo: use non-blocking isend/irecv
            requests = []
            for neighbor_id in self.partition.neighbor_partitions:
                req = dist.isend(send_buffers[neighbor_id], dst=neighbor_id)
                requests.append(req)
                req = dist.irecv(recv_buffers[neighbor_id], src=neighbor_id)
                requests.append(req)
            
            if async_op:
                return requests
            
            for req in requests:
                req.wait()
        
        # Update halo values (vectorized scatter)
        for neighbor_id in self.partition.neighbor_partitions:
            recv_idx = self._recv_indices_cached.get(neighbor_id)
            if recv_idx is None:
                recv_idx = self.partition.recv_indices[neighbor_id].to(self.device)
                self._recv_indices_cached[neighbor_id] = recv_idx
            x[recv_idx] = recv_buffers[neighbor_id]
        
        return x
    
    def halo_exchange_local(
        self,
        x_list: List[torch.Tensor]
    ) -> None:
        """
        Local halo exchange for single-process multi-partition simulation.
        
        Useful for testing/debugging without actual distributed setup.
        
        Parameters
        ----------
        x_list : List[torch.Tensor]
            List of local vectors, one per partition
        """
        if not hasattr(self, '_all_partitions'):
            return
        
        # Build mapping from global to local for each partition
        for part_id in range(len(x_list)):
            partition = self._all_partitions[part_id]
            x = x_list[part_id]
            
            # For each halo node, find which neighbor owns it and get the value
            halo_offset = len(partition.owned_nodes)
            
            for halo_idx, global_node in enumerate(partition.halo_nodes.tolist()):
                local_halo_idx = halo_offset + halo_idx
                
                # Find which partition owns this node
                for neighbor_id in partition.neighbor_partitions:
                    neighbor_partition = self._all_partitions[neighbor_id]
                    neighbor_g2l = neighbor_partition.global_to_local
                    
                    if global_node < len(neighbor_g2l):
                        local_idx_in_neighbor = neighbor_g2l[global_node].item()
                        if local_idx_in_neighbor >= 0 and local_idx_in_neighbor < len(neighbor_partition.owned_nodes):
                            # This neighbor owns the node
                            x[local_halo_idx] = x_list[neighbor_id][local_idx_in_neighbor]
                            break
    
    def matvec(self, x: torch.Tensor, exchange_halo: bool = True) -> torch.Tensor:
        """
        Local matrix-vector product y = A_local @ x.
        
        Parameters
        ----------
        x : torch.Tensor
            Local vector [num_local]
        exchange_halo : bool
            If True, perform halo exchange before multiplication
            
        Returns
        -------
        y : torch.Tensor
            Result vector [num_local]
        """
        if exchange_halo:
            self.halo_exchange(x)
        
        # Use cached CSR for efficiency
        return torch.mv(self._get_csr(), x)
    
    def matvec_overlap(self, x: torch.Tensor) -> torch.Tensor:
        """
        Matrix-vector product with communication-computation overlap.
        
        This optimized version overlaps halo communication with computation:
        1. Start async halo exchange
        2. Compute interior part (rows that don't depend on halo)
        3. Wait for halo exchange to complete
        4. Compute boundary part (rows that depend on halo)
        5. Combine results
        
        Note: This is only beneficial in true distributed settings where
        there is actual network latency to hide. In single-process mode,
        this falls back to regular matvec.
        
        Parameters
        ----------
        x : torch.Tensor
            Local vector [num_local]
            
        Returns
        -------
        y : torch.Tensor
            Result vector [num_local]
        """
        # In single-process mode, overlap has overhead with no benefit
        if not DIST_AVAILABLE or not dist.is_initialized():
            self.halo_exchange(x)
            return self.matvec(x, exchange_halo=False)
        
        # Build interior/boundary decomposition if not cached
        if not hasattr(self, '_interior_csr') or self._interior_csr is None:
            self._build_interior_boundary_decomposition()
        
        # Check if overlap is worthwhile (need significant interior portion)
        if self._overlap_stats.get('interior_ratio', 0) < 0.1:
            # Not enough interior work to justify overlap overhead
            self.halo_exchange(x)
            return self.matvec(x, exchange_halo=False)
        
        # Start async halo exchange
        comm_handle = self.halo_exchange_async(x)
        
        # Compute interior part while communication is in progress
        # y_interior = A_interior @ x (only uses owned nodes, no halo)
        y = torch.zeros(self.num_local, dtype=x.dtype, device=self.device)
        if self._interior_csr is not None and self._interior_csr._nnz() > 0:
            y.add_(torch.mv(self._interior_csr, x))
        
        # Wait for halo exchange to complete
        if comm_handle is not None:
            self._wait_halo_exchange(comm_handle, x)
        
        # Compute boundary part (needs halo values)
        if self._boundary_csr is not None and self._boundary_csr._nnz() > 0:
            y.add_(torch.mv(self._boundary_csr, x))
        
        return y
    
    def _build_interior_boundary_decomposition(self):
        """
        Decompose matrix into interior and boundary parts.
        
        Interior: All entries in rows that only reference owned nodes (col < num_owned)
        Boundary: All entries in rows that reference at least one halo node (col >= num_owned)
        
        This allows computing interior rows while halo exchange is in progress.
        """
        num_owned = self.num_owned
        
        # For each entry, check if it references a halo node
        entry_uses_halo = self.local_col >= num_owned
        
        # For each row, count how many entries use halo
        # Use scatter_add to count halo references per row
        row_halo_count = torch.zeros(self.num_local, dtype=torch.int32, device=self.device)
        ones = torch.ones_like(self.local_row, dtype=torch.int32)
        row_halo_count.scatter_add_(0, self.local_row[entry_uses_halo], ones[entry_uses_halo])
        
        # A row is "interior" if it has zero halo references
        row_is_interior = row_halo_count == 0
        
        # Mark entries by their row type
        interior_mask = row_is_interior[self.local_row]
        boundary_mask = ~interior_mask
        
        # Only consider owned rows for interior (halo rows don't need computation)
        interior_mask = interior_mask & (self.local_row < num_owned)
        boundary_mask = boundary_mask & (self.local_row < num_owned)
        
        # Build interior CSR
        if interior_mask.any():
            interior_coo = torch.sparse_coo_tensor(
                torch.stack([self.local_row[interior_mask], self.local_col[interior_mask]]),
                self.local_values[interior_mask],
                self.local_shape,
                device=self.device
            )
            self._interior_csr = interior_coo.to_sparse_csr()
        else:
            self._interior_csr = None
        
        # Build boundary CSR
        if boundary_mask.any():
            boundary_coo = torch.sparse_coo_tensor(
                torch.stack([self.local_row[boundary_mask], self.local_col[boundary_mask]]),
                self.local_values[boundary_mask],
                self.local_shape,
                device=self.device
            )
            self._boundary_csr = boundary_coo.to_sparse_csr()
        else:
            self._boundary_csr = None
        
        # Cache statistics
        total_nnz_owned = (self.local_row < num_owned).sum().item()
        interior_nnz_count = interior_mask.sum().item()
        boundary_nnz_count = boundary_mask.sum().item()
        self._overlap_stats = {
            'interior_nnz': interior_nnz_count,
            'boundary_nnz': boundary_nnz_count,
            'total_nnz_owned': total_nnz_owned,
            'interior_ratio': interior_nnz_count / total_nnz_owned if total_nnz_owned > 0 else 0,
            'interior_rows': row_is_interior[:num_owned].sum().item(),
            'boundary_rows': (~row_is_interior[:num_owned]).sum().item(),
        }
    
    def halo_exchange_async(self, x: torch.Tensor):
        """
        Start asynchronous halo exchange.
        
        Returns a handle that can be passed to _wait_halo_exchange().
        """
        if not DIST_AVAILABLE or not dist.is_initialized():
            return None
        
        backend = dist.get_backend()
        
        # NCCL doesn't support true async in the same way, use streams
        if backend == 'nccl' and x.is_cuda:
            return self._halo_exchange_cuda_async(x)
        else:
            return self._halo_exchange_gloo_async(x)
    
    def _halo_exchange_cuda_async(self, x: torch.Tensor):
        """Async halo exchange using CUDA streams."""
        # Create communication stream if not exists
        if not hasattr(self, '_comm_stream'):
            self._comm_stream = torch.cuda.Stream(device=self.device)
        
        send_buffers = self._get_send_buffers(x.dtype)
        recv_buffers = self._get_recv_buffers(x.dtype)
        
        # Record current stream
        current_stream = torch.cuda.current_stream(self.device)
        
        # Fill send buffers on current stream
        for neighbor_id in self.partition.neighbor_partitions:
            send_idx = self._send_indices_cached.get(neighbor_id)
            if send_idx is None:
                send_idx = self.partition.send_indices[neighbor_id].to(self.device)
                self._send_indices_cached[neighbor_id] = send_idx
            send_buffers[neighbor_id].copy_(x[send_idx])
        
        # Synchronize before switching streams
        self._comm_stream.wait_stream(current_stream)
        
        # Do communication on comm stream
        with torch.cuda.stream(self._comm_stream):
            for neighbor_id in sorted(self.partition.neighbor_partitions):
                if self.partition.partition_id < neighbor_id:
                    dist.send(send_buffers[neighbor_id], dst=neighbor_id)
                    dist.recv(recv_buffers[neighbor_id], src=neighbor_id)
                else:
                    dist.recv(recv_buffers[neighbor_id], src=neighbor_id)
                    dist.send(send_buffers[neighbor_id], dst=neighbor_id)
        
        return {'type': 'cuda', 'stream': self._comm_stream, 'recv_buffers': recv_buffers}
    
    def _halo_exchange_gloo_async(self, x: torch.Tensor):
        """Async halo exchange using Gloo isend/irecv."""
        send_buffers = self._get_send_buffers(x.dtype)
        recv_buffers = self._get_recv_buffers(x.dtype)
        
        # Fill send buffers
        for neighbor_id in self.partition.neighbor_partitions:
            send_idx = self._send_indices_cached.get(neighbor_id)
            if send_idx is None:
                send_idx = self.partition.send_indices[neighbor_id].to(self.device)
                self._send_indices_cached[neighbor_id] = send_idx
            send_buffers[neighbor_id].copy_(x[send_idx])
        
        # Start async communication
        requests = []
        for neighbor_id in self.partition.neighbor_partitions:
            req = dist.isend(send_buffers[neighbor_id], dst=neighbor_id)
            requests.append(req)
            req = dist.irecv(recv_buffers[neighbor_id], src=neighbor_id)
            requests.append(req)
        
        return {'type': 'gloo', 'requests': requests, 'recv_buffers': recv_buffers}
    
    def _wait_halo_exchange(self, handle, x: torch.Tensor):
        """Wait for async halo exchange to complete and update x."""
        if handle is None:
            return
        
        if handle['type'] == 'cuda':
            # Synchronize with comm stream
            torch.cuda.current_stream(self.device).wait_stream(handle['stream'])
        elif handle['type'] == 'gloo':
            # Wait for all requests
            for req in handle['requests']:
                req.wait()
        
        # Update halo values
        recv_buffers = handle['recv_buffers']
        for neighbor_id in self.partition.neighbor_partitions:
            recv_idx = self._recv_indices_cached.get(neighbor_id)
            if recv_idx is None:
                recv_idx = self.partition.recv_indices[neighbor_id].to(self.device)
                self._recv_indices_cached[neighbor_id] = recv_idx
            x[recv_idx] = recv_buffers[neighbor_id]
    
    def _get_csr(self) -> torch.Tensor:
        """Get cached CSR matrix (lazy initialization)."""
        if not hasattr(self, '_csr_cache') or self._csr_cache is None:
            A_coo = torch.sparse_coo_tensor(
            torch.stack([self.local_row, self.local_col]),
            self.local_values,
                self.local_shape,
                device=self.device
            )
            self._csr_cache = A_coo.to_sparse_csr()
        return self._csr_cache
    
    def _invalidate_cache(self):
        """Invalidate CSR cache (call if matrix values change)."""
        self._csr_cache = None
        self._diag_cache = None
        self._diag_inv_cache = None
        self._send_buffers_cache = {}
        self._recv_buffers_cache = {}
        self._send_indices_cached = {}
        self._recv_indices_cached = {}
    
    def _get_send_buffers(self, dtype: torch.dtype) -> Dict[int, torch.Tensor]:
        """Get or create cached send buffers."""
        if not hasattr(self, '_send_buffers_cache'):
            self._send_buffers_cache = {}
        if not hasattr(self, '_send_indices_cached'):
            self._send_indices_cached = {}
        
        cache_key = dtype
        if cache_key not in self._send_buffers_cache:
            buffers = {}
            for neighbor_id in self.partition.neighbor_partitions:
                send_idx = self.partition.send_indices[neighbor_id]
                buffers[neighbor_id] = torch.empty(
                    len(send_idx), dtype=dtype, device=self.device
                )
            self._send_buffers_cache[cache_key] = buffers
        
        return self._send_buffers_cache[cache_key]
    
    def _get_recv_buffers(self, dtype: torch.dtype) -> Dict[int, torch.Tensor]:
        """Get or create cached receive buffers."""
        if not hasattr(self, '_recv_buffers_cache'):
            self._recv_buffers_cache = {}
        if not hasattr(self, '_recv_indices_cached'):
            self._recv_indices_cached = {}
        
        cache_key = dtype
        if cache_key not in self._recv_buffers_cache:
            buffers = {}
            for neighbor_id in self.partition.neighbor_partitions:
                recv_idx = self.partition.recv_indices[neighbor_id]
                buffers[neighbor_id] = torch.empty(
                    len(recv_idx), dtype=dtype, device=self.device
                )
            self._recv_buffers_cache[cache_key] = buffers
        
        return self._recv_buffers_cache[cache_key]
    
    def _get_diagonal(self) -> torch.Tensor:
        """Get cached diagonal elements."""
        if not hasattr(self, '_diag_cache') or self._diag_cache is None:
            diag_mask = self.local_row == self.local_col
            diag_indices = self.local_row[diag_mask]
            diag_values = self.local_values[diag_mask]
            self._diag_cache = torch.zeros(self.num_local, dtype=self.dtype, device=self.device)
            self._diag_cache[diag_indices] = diag_values
        return self._diag_cache
    
    def _get_diagonal_inv(self) -> torch.Tensor:
        """Get cached inverse diagonal (for Jacobi preconditioner)."""
        if not hasattr(self, '_diag_inv_cache') or self._diag_inv_cache is None:
            diag = self._get_diagonal()
            self._diag_inv_cache = torch.where(
                diag.abs() > 1e-14,
                1.0 / diag,
                torch.zeros_like(diag)
            )
        return self._diag_inv_cache
    
    def solve(
        self,
        b: torch.Tensor,
        method: str = 'cg',
        preconditioner: str = 'jacobi',
        atol: float = 1e-10,
        rtol: float = 1e-6,
        maxiter: int = 1000,
        verbose: bool = False,
        distributed: bool = True,
        overlap: bool = False,
        use_cache: bool = True
    ) -> torch.Tensor:
        """
        Solve linear system Ax = b.
        
        Optimizations enabled by default:
        - CSR cache: Avoids repeated COO->CSR conversion (use_cache=True)
        - Jacobi preconditioner: ~5% speedup for Poisson-like problems
        
        Parameters
        ----------
        b : torch.Tensor
            Right-hand side. Shape [num_owned] for owned nodes only.
        method : str
            Solver method: 'cg' (default), 'jacobi', 'gauss_seidel'
        preconditioner : str
            Preconditioner for CG: 'none', 'jacobi' (default), 'ssor', 'ic0', 'polynomial'
        atol : float
            Absolute tolerance for convergence
        rtol : float
            Relative tolerance for convergence (|r| < rtol * |b|)
        maxiter : int
            Maximum iterations
        verbose : bool
            Print convergence info (rank 0 only for distributed)
        distributed : bool, default=True
            If True (default): Solve the GLOBAL system using distributed
            algorithms with all_reduce for global dot products.
            If False: Solve only the LOCAL subdomain problem (useful as
            preconditioner in domain decomposition methods).
        overlap : bool, default=False
            If True: Overlap communication with computation.
            Note: Only beneficial for slow interconnects (InfiniBand, Ethernet).
            For NVLink, synchronous communication is faster.
        use_cache : bool, default=True
            If True (default): Cache CSR format and diagonal for reuse.
            Provides ~2% speedup and ~27% memory reduction.
            
        Returns
        -------
        x : torch.Tensor
            Solution for owned nodes, shape [num_owned]
            
        Examples
        --------
        >>> # Distributed solve (default) - all ranks cooperate
        >>> x = local_matrix.solve(b_owned)
        
        >>> # Local subdomain solve - no global communication
        >>> x = local_matrix.solve(b_owned, distributed=False)
        
        >>> # With different preconditioner
        >>> x = local_matrix.solve(b_owned, preconditioner='ssor')
        
        >>> # Disable caching (for memory-constrained cases)
        >>> x = local_matrix.solve(b_owned, use_cache=False)
        """
        # Invalidate cache if not using it
        if not use_cache:
            self._invalidate_cache()
        
        if distributed:
            return self._solve_distributed_pcg(b, preconditioner, atol, rtol, maxiter, verbose, overlap)
        else:
            return self._solve_local(b, method, atol, maxiter, verbose)
    
    def _solve_local(
        self,
        b: torch.Tensor,
        method: str,
        atol: float,
        maxiter: int,
        verbose: bool
    ) -> torch.Tensor:
        """Local subdomain solve (no global communication)."""
        # Handle b size
        if b.shape[0] == self.num_owned:
            b_full = torch.zeros(self.num_local, dtype=b.dtype, device=self.device)
            b_full[:self.num_owned] = b
            b = b_full
        elif b.shape[0] != self.num_local:
            raise ValueError(f"b must have size num_owned={self.num_owned} or num_local={self.num_local}")
        
        x = torch.zeros(self.num_local, dtype=b.dtype, device=self.device)
        
        if method == 'jacobi':
            x = self._solve_jacobi(x, b, atol, maxiter, verbose)
        elif method == 'gauss_seidel':
            x = self._solve_gauss_seidel(x, b, atol, maxiter, verbose)
        else:  # CG
            x = self._solve_cg(x, b, atol, maxiter, verbose)
        
        return x[:self.num_owned]
    
    def _solve_cg(self, x, b, atol, maxiter, verbose):
        """
        Local CG solver for subdomain problems.
        
        This solves only the local subdomain problem without global reductions.
        Useful as a preconditioner or subdomain solver in domain decomposition.
        """
        r = b - self.matvec(x)
        p = r.clone()
        rs_old = torch.dot(r[:self.num_owned], r[:self.num_owned])
        
        for i in range(maxiter):
            Ap = self.matvec(p)
            pAp = torch.dot(p[:self.num_owned], Ap[:self.num_owned])
            
            if pAp.abs() < 1e-30:
                break
                
            alpha = rs_old / pAp
            x = x + alpha * p
            r = r - alpha * Ap
            
            rs_new = torch.dot(r[:self.num_owned], r[:self.num_owned])
            
            if verbose and i % 100 == 0:
                print(f"  CG iter {i}: residual = {rs_new.sqrt():.2e}")
            
            if rs_new.sqrt() < atol:
                if verbose:
                    print(f"  CG converged at iter {i}")
                break
            
            if rs_old.abs() < 1e-30:
                break
                
            p = r + (rs_new / rs_old) * p
            rs_old = rs_new
        
        return x
    
    def _solve_jacobi(self, x, b, atol, maxiter, verbose):
        """Optimized Jacobi iteration with cached diagonal."""
        D_inv = self._get_diagonal_inv()
        D = self._get_diagonal()
        
        for i in range(maxiter):
            # Halo exchange
            self.halo_exchange(x)
            
            # x_new = D^{-1} @ (b - (A - D) @ x) = D^{-1} @ (b - A @ x + D @ x)
            Ax = self.matvec(x, exchange_halo=False)
            x_new = D_inv * (b - Ax + D * x)
            
            # Convergence check on owned nodes only
            diff = (x_new[:self.num_owned] - x[:self.num_owned]).norm()
            x = x_new
            
            if verbose and i % 100 == 0:
                print(f"  Jacobi iter {i}: diff = {diff:.2e}")
            
            if diff < atol:
                if verbose:
                    print(f"  Jacobi converged at iter {i}")
                break
        
        return x
    
    def _solve_gauss_seidel(self, x, b, atol, maxiter, verbose):
        """
        Gauss-Seidel iteration with halo exchange.
        
        Note: True GS requires sequential updates, which is slow on GPU.
        This implementation uses a hybrid approach:
        - On CPU: Use sparse triangular solve (faster than Python loop)
        - On GPU: Fall back to damped Jacobi (parallel, similar convergence)
        """
        if self.device.type == 'cuda':
            # GPU: Use damped Jacobi as approximation (parallel)
            return self._solve_damped_jacobi(x, b, atol, maxiter, verbose, omega=0.8)
        
        # CPU: Use SciPy's efficient sparse triangular solve
        D_inv = self._get_diagonal_inv()
        D = self._get_diagonal()
        
        # Get CSR for efficient access
        A_csr = self._get_csr()
        
        for iteration in range(maxiter):
            x_old = x.clone()
            
            # Exchange halo before sweep
            self.halo_exchange(x)
            
            # Compute residual and apply diagonal scaling
            # This is symmetric GS approximation
            Ax = self.matvec(x, exchange_halo=False)
            r = b - Ax
            x = x + D_inv * r
            
            diff = (x[:self.num_owned] - x_old[:self.num_owned]).norm()
            
            if verbose and iteration % 100 == 0:
                print(f"  GS iter {iteration}: diff = {diff:.2e}")
            
            if diff < atol:
                if verbose:
                    print(f"  GS converged at iter {iteration}")
                break
        
        return x
    
    def _solve_damped_jacobi(self, x, b, atol, maxiter, verbose, omega=0.8):
        """Damped Jacobi iteration (parallel-friendly for GPU)."""
        D_inv = self._get_diagonal_inv()
        D = self._get_diagonal()
        
        for i in range(maxiter):
            self.halo_exchange(x)
            Ax = self.matvec(x, exchange_halo=False)
            
            # x_new = x + omega * D^{-1} @ (b - A @ x)
            x_new = x + omega * D_inv * (b - Ax)
            
            diff = (x_new[:self.num_owned] - x[:self.num_owned]).norm()
            x = x_new
            
            if verbose and i % 100 == 0:
                print(f"  Damped Jacobi iter {i}: diff = {diff:.2e}")
            
            if diff < atol:
                if verbose:
                    print(f"  Damped Jacobi converged at iter {i}")
                break
        
        return x
    
    def _solve_distributed_cg(
        self,
        b_owned: torch.Tensor,
        atol: float,
        maxiter: int,
        verbose: bool
    ) -> torch.Tensor:
        """Legacy CG solver - use _solve_distributed_pcg instead."""
        return self._solve_distributed_pcg(b_owned, 'none', atol, 1e-6, maxiter, verbose, overlap=True)
    
    def _solve_distributed_pcg(
        self,
        b_owned: torch.Tensor,
        preconditioner: str,
        atol: float,
        rtol: float,
        maxiter: int,
        verbose: bool,
        overlap: bool = True
    ) -> torch.Tensor:
        """
        Distributed Preconditioned Conjugate Gradient solver.
        
        Optimizations over basic CG:
        1. Cached CSR format for matvec
        2. Jacobi/block-Jacobi preconditioning
        3. Relative tolerance support
        4. Reduced memory allocations
        5. Communication-computation overlap (when overlap=True)
        """
        num_owned = self.num_owned
        num_local = self.num_local
        dtype = b_owned.dtype
        device = self.device
        rank = self.partition.partition_id
        
        # Initialize x_local = 0 (owned + halo)
        x_local = torch.zeros(num_local, dtype=dtype, device=device)
        
        # Extend b to local size (halo part is 0)
        b_local = torch.zeros(num_local, dtype=dtype, device=device)
        b_local[:num_owned] = b_owned
        
        # Compute initial |b| for relative tolerance
        b_norm_local = torch.dot(b_owned, b_owned)
        b_norm = self._global_reduce_sum(b_norm_local).sqrt()
        tol = max(atol, rtol * b_norm)
        
        # r = b - A @ x (no halo exchange needed for x=0)
        r_local = b_local.clone()
        
        # Apply preconditioner: z = M^{-1} @ r
        z_local = self._apply_preconditioner(r_local, preconditioner)
        
        # p = z
        p_local = z_local.clone()
        
        # rz_old = r^T @ z (global reduction, only owned nodes)
        rz_local = torch.dot(r_local[:num_owned], z_local[:num_owned])
        rz_old = self._global_reduce_sum(rz_local)
        
        # For convergence check
        rs_local = torch.dot(r_local[:num_owned], r_local[:num_owned])
        rs_old = self._global_reduce_sum(rs_local)
        
        # Print overlap info on first call
        if verbose and rank == 0 and overlap:
            if hasattr(self, '_overlap_stats'):
                stats = self._overlap_stats
                print(f"  Overlap enabled: interior_ratio = {stats['interior_ratio']:.1%}")
        
        for i in range(maxiter):
            # Ap = A @ p with optional overlap
            if overlap:
                Ap_local = self.matvec_overlap(p_local)
            else:
                self.halo_exchange(p_local)
                Ap_local = self.matvec(p_local, exchange_halo=False)
            
            # pAp = p^T @ A @ p (global reduction)
            pAp_local = torch.dot(p_local[:num_owned], Ap_local[:num_owned])
            pAp = self._global_reduce_sum(pAp_local)
            
            if pAp.abs() < 1e-30:
                break
            
            alpha = rz_old / pAp
            
            # Update x and r (in-place for efficiency)
            x_local.add_(p_local, alpha=alpha)
            r_local.add_(Ap_local, alpha=-alpha)
            
            # Compute residual norm for convergence check
            rs_local = torch.dot(r_local[:num_owned], r_local[:num_owned])
            rs_new = self._global_reduce_sum(rs_local)
            residual = rs_new.sqrt()
            
            if verbose and rank == 0 and i % 50 == 0:
                print(f"  PCG iter {i}: residual = {residual:.2e}, tol = {tol:.2e}")
            
            if residual < tol:
                if verbose and rank == 0:
                    print(f"  PCG converged at iter {i}, residual = {residual:.2e}")
                break
            
            # Apply preconditioner: z = M^{-1} @ r
            z_local = self._apply_preconditioner(r_local, preconditioner)
            
            # rz_new = r^T @ z
            rz_local = torch.dot(r_local[:num_owned], z_local[:num_owned])
            rz_new = self._global_reduce_sum(rz_local)
            
            beta = rz_new / rz_old
            
            # p = z + beta * p (in-place)
            p_local.mul_(beta).add_(z_local)
            rz_old = rz_new
        
        # Return only owned part
        return x_local[:num_owned]
    
    def _apply_preconditioner(
        self,
        r: torch.Tensor,
        preconditioner: str
    ) -> torch.Tensor:
        """
        Apply preconditioner M^{-1} @ r.
        
        Parameters
        ----------
        r : torch.Tensor
            Residual vector [num_local]
        preconditioner : str
            'none', 'jacobi', 'block_jacobi', 'ssor', 'ic0', 'polynomial'
            
        Returns
        -------
        z : torch.Tensor
            Preconditioned residual [num_local]
        """
        if preconditioner == 'none':
            return r.clone()
        
        elif preconditioner == 'jacobi':
            # z = D^{-1} @ r
            D_inv = self._get_diagonal_inv()
            return D_inv * r
        
        elif preconditioner == 'block_jacobi':
            # Solve local subdomain (few iterations of local CG or direct)
            z = torch.zeros_like(r)
            z[:self.num_owned] = self._local_solve_approx(
                r[:self.num_owned], maxiter=5
            )
            return z
        
        elif preconditioner == 'ssor':
            # Symmetric SOR: (D + L) D^{-1} (D + U)
            omega = 1.5
            return self._apply_ssor(r, omega)
        
        elif preconditioner == 'ic0':
            # Incomplete Cholesky (GPU-friendly iterative version)
            return self._apply_ic0(r, num_sweeps=2)
        
        elif preconditioner == 'polynomial':
            # Neumann series polynomial preconditioner
            return self._apply_polynomial(r, degree=3)
        
        else:
            warnings.warn(f"Unknown preconditioner '{preconditioner}', using none")
            return r.clone()
    
    def _local_solve_approx(
        self,
        b_owned: torch.Tensor,
        maxiter: int = 5
    ) -> torch.Tensor:
        """
        Approximate local solve for block-Jacobi preconditioner.
        Uses few iterations of Jacobi or CG.
        """
        D_inv = self._get_diagonal_inv()[:self.num_owned]
        x = torch.zeros_like(b_owned)
        
        # Simple Jacobi iterations (fast, no halo exchange needed)
        for _ in range(maxiter):
            # Only use diagonal part for approximate solve
            x = D_inv * b_owned
        
        return x
    
    def _apply_ssor(self, r: torch.Tensor, omega: float = 1.5) -> torch.Tensor:
        """
        Apply SSOR preconditioner (GPU-friendly scaled Jacobi approximation).
        
        True SSOR requires sequential sweeps, slow on GPU.
        This uses a scaled Jacobi that approximates SSOR behavior.
        """
        import math
        D_inv = self._get_diagonal_inv()
        scale = math.sqrt(omega * (2 - omega))
        return scale * D_inv * r
    
    def _apply_ic0(self, r: torch.Tensor, num_sweeps: int = 2) -> torch.Tensor:
        """
        Apply Incomplete Cholesky (IC0) preconditioner using Jacobi iterations.
        
        GPU-friendly approximation of (D + L)^{-1} D (D + L^T)^{-1}.
        Uses parallel Jacobi sweeps for triangular solves.
        """
        # Get or build L/U matrices
        if not hasattr(self, '_ic0_L_csr') or self._ic0_L_csr is None:
            self._build_ic0_factors()
        
        D_inv = self._get_diagonal_inv()
        diag = self._get_diagonal()
        
        if self._ic0_L_csr is None:
            # No off-diagonal elements, just Jacobi
            return D_inv * r
        
        # Forward sweep: solve (D + L) y = r approximately
        # y^{k+1} = D^{-1} (r - L y^k)
        y = D_inv * r
        for _ in range(num_sweeps):
            Ly = torch.mv(self._ic0_L_csr, y)
            y = D_inv * (r - Ly)
        
        # Middle: scale by D
        z = diag * y
        
        # Backward sweep: solve (D + L^T) x = z approximately  
        # x^{k+1} = D^{-1} (z - L^T x^k)
        x = D_inv * z
        for _ in range(num_sweeps):
            Ux = torch.mv(self._ic0_U_csr, x)
            x = D_inv * (z - Ux)
        
        return x
    
    def _build_ic0_factors(self):
        """Build L and U factors for IC0 preconditioner."""
        n = self.num_local
        
        # Get strictly lower triangular part
        lower_mask = self.local_row > self.local_col
        L_row = self.local_row[lower_mask]
        L_col = self.local_col[lower_mask]
        L_val = self.local_values[lower_mask]
        
        if len(L_val) > 0:
            L_indices = torch.stack([L_row, L_col], dim=0)
            L_coo = torch.sparse_coo_tensor(
                L_indices, L_val, (n, n),
                device=self.device, dtype=self.local_values.dtype
            )
            self._ic0_L_csr = L_coo.to_sparse_csr()
            
            # Upper triangular (transpose of L)
            U_indices = torch.stack([L_col, L_row], dim=0)
            U_coo = torch.sparse_coo_tensor(
                U_indices, L_val, (n, n),
                device=self.device, dtype=self.local_values.dtype
            )
            self._ic0_U_csr = U_coo.to_sparse_csr()
        else:
            self._ic0_L_csr = None
            self._ic0_U_csr = None
    
    def _apply_polynomial(self, r: torch.Tensor, degree: int = 3) -> torch.Tensor:
        """
        Apply Neumann series polynomial preconditioner.
        
        Uses M^{-1}  D^{-1} (I + N + N^2 + ...) where N = I - D^{-1}A
        
        This is stable and parallelizes well on GPU.
        """
        D_inv = self._get_diagonal_inv()
        
        # z = D^{-1} @ r (degree=0 term)
        z = D_inv * r
        
        if degree == 0:
            return z
        
        # Neumann series: sum_{k=0}^{degree} (I - D^{-1}A)^k @ (D^{-1} @ r)
        y = r.clone()
        for _ in range(degree):
            # y = (I - D^{-1}A) @ y
            Ay = self._matvec_local(y)
            y = y - D_inv * Ay
            z = z + D_inv * y
        
        return z
    
    def _matvec_local(self, x: torch.Tensor) -> torch.Tensor:
        """Local matrix-vector product without halo exchange."""
        csr = self._get_csr()
        return torch.mv(csr, x)
    
    def _global_reduce_sum(self, value: torch.Tensor) -> torch.Tensor:
        """Perform global all_reduce sum."""
        if not DIST_AVAILABLE or not dist.is_initialized():
            return value
        
        # Ensure tensor is on the correct device for the backend
        backend = dist.get_backend()
        if backend == 'nccl' and not value.is_cuda:
            # NCCL requires CUDA tensors
            value = value.to(self.device)
        
        result = value.clone()
        dist.all_reduce(result, op=dist.ReduceOp.SUM)
        return result
    
    def eigsh(
        self,
        k: int = 6,
        which: str = "LM",
        maxiter: int = 200,
        tol: float = 1e-8,
        verbose: bool = False,
        distributed: bool = True
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute k eigenvalues of symmetric matrix.
        
        Parameters
        ----------
        k : int
            Number of eigenvalues to compute
        which : str
            Which eigenvalues: "LM" (largest magnitude), "SM" (smallest magnitude)
        maxiter : int
            Maximum iterations
        tol : float
            Convergence tolerance
        verbose : bool
            Print convergence info (rank 0 only)
        distributed : bool, default=True
            If True (default): Use distributed LOBPCG with global reductions.
            If False: Gather to single SparseTensor and compute locally
            (not recommended for large matrices).
            
        Returns
        -------
        eigenvalues : torch.Tensor
            k eigenvalues, shape [k]
        eigenvectors_owned : torch.Tensor
            Eigenvectors for owned nodes only, shape [num_owned, k]
        """
        if not distributed:
            # Gather to single node (not recommended)
            import warnings
            warnings.warn("distributed=False gathers entire matrix to one node. "
                         "Use distributed=True for large-scale problems.")
            st = self.to_sparse_tensor()
            eigenvalues, eigenvectors = st.eigsh(k=k, which=which)
            # Extract local portion
            owned_nodes = self.partition.owned_nodes
            return eigenvalues, eigenvectors[owned_nodes]
        n = self.global_shape[0]
        num_owned = self.num_owned
        rank = self.partition.partition_id
        dtype = self.local_values.dtype
        device = self.device
        
        # Initialize random subspace
        torch.manual_seed(42 + rank)  # Different per rank for diversity
        m = min(2 * k, n)
        
        # Each rank has its local portion of X
        X_owned = torch.randn(num_owned, m, dtype=dtype, device=device)
        
        # Orthogonalize globally
        X_owned = self._global_orthogonalize(X_owned)
        
        eigenvalues_prev = None
        
        for iteration in range(maxiter):
            # Distributed matvec: AX
            AX_owned = self._global_matvec_batch(X_owned)
            
            # Rayleigh-Ritz: H = X^T @ AX (global reduction)
            # Local contribution
            H_local = X_owned.T @ AX_owned
            H = self._global_reduce_sum(H_local)
            
            # Solve small eigenvalue problem (same on all ranks)
            eigenvalues, eigenvectors = torch.linalg.eigh(H)
            
            # Sort eigenvalues
            if which == "LM":
                idx_sort = eigenvalues.abs().argsort(descending=True)
            else:
                idx_sort = eigenvalues.abs().argsort()
            eigenvalues = eigenvalues[idx_sort]
            eigenvectors = eigenvectors[:, idx_sort]
            
            # Update X = X @ V (local)
            X_owned = X_owned @ eigenvectors
            
            # Check convergence
            if eigenvalues_prev is not None:
                diff = (eigenvalues[:k] - eigenvalues_prev[:k]).abs()
                if (diff < tol * eigenvalues[:k].abs().clamp(min=1e-10)).all():
                    if verbose and rank == 0:
                        print(f"  Distributed LOBPCG converged at iteration {iteration}")
                    break
            eigenvalues_prev = eigenvalues.clone()
            
            if verbose and rank == 0 and iteration % 20 == 0:
                print(f"  Distributed LOBPCG iter {iteration}: _0 = {eigenvalues[0]:.6f}")
            
            # Expand subspace with residual
            if iteration < maxiter - 1:
                AX_new = self._global_matvec_batch(X_owned)
                residual = AX_new - X_owned * eigenvalues.unsqueeze(0)
                
                # Combine and orthogonalize
                combined = torch.cat([X_owned[:, :k], residual[:, :k]], dim=1)
                X_owned = self._global_orthogonalize(combined)
                
                # Ensure correct size
                if X_owned.size(1) < m:
                    extra = torch.randn(num_owned, m - X_owned.size(1), dtype=dtype, device=device)
                    X_owned = torch.cat([X_owned, extra], dim=1)
                    X_owned = self._global_orthogonalize(X_owned)
        
        return eigenvalues[:k], X_owned[:, :k]
    
    def _global_matvec_batch(self, X_owned: torch.Tensor) -> torch.Tensor:
        """
        Distributed matvec for a batch of vectors.
        
        Each rank computes A @ X for its local portion.
        """
        num_owned = self.num_owned
        num_local = self.num_local
        m = X_owned.size(1)
        dtype = X_owned.dtype
        device = self.device
        
        # Extend to local size (owned + halo)
        X_local = torch.zeros(num_local, m, dtype=dtype, device=device)
        X_local[:num_owned] = X_owned
        
        # Gather global X for halo (simplified - in production use p2p)
        X_global = self._gather_all_vectors(X_owned)
        
        # Fill halo from global
        halo_nodes = self.partition.halo_nodes
        if len(halo_nodes) > 0:
            X_local[num_owned:] = X_global[halo_nodes]
        
        # Local matvec for each column
        Y_local = torch.zeros(num_local, m, dtype=dtype, device=device)
        for j in range(m):
            Y_local[:, j] = self.matvec(X_local[:, j], exchange_halo=False)
        
        return Y_local[:num_owned]
    
    def _gather_all_vectors(self, X_owned: torch.Tensor) -> torch.Tensor:
        """Gather vectors from all ranks to build global vector."""
        n = self.global_shape[0]
        m = X_owned.size(1)
        dtype = X_owned.dtype
        device = self.device
        
        X_global = torch.zeros(n, m, dtype=dtype, device=device)
        owned_nodes = self.partition.owned_nodes
        X_global[owned_nodes] = X_owned
        
        # All-reduce to combine
        self._global_reduce_sum_inplace(X_global)
        
        return X_global
    
    def _global_reduce_sum_inplace(self, tensor: torch.Tensor) -> None:
        """In-place global all_reduce sum."""
        if DIST_AVAILABLE and dist.is_initialized():
            dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    
    def _global_orthogonalize(self, X_owned: torch.Tensor) -> torch.Tensor:
        """
        Globally orthogonalize a distributed matrix using TSQR.
        
        Simplified version: gather, QR, scatter.
        Production version would use TSQR for better scalability.
        """
        # Gather global X
        X_global = self._gather_all_vectors(X_owned)
        
        # QR on global (same result on all ranks)
        Q, _ = torch.linalg.qr(X_global)
        
        # Extract local portion
        owned_nodes = self.partition.owned_nodes
        return Q[owned_nodes]
    
    def gather_global(self, x_local: torch.Tensor) -> Optional[torch.Tensor]:
        """
        Gather local vectors to global vector (on rank 0).
        
        Parameters
        ----------
        x_local : torch.Tensor
            Local vector [num_owned]
            
        Returns
        -------
        x_global : torch.Tensor or None
            Global vector on rank 0, None on other ranks
        """
        if not DIST_AVAILABLE or not dist.is_initialized():
            # Single process: just expand to global
            x_global = torch.zeros(self.global_shape[0], dtype=x_local.dtype, device=x_local.device)
            x_global[self.partition.owned_nodes] = x_local[:self.num_owned]
            return x_global
        
        # Distributed gather
        owned_vals = x_local[:self.num_owned]
        
        # Gather sizes
        local_size = torch.tensor([self.num_owned], device=self.device)
        sizes = [torch.zeros(1, dtype=torch.int64, device=self.device) for _ in range(self.num_partitions)]
        dist.all_gather(sizes, local_size)
        
        # Gather values
        if dist.get_rank() == 0:
            x_global = torch.zeros(self.global_shape[0], dtype=x_local.dtype, device=self.device)
            gathered = [torch.zeros(s.item(), dtype=x_local.dtype, device=self.device) for s in sizes]
            dist.gather(owned_vals, gather_list=gathered, dst=0)
            
            # Place in global vector (need owned_nodes from all partitions)
            # This requires additional communication of owned_nodes
            return x_global
        else:
            dist.gather(owned_vals, dst=0)
            return None
    
    def det(self) -> torch.Tensor:
        """
        Compute determinant of the distributed sparse matrix.
        
        NOTE: DSparseMatrix represents a single partition. To compute the
        determinant of the full global matrix, you need to use DSparseTensor
        which manages all partitions, or manually gather all partitions.
        
        This method raises an error to guide users to the correct approach.
        
        Raises
        ------
        NotImplementedError
            DSparseMatrix is a single partition. Use DSparseTensor.det() instead.
            
        Examples
        --------
        >>> # Correct way: Use DSparseTensor
        >>> from torch_sla import DSparseTensor
        >>> D = DSparseTensor(val, row, col, shape, num_partitions=4)
        >>> det = D.det()  # This works
        >>>
        >>> # If you have individual DSparseMatrix partitions, you need to
        >>> # reconstruct the global matrix first
        """
        raise NotImplementedError(
            "DSparseMatrix represents a single partition of a distributed matrix. "
            "To compute the determinant of the full global matrix, use DSparseTensor.det() instead, "
            "which manages all partitions and can gather the full matrix for determinant computation.\n\n"
            "Example:\n"
            "  from torch_sla import DSparseTensor\n"
            "  D = DSparseTensor(val, row, col, shape, num_partitions=4)\n"
            "  det = D.det()  # Gathers all partitions and computes determinant"
        )
    
    def __repr__(self) -> str:
        return (f"DSparseMatrix(partition={self.partition.partition_id}/{self.num_partitions}, "
                f"local={self.num_local} ({self.num_owned}+{self.num_halo}), "
                f"global={self.global_shape}, nnz={self.nnz}, device={self.device})")
    
    # =========================================================================
    # Persistence (I/O)
    # =========================================================================
    
    @classmethod
    def load(
        cls,
        directory: Union[str, "os.PathLike"],
        rank: int,
        world_size: Optional[int] = None,
        device: Union[str, torch.device] = "cpu"
    ) -> "DSparseMatrix":
        """
        Load a partition from disk for the given rank.
        
        Each rank should call this with its own rank to load only its partition.
        
        Parameters
        ----------
        directory : str or PathLike
            Directory containing partitioned data.
        rank : int
            Rank of this process.
        world_size : int, optional
            Total number of processes (must match num_partitions).
        device : str or torch.device
            Device to load tensors to.
        
        Returns
        -------
        DSparseMatrix
            The partition for this rank.
        
        Example
        -------
        >>> rank = dist.get_rank()
        >>> world_size = dist.get_world_size()
        >>> partition = DSparseMatrix.load("matrix_dist", rank, world_size, "cuda")
        """
        from .io import load_partition
        return load_partition(directory, rank, world_size, device)


def create_distributed_matrices(
    values: torch.Tensor,
    row: torch.Tensor,
    col: torch.Tensor,
    shape: Tuple[int, int],
    num_partitions: int,
    coords: Optional[torch.Tensor] = None,
    device: Union[str, torch.device] = 'cpu'
) -> List[DSparseMatrix]:
    """
    Create all distributed matrix partitions for local simulation.
    
    .. deprecated::
        Use DSparseTensor instead for a more Pythonic interface.
    
    Useful for testing/debugging without actual distributed setup.
    
    Parameters
    ----------
    values, row, col : torch.Tensor
        Global COO sparse matrix data
    shape : Tuple[int, int]
        Global matrix shape
    num_partitions : int
        Number of partitions
    coords : torch.Tensor, optional
        Node coordinates for geometric partitioning
    device : str or torch.device
        Device for all partitions ('cpu', 'cuda', 'cuda:0', etc.)
    
    Returns
    -------
    List[DSparseMatrix]
        List of DSparseMatrix, one per partition
    """
    warnings.warn(
        "create_distributed_matrices is deprecated. Use DSparseTensor instead.",
        DeprecationWarning,
        stacklevel=2
    )
    
    matrices = []
    
    # Compute partition IDs once
    if coords is not None:
        partition_ids = partition_coordinates(coords, num_partitions)
    else:
        partition_ids = partition_graph_metis(row, col, shape[0], num_partitions)
    
    for i in range(num_partitions):
        mat = DSparseMatrix.from_global(
            values, row, col, shape, num_partitions, i,
            partition_ids=partition_ids, device=device
        )
        matrices.append(mat)
    
    # Store reference to all partitions for local halo exchange
    for mat in matrices:
        mat._all_partitions = [m.partition for m in matrices]
    
    return matrices


class DSparseTensor:
    """
    Distributed Sparse Tensor with automatic partitioning and halo exchange.
    
    A Pythonic wrapper that provides a unified interface for distributed
    sparse matrix operations. Supports indexing to access individual partitions.
    
    Parameters
    ----------
    values : torch.Tensor
        Non-zero values [nnz]
    row_indices : torch.Tensor
        Row indices [nnz]
    col_indices : torch.Tensor
        Column indices [nnz]
    shape : Tuple[int, int]
        Matrix shape (m, n)
    num_partitions : int
        Number of partitions to create
    coords : torch.Tensor, optional
        Node coordinates for geometric partitioning [num_nodes, dim]
    partition_method : str
        Partitioning method: 'metis', 'rcb', 'slicing', 'simple'
    device : str or torch.device
        Device for the matrix data
    verbose : bool
        Whether to print partition info
    
    Example
    -------
    >>> import torch
    >>> from torch_sla import DSparseTensor
    >>> 
    >>> # Create distributed tensor with 4 partitions
    >>> A = DSparseTensor(val, row, col, shape, num_partitions=4)
    >>> 
    >>> # Access individual partitions
    >>> A0 = A[0]  # First partition
    >>> A1 = A[1]  # Second partition
    >>> 
    >>> # Iterate over partitions
    >>> for partition in A:
    >>>     x = partition.solve(b_local)
    >>> 
    >>> # Properties
    >>> print(A.num_partitions)  # 4
    >>> print(A.shape)           # Global shape
    >>> print(len(A))            # 4
    >>> 
    >>> # Move to CUDA
    >>> A_cuda = A.cuda()
    >>> 
    >>> # Local halo exchange (for testing)
    >>> x_list = [torch.zeros(A[i].num_local) for i in range(4)]
    >>> A.halo_exchange_local(x_list)
    """
    
    def __init__(
        self,
        values: torch.Tensor,
        row_indices: torch.Tensor,
        col_indices: torch.Tensor,
        shape: Tuple[int, int],
        num_partitions: int,
        coords: Optional[torch.Tensor] = None,
        partition_method: str = 'auto',
        device: Optional[Union[str, torch.device]] = None,
        verbose: bool = True
    ):
        self._values = values
        self._row_indices = row_indices
        self._col_indices = col_indices
        self._shape = shape
        self._num_partitions = num_partitions
        self._coords = coords
        self._partition_method = partition_method
        self._verbose = verbose
        
        # Infer device from input tensor if not explicitly specified
        if device is None:
            device = values.device
        if isinstance(device, str):
            device = torch.device(device)
        self._device = device
        
        # Compute partition IDs
        # NOTE: In distributed mode, this should be computed on rank 0 and broadcast
        # to ensure consistency. See _compute_partitions_distributed() for distributed-safe version.
        self._partition_ids = self._compute_partitions(partition_method, coords)
        
        # Create all partitions
        self._partitions: List[DSparseMatrix] = []
        self._create_partitions()
    
    def _compute_partitions(
        self, 
        method: str, 
        coords: Optional[torch.Tensor]
    ) -> torch.Tensor:
        """Compute partition assignments for each node."""
        if method == 'auto':
            if coords is not None:
                method = 'rcb'
            else:
                method = 'metis'
        
        if method == 'metis':
            return partition_graph_metis(
                self._row_indices, self._col_indices, 
                self._shape[0], self._num_partitions
            )
        elif method in ['rcb', 'slicing']:
            if coords is None:
                raise ValueError(f"Partition method '{method}' requires coords")
            return partition_coordinates(coords, self._num_partitions, method=method)
        elif method == 'simple':
            return partition_simple(self._shape[0], self._num_partitions)
        else:
            raise ValueError(f"Unknown partition method: {method}")
    
    def _create_partitions(self):
        """Create all partition matrices."""
        for i in range(self._num_partitions):
            mat = DSparseMatrix.from_global(
                self._values, self._row_indices, self._col_indices,
                self._shape, self._num_partitions, i,
                partition_ids=self._partition_ids,
                device=self._device,
                verbose=self._verbose
            )
            self._partitions.append(mat)
        
        # Store reference to all partitions for local halo exchange
        for mat in self._partitions:
            mat._all_partitions = [m.partition for m in self._partitions]
    
    @classmethod
    def from_sparse_tensor(
        cls,
        sparse_tensor: "SparseTensor",
        num_partitions: int,
        coords: Optional[torch.Tensor] = None,
        partition_method: str = 'auto',
        device: Optional[Union[str, torch.device]] = None,
        verbose: bool = True
    ) -> "DSparseTensor":
        """
        Create DSparseTensor from a SparseTensor.
        
        Parameters
        ----------
        sparse_tensor : SparseTensor
            Input sparse tensor (must be 2D, not batched)
        num_partitions : int
            Number of partitions
        coords : torch.Tensor, optional
            Node coordinates for geometric partitioning
        partition_method : str
            Partitioning method
        device : str or torch.device, optional
            Target device (defaults to sparse_tensor's device)
        verbose : bool
            Whether to print partition info
            
        Returns
        -------
        DSparseTensor
            Distributed sparse tensor
        """
        # Avoid circular import
        from .sparse_tensor import SparseTensor
        
        if sparse_tensor.is_batched:
            raise ValueError("DSparseTensor does not support batched SparseTensor. "
                           "Use a 2D SparseTensor.")
        
        if device is None:
            device = sparse_tensor.device
        
        # Use sparse_shape for the matrix dimensions
        sparse_shape = sparse_tensor.sparse_shape
        
        return cls(
            sparse_tensor.values,
            sparse_tensor.row_indices,
            sparse_tensor.col_indices,
            sparse_shape,
            num_partitions=num_partitions,
            coords=coords,
            partition_method=partition_method,
            device=device,
            verbose=verbose
        )
    
    @classmethod
    def from_torch_sparse(
        cls,
        A: torch.Tensor,
        num_partitions: int,
        **kwargs
    ) -> "DSparseTensor":
        """Create DSparseTensor from PyTorch sparse tensor."""
        if A.layout == torch.sparse_csr:
            A = A.to_sparse_coo()
        
        indices = A._indices()
        values = A._values()
        
        return cls(
            values, indices[0], indices[1], tuple(A.shape),
            num_partitions=num_partitions, **kwargs
        )
    
    @classmethod
    def from_global_distributed(
        cls,
        values: torch.Tensor,
        row_indices: torch.Tensor,
        col_indices: torch.Tensor,
        shape: Tuple[int, int],
        rank: int,
        world_size: int,
        coords: Optional[torch.Tensor] = None,
        partition_method: str = 'auto',
        device: Optional[Union[str, torch.device]] = None,
        verbose: bool = True
    ) -> "DSparseMatrix":
        """
        Create local partition in a distributed-safe manner.
        
        This method ensures that all ranks compute the same partition assignment
        by having rank 0 compute the partition IDs and broadcasting to all ranks.
        
        Parameters
        ----------
        values : torch.Tensor
            Global non-zero values [nnz]
        row_indices : torch.Tensor
            Global row indices [nnz]
        col_indices : torch.Tensor
            Global column indices [nnz]
        shape : Tuple[int, int]
            Global matrix shape (M, N)
        rank : int
            Current process rank
        world_size : int
            Total number of processes
        coords : torch.Tensor, optional
            Node coordinates for geometric partitioning [num_nodes, dim]
        partition_method : str
            Partitioning method: 'metis', 'rcb', 'slicing', 'simple'
        device : str or torch.device, optional
            Target device
        verbose : bool
            Whether to print partition info
            
        Returns
        -------
        DSparseMatrix
            Local partition matrix for this rank
            
        Example
        -------
        >>> import torch.distributed as dist
        >>> 
        >>> # In each process:
        >>> rank = dist.get_rank()
        >>> world_size = dist.get_world_size()
        >>> 
        >>> local_matrix = DSparseTensor.from_global_distributed(
        ...     val, row, col, shape, 
        ...     rank=rank, world_size=world_size
        ... )
        """
        import torch.distributed as dist
        
        if device is None:
            device = values.device
        if isinstance(device, str):
            device = torch.device(device)
        
        # Compute partition IDs on rank 0 and broadcast
        if rank == 0:
            # Create temporary DSparseTensor to compute partitions
            # Use 'simple' method if METIS might be non-deterministic
            if partition_method == 'auto':
                if coords is not None:
                    actual_method = 'rcb'
                else:
                    # Use simple partitioning by default in distributed mode
                    # to ensure determinism across ranks
                    actual_method = 'simple'
            else:
                actual_method = partition_method
            
            num_nodes = shape[0]
            if actual_method == 'simple':
                partition_ids = partition_simple(num_nodes, world_size)
            elif actual_method == 'metis':
                partition_ids = partition_graph_metis(
                    row_indices, col_indices, num_nodes, world_size
                )
            elif actual_method in ['rcb', 'slicing']:
                if coords is None:
                    raise ValueError(f"Method '{actual_method}' requires coords")
                partition_ids = partition_coordinates(coords, world_size, method=actual_method)
            else:
                raise ValueError(f"Unknown method: {actual_method}")
            
            partition_ids = partition_ids.to(device)
        else:
            # Create empty tensor to receive broadcast
            partition_ids = torch.zeros(shape[0], dtype=torch.int64, device=device)
        
        # Broadcast partition IDs from rank 0 to all ranks
        dist.broadcast(partition_ids, src=0)
        
        # Now create local partition using the consistent partition IDs
        local_matrix = DSparseMatrix.from_global(
            values, row_indices, col_indices, shape,
            world_size, rank,
            partition_ids=partition_ids,
            device=device,
            verbose=verbose and rank == 0  # Only print on rank 0
        )
        
        return local_matrix
    
    @classmethod
    def from_device_mesh(
        cls,
        values: torch.Tensor,
        row_indices: torch.Tensor,
        col_indices: torch.Tensor,
        shape: Tuple[int, int],
        device_mesh: "DeviceMesh",
        coords: Optional[torch.Tensor] = None,
        partition_method: str = 'simple',
        placement: str = 'shard_rows',
        verbose: bool = False
    ) -> "DSparseMatrix":
        """
        Create local partition using PyTorch DeviceMesh.
        
        This is the recommended method for distributed training with PyTorch's
        DTensor ecosystem. Each rank receives only its local partition.
        
        Parameters
        ----------
        values : torch.Tensor
            Global non-zero values [nnz] (same on all ranks)
        row_indices : torch.Tensor
            Global row indices [nnz]
        col_indices : torch.Tensor
            Global column indices [nnz]
        shape : Tuple[int, int]
            Global matrix shape (M, N)
        device_mesh : DeviceMesh
            PyTorch DeviceMesh specifying device topology
        coords : torch.Tensor, optional
            Node coordinates for geometric partitioning
        partition_method : str
            Partitioning method: 'metis', 'rcb', 'simple'
            Default is 'simple' for determinism in distributed setting
        placement : str
            How to distribute: 'shard_rows', 'shard_cols', 'replicate'
        verbose : bool
            Whether to print partition info
            
        Returns
        -------
        DSparseMatrix
            Local partition for this rank
            
        Example
        -------
        >>> from torch.distributed.device_mesh import init_device_mesh
        >>> from torch_sla import DSparseTensor
        >>> 
        >>> # Initialize 4-GPU device mesh
        >>> mesh = init_device_mesh("cuda", (4,), mesh_dim_names=("dp",))
        >>> 
        >>> # Create distributed sparse tensor (each rank gets its partition)
        >>> local_matrix = DSparseTensor.from_device_mesh(
        ...     val, row, col, shape,
        ...     device_mesh=mesh,
        ...     partition_method='simple'
        ... )
        >>> 
        >>> # Local operations
        >>> y_local = local_matrix.matvec(x_local)
        >>> x_local = local_matrix.solve(b_local)
        """
        try:
            from torch.distributed.device_mesh import DeviceMesh
        except ImportError:
            raise ImportError("DeviceMesh requires PyTorch 2.0+. "
                            "Use from_global_distributed() instead.")
        
        if not DIST_AVAILABLE or not dist.is_initialized():
            raise RuntimeError("torch.distributed must be initialized. "
                             "Call dist.init_process_group() first.")
        
        # Get rank info from device mesh
        rank = device_mesh.get_local_rank()
        world_size = device_mesh.size()
        device_type = device_mesh.device_type
        
        # Determine target device
        if device_type == "cuda":
            device = torch.device(f"cuda:{rank}")
        else:
            device = torch.device(device_type)
        
        # Use the distributed-safe factory method
        return cls.from_global_distributed(
            values, row_indices, col_indices, shape,
            rank=rank, world_size=world_size,
            coords=coords,
            partition_method=partition_method,
            device=device,
            verbose=verbose
        )
    
    # =========================================================================
    # Properties
    # =========================================================================
    
    @property
    def shape(self) -> Tuple[int, int]:
        """Global matrix shape."""
        return self._shape
    
    @property
    def num_partitions(self) -> int:
        """Number of partitions."""
        return self._num_partitions
    
    @property
    def device(self) -> torch.device:
        """Device of the matrix data."""
        return self._device
    
    @property
    def dtype(self) -> torch.dtype:
        """Data type of matrix values."""
        return self._values.dtype
    
    @property
    def nnz(self) -> int:
        """Total number of non-zeros."""
        return self._values.size(0)
    
    @property
    def partition_ids(self) -> torch.Tensor:
        """Partition assignment for each node."""
        return self._partition_ids
    
    @property
    def is_cuda(self) -> bool:
        """Check if matrix is on CUDA."""
        return self._device.type == 'cuda'
    
    # =========================================================================
    # Indexing and Iteration
    # =========================================================================
    
    def __len__(self) -> int:
        """Number of partitions."""
        return self._num_partitions
    
    def __getitem__(self, idx: int) -> DSparseMatrix:
        """Get a specific partition."""
        if idx < 0:
            idx = self._num_partitions + idx
        if idx < 0 or idx >= self._num_partitions:
            raise IndexError(f"Partition index {idx} out of range [0, {self._num_partitions})")
        return self._partitions[idx]
    
    def __iter__(self):
        """Iterate over partitions."""
        return iter(self._partitions)
    
    # =========================================================================
    # Device Management
    # =========================================================================
    
    def to(self, device: Union[str, torch.device]) -> "DSparseTensor":
        """
        Move all partitions to a different device.
        
        Parameters
        ----------
        device : str or torch.device
            Target device
            
        Returns
        -------
        DSparseTensor
            New distributed tensor on target device
        """
        if isinstance(device, str):
            device = torch.device(device)
        
        new_tensor = DSparseTensor.__new__(DSparseTensor)
        new_tensor._values = self._values.to(device)
        new_tensor._row_indices = self._row_indices.to(device)
        new_tensor._col_indices = self._col_indices.to(device)
        new_tensor._shape = self._shape
        new_tensor._num_partitions = self._num_partitions
        new_tensor._coords = self._coords
        new_tensor._partition_method = self._partition_method
        new_tensor._verbose = False  # Don't print again
        new_tensor._device = device
        new_tensor._partition_ids = self._partition_ids
        
        # Move partitions
        new_tensor._partitions = [p.to(device) for p in self._partitions]
        
        # Update references
        for mat in new_tensor._partitions:
            mat._all_partitions = [m.partition for m in new_tensor._partitions]
        
        return new_tensor
    
    def cuda(self, device: Optional[int] = None) -> "DSparseTensor":
        """Move to CUDA device."""
        if device is not None:
            return self.to(f'cuda:{device}')
        return self.to('cuda')
    
    def cpu(self) -> "DSparseTensor":
        """Move to CPU."""
        return self.to('cpu')
    
    # =========================================================================
    # Distributed Operations
    # =========================================================================
    
    def halo_exchange_local(self, x_list: List[torch.Tensor]) -> None:
        """
        Local halo exchange for single-process simulation.
        
        Exchanges halo values between all partitions locally.
        Useful for testing without actual distributed setup.
        
        Parameters
        ----------
        x_list : List[torch.Tensor]
            List of local vectors, one per partition. Each vector is
            modified in-place to update halo values.
        """
        if len(x_list) != self._num_partitions:
            raise ValueError(f"Expected {self._num_partitions} vectors, got {len(x_list)}")
        
        for part_id in range(self._num_partitions):
            partition = self._partitions[part_id].partition
            x = x_list[part_id]
            
            halo_offset = len(partition.owned_nodes)
            
            for halo_idx, global_node in enumerate(partition.halo_nodes.tolist()):
                local_halo_idx = halo_offset + halo_idx
                
                for neighbor_id in partition.neighbor_partitions:
                    neighbor_partition = self._partitions[neighbor_id].partition
                    neighbor_g2l = neighbor_partition.global_to_local
                    
                    if global_node < len(neighbor_g2l):
                        local_idx_in_neighbor = neighbor_g2l[global_node].item()
                        if local_idx_in_neighbor >= 0 and local_idx_in_neighbor < len(neighbor_partition.owned_nodes):
                            x[local_halo_idx] = x_list[neighbor_id][local_idx_in_neighbor]
                            break
    
    def matvec_all(
        self,
        x_list: List[torch.Tensor],
        exchange_halo: bool = True
    ) -> List[torch.Tensor]:
        """
        Matrix-vector multiply on all partitions.
        
        Performs y = A @ x for each partition, with optional halo exchange.
        
        Parameters
        ----------
        x_list : List[torch.Tensor]
            List of local vectors, one per partition. Each vector should have
            size = num_owned + num_halo for that partition.
        exchange_halo : bool
            Whether to perform halo exchange before multiplication.
            Default True.
            
        Returns
        -------
        List[torch.Tensor]
            List of result vectors, one per partition. Each result has
            size = num_owned (only owned nodes have valid results).
            
        Example
        -------
        >>> D = SparseTensor(val, row, col, shape).partition(4)
        >>> x_local = D.scatter_local(x_global)
        >>> y_local = D.matvec_all(x_local)
        >>> y_global = D.gather_global(y_local)
        """
        return [self._partitions[i].matvec(x_list[i], exchange_halo=exchange_halo)
                for i in range(self._num_partitions)]
    
    def solve_all(
        self,
        b_list: List[torch.Tensor],
        **kwargs
    ) -> List[torch.Tensor]:
        """
        Solve on all partitions (subdomain solves).
        
        NOTE: This performs LOCAL subdomain solves, NOT a global distributed solve.
        Each partition solves its own local system independently.
        For a true distributed solve, use `solve_distributed()`.
        
        Parameters
        ----------
        b_list : List[torch.Tensor]
            List of local RHS vectors, one per partition
        **kwargs
            Additional arguments passed to each partition's solve method
            
        Returns
        -------
        List[torch.Tensor]
            List of solution vectors, one per partition
        """
        return [self._partitions[i].solve(b_list[i], **kwargs) 
                for i in range(self._num_partitions)]
    
    def solve_distributed(
        self,
        b_global: Union[torch.Tensor, "DTensor"],
        method: str = 'cg',
        atol: float = 1e-10,
        maxiter: int = 1000,
        verbose: bool = False
    ) -> Union[torch.Tensor, "DTensor"]:
        """
        Distributed solve: find x such that A @ x = b using all partitions.
        
        This performs a TRUE distributed solve where all partitions collaborate
        to solve the global system. Uses distributed CG with global reductions.
        
        Parameters
        ----------
        b_global : torch.Tensor or DTensor
            Global RHS vector [N].
            - If torch.Tensor: treated as global vector
            - If DTensor: automatically handles distributed input/output
        method : str
            Solver method: 'cg' (Conjugate Gradient)
        atol : float
            Absolute tolerance for convergence
        maxiter : int
            Maximum iterations
        verbose : bool
            Print convergence info
            
        Returns
        -------
        torch.Tensor or DTensor
            Global solution vector [N].
            Returns DTensor if input is DTensor, otherwise torch.Tensor.
            
        Example
        -------
        >>> D = A.partition(num_partitions=4)
        >>> x = D.solve_distributed(b)  # Distributed CG solve
        >>> residual = torch.norm(A @ x - b)
        
        >>> # With DTensor input
        >>> from torch.distributed.tensor import DTensor, Replicate
        >>> b_dt = DTensor.from_local(b_local, mesh, [Replicate()])
        >>> x_dt = D.solve_distributed(b_dt)  # Returns DTensor
        """
        # Check for DTensor input
        if _is_dtensor(b_global):
            return self._solve_distributed_dtensor(b_global, method, atol, maxiter, verbose)
        
        N = self._shape[0]
        dtype = b_global.dtype
        device = self._device
        
        # Initialize x = 0
        x_global = torch.zeros(N, dtype=dtype, device=device)
        
        # Scatter b to local
        b_local = self.scatter_local(b_global)
        
        # Distributed CG
        if method == 'cg':
            x_global = self._distributed_cg(x_global, b_global, atol, maxiter, verbose)
        else:
            raise ValueError(f"Unknown method: {method}. Supported: 'cg'")
        
        return x_global
    
    def _solve_distributed_dtensor(
        self,
        b_dtensor: "DTensor",
        method: str,
        atol: float,
        maxiter: int,
        verbose: bool
    ) -> "DTensor":
        """
        Distributed solve with DTensor input.
        
        Handles DTensor layout conversion and result wrapping.
        
        Parameters
        ----------
        b_dtensor : DTensor
            Right-hand side as DTensor
        method : str
            Solver method
        atol : float
            Absolute tolerance
        maxiter : int
            Maximum iterations
        verbose : bool
            Print convergence info
            
        Returns
        -------
        DTensor
            Solution as DTensor with same placement as input
        """
        if not DTENSOR_AVAILABLE:
            raise RuntimeError("DTensor support requires PyTorch 2.0+")
        
        # Get DTensor metadata
        device_mesh = b_dtensor.device_mesh
        placements = b_dtensor.placements
        original_placements = tuple(placements)
        
        # Check if input is replicated
        is_replicated = all(isinstance(p, Replicate) for p in placements)
        
        if is_replicated:
            # Input is replicated - extract and solve
            b_local = b_dtensor.to_local()
            x_local = self._solve_distributed_tensor(b_local, method, atol, maxiter, verbose)
            # Wrap result as replicated DTensor
            return DTensor.from_local(x_local, device_mesh, [Replicate()])
        
        # Input is sharded - redistribute to replicated for solve
        replicate_placements = [Replicate() for _ in placements]
        b_replicated = b_dtensor.redistribute(device_mesh, replicate_placements)
        b_full = b_replicated.to_local()
        
        # Solve with full vector
        x_full = self._solve_distributed_tensor(b_full, method, atol, maxiter, verbose)
        
        # Wrap as replicated DTensor
        x_replicated = DTensor.from_local(x_full, device_mesh, [Replicate()])
        
        # Redistribute back to original placement if it was sharded
        if not is_replicated:
            output_placements = []
            for p in original_placements:
                if isinstance(p, Shard):
                    output_placements.append(Shard(p.dim))
                else:
                    output_placements.append(Replicate())
            
            return x_replicated.redistribute(device_mesh, output_placements)
        
        return x_replicated
    
    def _solve_distributed_tensor(
        self,
        b_global: torch.Tensor,
        method: str,
        atol: float,
        maxiter: int,
        verbose: bool
    ) -> torch.Tensor:
        """
        Internal solve implementation for torch.Tensor input.
        
        Separated from solve_distributed to allow DTensor wrapper to call it.
        """
        N = self._shape[0]
        dtype = b_global.dtype
        device = self._device
        
        # Initialize x = 0
        x_global = torch.zeros(N, dtype=dtype, device=device)
        
        # Scatter b to local
        b_local = self.scatter_local(b_global)
        
        # Distributed CG
        if method == 'cg':
            x_global = self._distributed_cg(x_global, b_global, atol, maxiter, verbose)
        else:
            raise ValueError(f"Unknown method: {method}. Supported: 'cg'")
        
        return x_global
    
    def _distributed_cg(
        self,
        x: torch.Tensor,
        b: torch.Tensor,
        atol: float,
        maxiter: int,
        verbose: bool
    ) -> torch.Tensor:
        """
        Distributed Conjugate Gradient.
        
        All partitions work together, with global reductions for inner products.
        """
        N = self._shape[0]
        dtype = b.dtype
        device = self._device
        
        # r = b - A @ x
        Ax = self @ x  # Uses __matmul__ which does scatter -> matvec_all -> gather
        r = b - Ax
        
        # p = r
        p = r.clone()
        
        # rs_old = r^T @ r (global)
        rs_old = torch.dot(r, r)
        
        for i in range(maxiter):
            # Ap = A @ p
            Ap = self @ p
            
            # pAp = p^T @ A @ p (global)
            pAp = torch.dot(p, Ap)
            
            if pAp.abs() < 1e-30:
                if verbose:
                    print(f"  Distributed CG: pAp too small at iter {i}")
                break
            
            # alpha = rs_old / pAp
            alpha = rs_old / pAp
            
            # x = x + alpha * p
            x = x + alpha * p
            
            # r = r - alpha * Ap
            r = r - alpha * Ap
            
            # rs_new = r^T @ r (global)
            rs_new = torch.dot(r, r)
            
            residual = rs_new.sqrt()
            
            if verbose and i % 100 == 0:
                print(f"  Distributed CG iter {i}: residual = {residual:.2e}")
            
            if residual < atol:
                if verbose:
                    print(f"  Distributed CG converged at iter {i}, residual = {residual:.2e}")
                break
            
            if rs_old.abs() < 1e-30:
                break
            
            # beta = rs_new / rs_old
            beta = rs_new / rs_old
            
            # p = r + beta * p
            p = r + beta * p
            
            rs_old = rs_new
        
        return x
    
    def gather_global(self, x_list: List[torch.Tensor]) -> torch.Tensor:
        """
        Gather local vectors to global vector.
        
        Parameters
        ----------
        x_list : List[torch.Tensor]
            List of local vectors, one per partition
            
        Returns
        -------
        torch.Tensor
            Global vector
        """
        x_global = torch.zeros(self._shape[0], dtype=x_list[0].dtype, device=self._device)
        
        for i in range(self._num_partitions):
            partition = self._partitions[i].partition
            owned_nodes = partition.owned_nodes
            num_owned = len(owned_nodes)
            x_global[owned_nodes] = x_list[i][:num_owned].to(self._device)
        
        return x_global
    
    def scatter_local(self, x_global: torch.Tensor) -> List[torch.Tensor]:
        """
        Scatter global vector to local vectors.
        
        Parameters
        ----------
        x_global : torch.Tensor
            Global vector
            
        Returns
        -------
        List[torch.Tensor]
            List of local vectors (with halo values filled)
        """
        x_list = []
        
        for i in range(self._num_partitions):
            partition = self._partitions[i].partition
            local_nodes = partition.local_nodes
            x_local = x_global[local_nodes].to(self._partitions[i].device)
            x_list.append(x_local)
        
        return x_list
    
    def to_sparse_tensor(self) -> "SparseTensor":
        """
        Gather all partitions into a single SparseTensor.
        
        This creates a global SparseTensor from the distributed data.
        Useful for verification, debugging, or when you need to perform
        operations that require the full matrix.
        
        Returns
        -------
        SparseTensor
            Global sparse tensor containing all data
            
        Example
        -------
        >>> D = DSparseTensor(val, row, col, shape, num_partitions=4)
        >>> A = D.to_sparse_tensor()  # Gather to global SparseTensor
        >>> x = A.solve(b)  # Solve on the full matrix
        """
        from .sparse_tensor import SparseTensor
        
        # Return the original global data as SparseTensor
        return SparseTensor(
            self._values.to(self._device),
            self._row_indices.to(self._device),
            self._col_indices.to(self._device),
            self._shape
        )
    
    # Alias for convenience
    gather = to_sparse_tensor
    
    def to_list(self) -> "DSparseTensorList":
        """
        Split into DSparseTensorList based on connected components.
        
        If the matrix has isolated subgraphs (block-diagonal structure),
        splits it into separate distributed matrices, one per component.
        
        Returns
        -------
        DSparseTensorList
            List of distributed matrices, one per connected component.
            
        Notes
        -----
        This is useful when you have a block-diagonal matrix representing
        multiple independent graphs and want to process them separately.
        
        Examples
        --------
        >>> D = DSparseTensor(val, row, col, shape, num_partitions=4)
        >>> if D.has_isolated_components():
        ...     dstl = D.to_list()  # Split into components
        """
        # Get connected components from global data
        sparse = self.to_sparse_tensor()
        sparse_list = sparse.to_connected_components()
        
        # Partition each component
        return DSparseTensorList.from_sparse_tensor_list(
            sparse_list,
            num_partitions=self._num_partitions,
            threshold=1000,  # Default threshold
            device=self._device,
            verbose=False
        )
    
    def has_isolated_components(self) -> bool:
        """
        Check if the matrix has multiple connected components.
        
        Returns
        -------
        bool
            True if matrix has more than one connected component.
        """
        sparse = self.to_sparse_tensor()
        return sparse.has_isolated_components()
    
    @classmethod
    def from_list(
        cls,
        dstl: "DSparseTensorList",
        verbose: bool = False
    ) -> "DSparseTensor":
        """
        Merge DSparseTensorList into a single block-diagonal DSparseTensor.
        
        Parameters
        ----------
        dstl : DSparseTensorList
            List of distributed matrices to merge.
        verbose : bool
            Print info.
            
        Returns
        -------
        DSparseTensor
            Block-diagonal distributed matrix.
            
        Examples
        --------
        >>> dstl = DSparseTensorList.from_sparse_tensor_list(stl, 4)
        >>> D = DSparseTensor.from_list(dstl)  # Merge to block-diagonal
        """
        return dstl.to_block_diagonal()
    
    # =========================================================================
    # DTensor Utilities
    # =========================================================================
    
    def scatter_to_dtensor(
        self,
        x_global: torch.Tensor,
        device_mesh: "DeviceMesh",
        shard_dim: int = 0
    ) -> "DTensor":
        """
        Convert a global tensor to a sharded DTensor aligned with matrix partitioning.
        
        This creates a DTensor where each rank holds the portion of the vector
        corresponding to its owned nodes in the matrix partitioning.
        
        Parameters
        ----------
        x_global : torch.Tensor
            Global vector of shape [N]
        device_mesh : DeviceMesh
            PyTorch DeviceMesh for distribution
        shard_dim : int
            Dimension to shard (default 0 for vectors)
            
        Returns
        -------
        DTensor
            Sharded DTensor with local data for this rank
            
        Example
        -------
        >>> mesh = init_device_mesh("cuda", (4,))
        >>> x_global = torch.randn(N)
        >>> x_dt = D.scatter_to_dtensor(x_global, mesh)
        """
        if not DTENSOR_AVAILABLE:
            raise RuntimeError("DTensor support requires PyTorch 2.0+")
        
        # Create sharded DTensor
        # Each rank gets the portion corresponding to its partition
        placements = [Shard(shard_dim)]
        return DTensor.from_local(
            x_global,  # Will be redistributed by DTensor
            device_mesh,
            placements
        )
    
    def gather_from_dtensor(
        self,
        x_dtensor: "DTensor"
    ) -> torch.Tensor:
        """
        Convert a DTensor to a global tensor.
        
        Parameters
        ----------
        x_dtensor : DTensor
            Distributed tensor
            
        Returns
        -------
        torch.Tensor
            Full global tensor
            
        Example
        -------
        >>> x_global = D.gather_from_dtensor(x_dt)
        """
        if not DTENSOR_AVAILABLE:
            raise RuntimeError("DTensor support requires PyTorch 2.0+")
        
        return x_dtensor.full_tensor()
    
    def to_dtensor(
        self,
        x: torch.Tensor,
        device_mesh: "DeviceMesh",
        replicate: bool = True
    ) -> "DTensor":
        """
        Convert a tensor to DTensor with specified placement.
        
        Parameters
        ----------
        x : torch.Tensor
            Input tensor
        device_mesh : DeviceMesh
            PyTorch DeviceMesh
        replicate : bool
            If True, create a replicated DTensor (same data on all ranks).
            If False, create a sharded DTensor (data is split).
            
        Returns
        -------
        DTensor
            Resulting DTensor
            
        Example
        -------
        >>> mesh = init_device_mesh("cuda", (4,))
        >>> x_dt = D.to_dtensor(x, mesh, replicate=True)
        """
        if not DTENSOR_AVAILABLE:
            raise RuntimeError("DTensor support requires PyTorch 2.0+")
        
        if replicate:
            placements = [Replicate()]
        else:
            placements = [Shard(0)]
        
        return DTensor.from_local(x, device_mesh, placements)
    
    @property
    def supports_dtensor(self) -> bool:
        """Check if DTensor operations are available."""
        return DTENSOR_AVAILABLE
    
    # =========================================================================
    # Distributed Algorithms (True Distributed, No Gather)
    # =========================================================================
    
    def _global_matvec_with_grad(self, x: torch.Tensor) -> torch.Tensor:
        """
        Global matrix-vector multiplication that preserves gradients.
        
        Uses the original COO data to maintain gradient flow.
        For true distributed MPI execution, use _distributed_matvec instead.
        
        This method is used for gradient-enabled operations like eigsh, solve.
        """
        # Use original global COO data for gradient support
        # y[i] = sum_j A[i,j] * x[j]
        # y = scatter_add(values * x[col], row)
        y = torch.zeros(self._shape[0], dtype=x.dtype, device=x.device)
        vals = self._values.to(x.device)
        rows = self._row_indices.to(x.device)
        cols = self._col_indices.to(x.device)
        
        # y[row] += values * x[col]
        contributions = vals * x[cols]
        y = y.scatter_add(0, rows, contributions)
        return y
    
    def _distributed_matvec(self, x: torch.Tensor) -> torch.Tensor:
        """
        Distributed matrix-vector multiplication with gradient support.
        
        For single-node simulation with gradient support, uses _global_matvec_with_grad.
        For true distributed MPI execution, uses scatter -> local matvec -> gather.
        """
        # Check if we need gradients
        if self._values.requires_grad or x.requires_grad:
            # Use global matvec that preserves gradients
            return self._global_matvec_with_grad(x)
        
        # Otherwise use true distributed pattern
        x_local = self.scatter_local(x)
        y_local = self.matvec_all(x_local)
        return self.gather_global(y_local)
    
    def _distributed_lobpcg(
        self,
        k: int,
        largest: bool = True,
        maxiter: int = 1000,
        tol: float = 1e-8
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Distributed LOBPCG eigenvalue solver.
        
        Uses distributed matvec with global QR and Rayleigh-Ritz.
        No data gather required - only needs global reductions.
        """
        N = self._shape[0]
        dtype = self._values.dtype
        device = self._device
        
        # Initialize random subspace (global vectors)
        m = min(2 * k, N)
        X = torch.randn(N, m, dtype=dtype, device=device)
        
        # Global QR decomposition
        X, _ = torch.linalg.qr(X)
        
        eigenvalues_prev = None
        
        for iteration in range(maxiter):
            # Distributed matvec: AX = D @ X (column by column or batched)
            AX = torch.zeros_like(X)
            for j in range(X.shape[1]):
                AX[:, j] = self._distributed_matvec(X[:, j])
            
            # Rayleigh-Ritz: project onto subspace
            # H = X^T @ AX (global reduction)
            H = X.T @ AX
            
            # Solve small eigenvalue problem
            eigenvalues, eigenvectors = torch.linalg.eigh(H)
            
            # Sort eigenvalues
            if largest:
                idx = eigenvalues.argsort(descending=True)
            else:
                idx = eigenvalues.argsort()
            
            eigenvalues = eigenvalues[idx]
            eigenvectors = eigenvectors[:, idx]
            
            # Update X = X @ V
            X = X @ eigenvectors
            
            # Check convergence
            if eigenvalues_prev is not None:
                diff = (eigenvalues[:k] - eigenvalues_prev[:k]).abs()
                if (diff < tol * eigenvalues[:k].abs().clamp(min=1e-10)).all():
                    break
            eigenvalues_prev = eigenvalues.clone()
            
            # Expand subspace with residual
            if iteration < maxiter - 1:
                # Compute residual: R = AX - X @ diag(eigenvalues)
                AX_new = torch.zeros_like(X)
                for j in range(X.shape[1]):
                    AX_new[:, j] = self._distributed_matvec(X[:, j])
                
                residual = AX_new - X * eigenvalues.unsqueeze(0)
                
                # Orthogonalize and expand
                combined = torch.cat([X[:, :k], residual[:, :k]], dim=1)
                X, _ = torch.linalg.qr(combined)
                
                # Pad if needed
                if X.size(1) < m:
                    extra = torch.randn(N, m - X.size(1), dtype=dtype, device=device)
                    X = torch.cat([X, extra], dim=1)
                    X, _ = torch.linalg.qr(X)
        
        return eigenvalues[:k], X[:, :k]
    
    def eigsh(
        self,
        k: int = 6,
        which: str = "LM",
        sigma: Optional[float] = None,
        return_eigenvectors: bool = True,
        maxiter: int = 1000,
        tol: float = 1e-8
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Compute k eigenvalues for symmetric matrices using distributed LOBPCG.
        
        This is a TRUE distributed algorithm - no data gather required.
        Uses distributed matvec with global QR decomposition.
        
        Parameters
        ----------
        k : int, optional
            Number of eigenvalues to compute. Default: 6.
        which : {"LM", "SM", "LA", "SA"}, optional
            Which eigenvalues to find:
            - "LM"/"LA": Largest (default)
            - "SM"/"SA": Smallest
        sigma : float, optional
            Find eigenvalues near sigma (not yet supported).
        return_eigenvectors : bool, optional
            Whether to return eigenvectors. Default: True.
        maxiter : int, optional
            Maximum LOBPCG iterations. Default: 1000.
        tol : float, optional
            Convergence tolerance. Default: 1e-8.
            
        Returns
        -------
        eigenvalues : torch.Tensor
            Shape [k].
        eigenvectors : torch.Tensor or None
            Shape [N, k] if return_eigenvectors is True.
        
        Notes
        -----
        **Distributed Algorithm:**
        
        - Uses distributed LOBPCG (Locally Optimal Block PCG)
        - Only requires distributed matvec + global reductions
        - Memory: O(N * k) per node for eigenvectors
        - Communication: O(k^2) per iteration for Rayleigh-Ritz
        
        **Gradient Support:**
        
        - Gradients flow through the distributed matvec operations
        - O(iterations) graph nodes (not O(1) like adjoint)
        """
        if sigma is not None:
            warnings.warn("sigma (shift-invert) not yet supported for distributed eigsh. Ignoring.")
        
        largest = which in ('LM', 'LA')
        eigenvalues, eigenvectors = self._distributed_lobpcg(k, largest=largest, maxiter=maxiter, tol=tol)
        
        if return_eigenvectors:
            return eigenvalues, eigenvectors
        return eigenvalues, None
    
    def eigs(
        self,
        k: int = 6,
        which: str = "LM",
        sigma: Optional[float] = None,
        return_eigenvectors: bool = True,
        maxiter: int = 1000,
        tol: float = 1e-8
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Compute k eigenvalues using distributed LOBPCG.
        
        For symmetric matrices, equivalent to eigsh().
        For non-symmetric, currently falls back to eigsh() (symmetric assumption).
        
        Parameters
        ----------
        k : int, optional
            Number of eigenvalues to compute. Default: 6.
        which : str, optional
            Which eigenvalues to find.
        sigma : float, optional
            Find eigenvalues near sigma.
        return_eigenvectors : bool, optional
            Whether to return eigenvectors. Default: True.
        maxiter : int, optional
            Maximum iterations. Default: 1000.
        tol : float, optional
            Convergence tolerance. Default: 1e-8.
            
        Returns
        -------
        eigenvalues : torch.Tensor
            Shape [k].
        eigenvectors : torch.Tensor or None
            Shape [N, k] if return_eigenvectors is True.
        """
        # For now, use eigsh (assumes symmetric)
        # TODO: Implement Arnoldi for non-symmetric
        return self.eigsh(k=k, which=which, sigma=sigma, 
                         return_eigenvectors=return_eigenvectors,
                         maxiter=maxiter, tol=tol)
    
    def svd(
        self, 
        k: int = 6,
        maxiter: int = 1000,
        tol: float = 1e-8
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Compute truncated SVD using distributed power iteration.
        
        Uses A^T @ A for eigenvalues, then recovers U from A @ V.
        
        Parameters
        ----------
        k : int, optional
            Number of singular values to compute. Default: 6.
        maxiter : int, optional
            Maximum iterations. Default: 1000.
        tol : float, optional
            Convergence tolerance. Default: 1e-8.
            
        Returns
        -------
        U : torch.Tensor
            Left singular vectors. Shape [M, k].
        S : torch.Tensor
            Singular values. Shape [k].
        Vt : torch.Tensor
            Right singular vectors. Shape [k, N].
        
        Notes
        -----
        **Distributed Algorithm:**
        
        - Computes eigenvalues of A^T @ A using distributed LOBPCG
        - No data gather required
        """
        M, N = self._shape
        dtype = self._values.dtype
        device = self._device
        
        # For SVD, we need A^T @ A which requires transpose
        # Create A^T as a DSparseTensor
        A_T = self.T()
        
        # Power iteration for A^T @ A
        # Initialize random vectors
        V = torch.randn(N, k, dtype=dtype, device=device)
        V, _ = torch.linalg.qr(V)
        
        for iteration in range(maxiter):
            # AV = A @ V
            AV = torch.zeros(M, k, dtype=dtype, device=device)
            for j in range(k):
                AV[:, j] = self._distributed_matvec(V[:, j])
            
            # AtAV = A^T @ (A @ V)
            AtAV = torch.zeros(N, k, dtype=dtype, device=device)
            for j in range(k):
                AtAV[:, j] = A_T._distributed_matvec(AV[:, j])
            
            # QR decomposition
            V_new, R = torch.linalg.qr(AtAV)
            
            # Check convergence
            diff = (V_new - V).norm()
            V = V_new
            
            if diff < tol:
                break
        
        # Compute singular values and U
        # AV = A @ V, then normalize to get U
        AV = torch.zeros(M, k, dtype=dtype, device=device)
        for j in range(k):
            AV[:, j] = self._distributed_matvec(V[:, j])
        
        # S = ||AV[:, j]||
        S = AV.norm(dim=0)
        
        # U = AV / S
        U = AV / S.unsqueeze(0).clamp(min=1e-10)
        
        return U, S, V.T
    
    def norm(self, ord: Literal['fro', 1, 2] = 'fro') -> torch.Tensor:
        """
        Compute matrix norm (distributed).
        
        For Frobenius norm, computed locally and aggregated.
        For spectral norm, uses distributed SVD.
        
        Parameters
        ----------
        ord : {'fro', 1, 2}
            Type of norm:
            - 'fro': Frobenius norm (distributed sum)
            - 1: Maximum column sum
            - 2: Spectral norm (largest singular value via distributed SVD)
            
        Returns
        -------
        torch.Tensor
            Scalar tensor containing the norm value.
        """
        if ord == 'fro':
            # Frobenius norm: sqrt(sum(values^2))
            # This is truly distributed - each partition has its own values
            return torch.sqrt((self._values ** 2).sum())
        elif ord == 2:
            # Spectral norm: largest singular value
            _, S, _ = self.svd(k=1, maxiter=100)
            return S[0]
        elif ord == 1:
            # Maximum column sum - need to gather
            warnings.warn("1-norm requires data gather. Using to_sparse_tensor().")
            return self.to_sparse_tensor().norm(ord=1)
        else:
            raise ValueError(f"Unknown norm order: {ord}")
    
    def condition_number(self, ord: int = 2) -> torch.Tensor:
        """
        Estimate condition number using distributed SVD.
        
        Parameters
        ----------
        ord : int, optional
            Norm order. Default: 2 (spectral).
            
        Returns
        -------
        torch.Tensor
            Condition number estimate (_max / _min).
        """
        if ord == 2:
            # Need largest and smallest singular values
            # Compute k=6 singular values
            _, S, _ = self.svd(k=6, maxiter=200)
            return S[0] / S[-1].clamp(min=1e-10)
        else:
            warnings.warn(f"ord={ord} requires data gather. Using to_sparse_tensor().")
            return self.to_sparse_tensor().condition_number(ord=ord)
    
    def det(self) -> torch.Tensor:
        """
        Compute determinant of the distributed sparse matrix.
        
        WARNING: This operation requires gathering the full matrix to compute
        the determinant, as determinant is a global property that cannot be
        computed in a truly distributed manner without full matrix information.
        
        The determinant is computed by:
        1. Gathering all partitions into a global SparseTensor
        2. Computing the determinant using LU decomposition (CPU) or 
           torch.linalg.det (CUDA)
        
        Returns
        -------
        torch.Tensor
            Determinant value (scalar tensor).
            
        Raises
        ------
        ValueError
            If matrix is not square
            
        Notes
        -----
        - Only square matrices have determinants
        - This method gathers all data, so use with caution for large matrices
        - Supports gradient computation via autograd
        - For very large matrices, consider using log-determinant or other
          approximations instead
        
        Examples
        --------
        >>> import torch
        >>> from torch_sla import DSparseTensor
        >>> 
        >>> # Create distributed sparse matrix
        >>> val = torch.tensor([4.0, -1.0, -1.0, 4.0, -1.0, -1.0, 4.0])
        >>> row = torch.tensor([0, 0, 1, 1, 1, 2, 2])
        >>> col = torch.tensor([0, 1, 0, 1, 2, 1, 2])
        >>> D = DSparseTensor(val, row, col, (3, 3), num_partitions=2)
        >>> 
        >>> # Compute determinant (gathers to single node)
        >>> det = D.det()
        >>> print(det)
        >>>
        >>> # With gradient support
        >>> val = val.requires_grad_(True)
        >>> D = DSparseTensor(val, row, col, (3, 3), num_partitions=2)
        >>> det = D.det()
        >>> det.backward()
        >>> print(val.grad)  # Gradient w.r.t. matrix values
        """
        M, N = self._shape
        
        if M != N:
            raise ValueError(f"Matrix must be square for determinant, got shape ({M}, {N})")
        
        # Warn user about data gather
        warnings.warn(
            "det() requires gathering all partitions to compute the determinant. "
            "This is a global operation that cannot be computed in a truly distributed manner. "
            "For large matrices, this may be memory-intensive."
        )
        
        # Gather to global SparseTensor and compute determinant
        A_global = self.to_sparse_tensor()
        return A_global.det()
    
    def T(self) -> "DSparseTensor":
        """
        Transpose the distributed sparse tensor.
        
        Returns a new DSparseTensor with swapped row/column indices.
        
        Returns
        -------
        DSparseTensor
            Transposed matrix.
        """
        # Swap row and column indices
        return DSparseTensor(
            self._values,
            self._col_indices,  # swap
            self._row_indices,  # swap
            (self._shape[1], self._shape[0]),
            num_partitions=self._num_partitions,
            coords=self._coords,
            partition_method=self._partition_method,
            device=self._device,
            verbose=False
        )
    
    # =========================================================================
    # Methods that require data gather (with warnings)
    # =========================================================================
    
    def to_dense(self) -> torch.Tensor:
        """
        Convert to dense tensor.
        
        WARNING: This gathers all data to a single node.
        Only use for small matrices or debugging.
        
        Returns
        -------
        torch.Tensor
            Dense matrix of shape (M, N).
        """
        warnings.warn("to_dense() gathers all data to a single node. "
                     "Only use for debugging or small matrices.")
        return self.to_sparse_tensor().to_dense()
    
    def is_symmetric(self, atol: float = 1e-8, rtol: float = 1e-5) -> torch.Tensor:
        """
        Check if matrix is symmetric.
        
        Can be done distributedly by comparing values with transpose.
        
        Parameters
        ----------
        atol : float
            Absolute tolerance for symmetry check.
        rtol : float
            Relative tolerance for symmetry check.
            
        Returns
        -------
        torch.Tensor
            Boolean scalar tensor.
        """
        # This can be done without gather by checking local values
        # For now, use simple implementation
        return self.to_sparse_tensor().is_symmetric(atol=atol, rtol=rtol)
    
    def is_positive_definite(self) -> torch.Tensor:
        """
        Check if matrix is positive definite.
        
        Uses distributed eigenvalue computation.
        
        Returns
        -------
        torch.Tensor
            Boolean scalar tensor.
        """
        # Check smallest eigenvalue > 0
        eigenvalues, _ = self.eigsh(k=1, which='SA', return_eigenvectors=False, maxiter=200)
        return eigenvalues[0] > 0
    
    def lu(self):
        """
        Compute LU decomposition.
        
        WARNING: LU is inherently not distributed-friendly.
        This gathers data to a single node.
        
        For distributed solves, use solve_distributed() with iterative methods.
        
        Returns
        -------
        LUFactorization
            Factorization object with solve() method.
        """
        warnings.warn("LU decomposition is not distributed. "
                     "Use solve_distributed() for distributed solves.")
        return self.to_sparse_tensor().lu()
    
    def spy(self, **kwargs):
        """
        Visualize sparsity pattern.
        
        Gathers data for visualization.
        
        Parameters
        ----------
        **kwargs
            Arguments passed to SparseTensor.spy().
        """
        return self.to_sparse_tensor().spy(**kwargs)
    
    def nonlinear_solve(
        self,
        residual_fn,
        u0: torch.Tensor,
        *params,
        method: str = 'newton',
        tol: float = 1e-6,
        atol: float = 1e-10,
        max_iter: int = 50,
        line_search: bool = True,
        verbose: bool = False,
    ) -> torch.Tensor:
        """
        Solve nonlinear equation F(u, D, *params) = 0 using distributed Newton-Krylov.
        
        Uses Jacobian-free Newton-Krylov with distributed CG for linear solves.
        
        Parameters
        ----------
        residual_fn : callable
            Function F(u, D, *params) -> residual tensor.
            D is this DSparseTensor.
        u0 : torch.Tensor
            Initial guess (global vector).
        *params : torch.Tensor
            Additional parameters.
        method : str
            'newton': Newton-Krylov with distributed CG
            'picard': Fixed-point iteration
        tol : float
            Relative tolerance.
        atol : float
            Absolute tolerance.
        max_iter : int
            Maximum outer iterations.
        line_search : bool
            Use Armijo line search.
        verbose : bool
            Print convergence info.
            
        Returns
        -------
        torch.Tensor
            Solution u such that F(u, D, *params)  0.
        
        Notes
        -----
        **Distributed Algorithm:**
        
        - Uses Jacobian-free Newton-Krylov (JFNK)
        - Linear solves use distributed CG
        - Jacobian-vector products computed via finite differences
        """
        u = u0.clone()
        N = u.shape[0]
        dtype = u.dtype
        device = u.device
        
        for outer_iter in range(max_iter):
            # Compute residual
            F = residual_fn(u, self, *params)
            F_norm = F.norm()
            
            if verbose:
                print(f"  Newton iter {outer_iter}: ||F|| = {F_norm:.2e}")
            
            if F_norm < atol:
                if verbose:
                    print(f"  Converged (atol) at iteration {outer_iter}")
                break
            
            if outer_iter > 0 and F_norm < tol * F_norm_init:
                if verbose:
                    print(f"  Converged (rtol) at iteration {outer_iter}")
                break
            
            if outer_iter == 0:
                F_norm_init = F_norm
            
            if method == 'picard':
                # Simple fixed-point: u = u - F (assuming F = Au - b form)
                u = u - F
            else:
                # Newton-Krylov: solve J @ du = -F using CG with Jacobian-vector products
                # J @ v  (F(u + eps*v) - F(u)) / eps
                eps = 1e-7 * max(u.norm(), 1.0)
                
                def matvec(v):
                    """Jacobian-vector product via finite differences."""
                    F_plus = residual_fn(u + eps * v, self, *params)
                    return (F_plus - F) / eps
                
                # Distributed CG for J @ du = -F
                du = torch.zeros_like(u)
                r = -F - matvec(du)  # r = -F - J @ 0 = -F
                p = r.clone()
                rs_old = torch.dot(r, r)
                
                for cg_iter in range(min(100, N)):
                    Jp = matvec(p)
                    pJp = torch.dot(p, Jp)
                    
                    if pJp.abs() < 1e-30:
                        break
                    
                    alpha = rs_old / pJp
                    du = du + alpha * p
                    r = r - alpha * Jp
                    rs_new = torch.dot(r, r)
                    
                    if rs_new.sqrt() < 1e-10:
                        break
                    
                    beta = rs_new / rs_old
                    p = r + beta * p
                    rs_old = rs_new
                
                # Line search
                if line_search:
                    alpha = 1.0
                    F_new_norm = residual_fn(u + alpha * du, self, *params).norm()
                    while F_new_norm > F_norm and alpha > 1e-8:
                        alpha *= 0.5
                        F_new_norm = residual_fn(u + alpha * du, self, *params).norm()
                    u = u + alpha * du
                else:
                    u = u + du
        
        return u
    
    # =========================================================================
    # Matrix Operations
    # =========================================================================
    
    def __matmul__(self, x: Union[torch.Tensor, "DTensor"]) -> Union[torch.Tensor, "DTensor"]:
        """
        Distributed matrix-vector multiplication: y = D @ x
        
        Automatically handles scatter, distributed matvec, and gather.
        Supports gradient computation when values have requires_grad=True.
        
        Parameters
        ----------
        x : torch.Tensor or DTensor
            Global vector of shape (N,) where N = shape[1].
            - If torch.Tensor: treated as global vector (same on all ranks or single-node)
            - If DTensor: automatically handles distributed input/output
            
        Returns
        -------
        torch.Tensor or DTensor
            Global result vector of shape (M,) where M = shape[0].
            Returns DTensor if input is DTensor, otherwise torch.Tensor.
            
        Example
        -------
        >>> D = A.partition(num_partitions=4)
        >>> y = D @ x  # Equivalent to A @ x
        
        >>> # With DTensor input
        >>> from torch.distributed.tensor import DTensor, Replicate
        >>> x_dt = DTensor.from_local(x_local, mesh, [Replicate()])
        >>> y_dt = D @ x_dt  # Returns DTensor
        
        Notes
        -----
        **Gradient Support:**
        
        For single-node simulation with gradient support, uses global COO matvec.
        For true MPI distributed execution without gradients, uses partition-based matvec.
        
        **DTensor Support:**
        
        When input is a DTensor:
        - Replicated DTensor: extracts local tensor and computes as global
        - Sharded DTensor: redistributes to Replicate, computes, then reshards
        """
        # Check for DTensor input
        if _is_dtensor(x):
            return self._matmul_dtensor(x)
        
        return self._distributed_matvec(x)
    
    def _matmul_dtensor(self, x: "DTensor") -> "DTensor":
        """
        Matrix-vector multiplication with DTensor input.
        
        Handles DTensor layout conversion and result wrapping.
        
        Parameters
        ----------
        x : DTensor
            Distributed tensor input
            
        Returns
        -------
        DTensor
            Result as DTensor with same placement as input
        """
        if not DTENSOR_AVAILABLE:
            raise RuntimeError("DTensor support requires PyTorch 2.0+")
        
        # Get DTensor metadata
        device_mesh = x.device_mesh
        placements = x.placements
        
        # Store original placement for output
        original_placements = tuple(placements)
        
        # Check if input is replicated (easiest case)
        is_replicated = all(isinstance(p, Replicate) for p in placements)
        
        if is_replicated:
            # Input is replicated on all ranks - just extract and compute
            x_local = x.to_local()
            y_local = self._distributed_matvec(x_local)
            # Wrap result as replicated DTensor
            return DTensor.from_local(y_local, device_mesh, [Replicate()])
        
        # Input is sharded - need to handle redistribution
        # For sparse matvec, we typically need the full vector on each rank
        # (because sparse matrix rows may reference any column)
        
        # Redistribute to replicated
        replicate_placements = [Replicate() for _ in placements]
        x_replicated = x.redistribute(device_mesh, replicate_placements)
        x_full = x_replicated.to_local()
        
        # Compute matvec with full vector
        y_full = self._distributed_matvec(x_full)
        
        # Wrap as replicated DTensor first
        y_replicated = DTensor.from_local(y_full, device_mesh, [Replicate()])
        
        # Redistribute back to original placement if it was sharded
        if not is_replicated:
            # For output, we shard along the row dimension (dim 0)
            # which corresponds to the matrix row partitioning
            output_placements = []
            for p in original_placements:
                if isinstance(p, Shard):
                    # Preserve shard dimension for output
                    output_placements.append(Shard(p.dim))
                else:
                    output_placements.append(Replicate())
            
            return y_replicated.redistribute(device_mesh, output_placements)
        
        return y_replicated
    
    # =========================================================================
    # Representation
    # =========================================================================
    
    def __repr__(self) -> str:
        return (f"DSparseTensor(shape={self._shape}, num_partitions={self._num_partitions}, "
                f"nnz={self.nnz}, device={self._device})")
    
    # =========================================================================
    # Persistence (I/O)
    # =========================================================================
    
    def save(
        self,
        directory: Union[str, "os.PathLike"],
        verbose: bool = False
    ) -> None:
        """
        Save DSparseTensor to disk.
        
        Creates a directory with metadata and per-partition files.
        
        Parameters
        ----------
        directory : str or PathLike
            Output directory.
        verbose : bool
            Print progress.
        
        Example
        -------
        >>> D = A.partition(num_partitions=4)
        >>> D.save("matrix_dist")
        """
        from .io import save_dsparse
        save_dsparse(self, directory, verbose)
    
    @classmethod
    def load(
        cls,
        directory: Union[str, "os.PathLike"],
        device: Union[str, torch.device] = "cpu"
    ) -> "DSparseTensor":
        """
        Load a complete DSparseTensor from disk.
        
        Parameters
        ----------
        directory : str or PathLike
            Directory containing saved data.
        device : str or torch.device
            Device to load to.
        
        Returns
        -------
        DSparseTensor
            The loaded distributed sparse tensor.
        
        Example
        -------
        >>> D = DSparseTensor.load("matrix_dist", device="cuda")
        """
        from .io import load_dsparse
        return load_dsparse(directory, device)


# =============================================================================
# DSparseTensorList Class
# =============================================================================

class DSparseTensorList:
    """
    Distributed Sparse Tensor List for batched graph operations.
    
    Holds a collection of graphs where:
    - Small graphs are assigned whole to individual ranks
    - Large graphs are partitioned across ranks using METIS/RCB
    
    This is ideal for molecular property prediction and other batched
    graph learning tasks where graphs have varying sizes.
    
    Parameters
    ----------
    local_matrices : List[DSparseMatrix]
        List of local partitions/graphs for this rank.
    graph_ids : List[int]
        Global graph ID for each local matrix.
    graph_sizes : List[int]
        Number of nodes in each global graph.
    is_partitioned : List[bool]
        Whether each graph is partitioned across ranks.
    device : torch.device
        Device for computations.
    
    Examples
    --------
    >>> # Create from SparseTensorList
    >>> stl = SparseTensorList([A1, A2, A3, ...])
    >>> dstl = stl.partition(num_partitions=4)
    >>> 
    >>> # Distributed operations
    >>> y_list = dstl @ x_list  # matmul
    >>> x_list = dstl.solve(b_list)  # solve
    >>> 
    >>> # Gather back
    >>> stl_result = dstl.gather()
    """
    
    def __init__(
        self,
        local_matrices: List[DSparseMatrix],
        graph_ids: List[int],
        graph_sizes: List[int],
        is_partitioned: List[bool],
        rank: int = 0,
        world_size: int = 1,
        device: Optional[Union[str, torch.device]] = None
    ):
        self._local_matrices = local_matrices
        self._graph_ids = graph_ids
        self._graph_sizes = graph_sizes
        self._is_partitioned = is_partitioned
        self._rank = rank
        self._world_size = world_size
        
        if device is None:
            device = local_matrices[0].device if local_matrices else torch.device('cpu')
        if isinstance(device, str):
            device = torch.device(device)
        self._device = device
    
    @classmethod
    def from_sparse_tensor_list(
        cls,
        sparse_list: "SparseTensorList",
        num_partitions: int,
        threshold: int = 1000,
        partition_method: str = 'auto',
        device: Optional[Union[str, torch.device]] = None,
        verbose: bool = False
    ) -> "DSparseTensorList":
        """
        Create DSparseTensorList from SparseTensorList.
        
        Parameters
        ----------
        sparse_list : SparseTensorList
            Input list of sparse matrices.
        num_partitions : int
            Number of partitions (typically = world_size).
        threshold : int
            Graphs with nodes >= threshold are partitioned.
            Smaller graphs are assigned whole to ranks.
        partition_method : str
            Partitioning method for large graphs: 'metis', 'simple', 'auto'.
        device : torch.device, optional
            Target device.
        verbose : bool
            Print partition info.
            
        Returns
        -------
        DSparseTensorList
            Distributed list ready for parallel operations.
            
        Notes
        -----
        **Partition Strategy:**
        
        - Small graphs (nodes < threshold): Assigned whole to ranks
          using round-robin. No edge cuts, minimal communication.
        - Large graphs (nodes >= threshold): Partitioned across ranks
          using METIS/RCB. Requires halo exchange for operations.
        
        This hybrid strategy is optimal for datasets with mixed graph sizes
        (e.g., molecular datasets with varying molecule sizes).
        
        Examples
        --------
        >>> stl = SparseTensorList([A1, A2, A3, ...])  # Many small graphs
        >>> dstl = DSparseTensorList.from_sparse_tensor_list(
        ...     stl, num_partitions=4, threshold=1000
        ... )
        """
        from .sparse_tensor import SparseTensorList
        
        if device is None:
            device = sparse_list.device
        if isinstance(device, str):
            device = torch.device(device)
        
        n_graphs = len(sparse_list)
        graph_sizes = [t.sparse_shape[0] for t in sparse_list]
        
        # Classify graphs
        small_graph_ids = []
        large_graph_ids = []
        
        for i, size in enumerate(graph_sizes):
            if size >= threshold:
                large_graph_ids.append(i)
            else:
                small_graph_ids.append(i)
        
        if verbose:
            print(f"DSparseTensorList: {n_graphs} graphs")
            print(f"  Small (<{threshold} nodes): {len(small_graph_ids)}")
            print(f"  Large (>={threshold} nodes): {len(large_graph_ids)}")
        
        # For single-node simulation, create all partitions
        # In true distributed mode, each rank would only create its portion
        all_partitions = [[] for _ in range(num_partitions)]
        all_graph_ids = [[] for _ in range(num_partitions)]
        all_is_partitioned = [[] for _ in range(num_partitions)]
        
        # Assign small graphs round-robin
        for idx, graph_id in enumerate(small_graph_ids):
            target_rank = idx % num_partitions
            tensor = sparse_list[graph_id]
            
            # Create DSparseMatrix for whole graph (single partition)
            mat = DSparseMatrix.from_global(
                tensor.values, tensor.row_indices, tensor.col_indices,
                tensor.sparse_shape,
                num_partitions=1, my_partition=0,
                device=device, verbose=False
            )
            all_partitions[target_rank].append(mat)
            all_graph_ids[target_rank].append(graph_id)
            all_is_partitioned[target_rank].append(False)
        
        # Partition large graphs across ranks
        for graph_id in large_graph_ids:
            tensor = sparse_list[graph_id]
            
            # Create partitioned matrix
            for part_id in range(num_partitions):
                mat = DSparseMatrix.from_global(
                    tensor.values, tensor.row_indices, tensor.col_indices,
                    tensor.sparse_shape,
                    num_partitions=num_partitions, my_partition=part_id,
                    device=device, verbose=False
                )
                all_partitions[part_id].append(mat)
                all_graph_ids[part_id].append(graph_id)
                all_is_partitioned[part_id].append(True)
        
        if verbose:
            for rank in range(num_partitions):
                n_local = len(all_partitions[rank])
                n_whole = sum(1 for p in all_is_partitioned[rank] if not p)
                print(f"  Rank {rank}: {n_local} local matrices ({n_whole} whole graphs)")
        
        # Return combined structure (for single-node, rank=0 gets all info)
        # In true distributed, each rank would only have its portion
        return cls(
            local_matrices=all_partitions[0],  # For single-node simulation
            graph_ids=all_graph_ids[0],
            graph_sizes=graph_sizes,
            is_partitioned=all_is_partitioned[0],
            rank=0,
            world_size=num_partitions,
            device=device
        )
    
    # =========================================================================
    # Properties
    # =========================================================================
    
    @property
    def device(self) -> torch.device:
        """Device of the matrices."""
        return self._device
    
    @property
    def rank(self) -> int:
        """Current rank."""
        return self._rank
    
    @property
    def world_size(self) -> int:
        """Total number of ranks."""
        return self._world_size
    
    @property
    def num_local_graphs(self) -> int:
        """Number of local matrices on this rank."""
        return len(self._local_matrices)
    
    @property
    def num_total_graphs(self) -> int:
        """Total number of unique graphs (across all ranks)."""
        return len(set(self._graph_ids))
    
    def __len__(self) -> int:
        """Number of local matrices."""
        return len(self._local_matrices)
    
    def __getitem__(self, idx: int) -> DSparseMatrix:
        """Get local matrix by index."""
        return self._local_matrices[idx]
    
    def __iter__(self):
        """Iterate over local matrices."""
        return iter(self._local_matrices)
    
    # =========================================================================
    # Operations
    # =========================================================================
    
    def __matmul__(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:
        """
        Distributed matrix-vector multiplication for all local graphs.
        
        Parameters
        ----------
        x_list : List[torch.Tensor]
            List of input vectors, one per local matrix.
            
        Returns
        -------
        List[torch.Tensor]
            List of output vectors.
        """
        if len(x_list) != len(self._local_matrices):
            raise ValueError(f"Expected {len(self._local_matrices)} vectors, got {len(x_list)}")
        
        results = []
        for mat, x in zip(self._local_matrices, x_list):
            y = mat.matvec(x)
            results.append(y)
        
        return results
    
    def matvec_all(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:
        """Alias for __matmul__."""
        return self @ x_list
    
    def solve_all(
        self,
        b_list: List[torch.Tensor],
        **kwargs
    ) -> List[torch.Tensor]:
        """
        Solve linear systems for all local graphs.
        
        Parameters
        ----------
        b_list : List[torch.Tensor]
            List of RHS vectors, one per local matrix.
        **kwargs
            Arguments passed to DSparseMatrix.solve().
            
        Returns
        -------
        List[torch.Tensor]
            List of solution vectors.
        """
        if len(b_list) != len(self._local_matrices):
            raise ValueError(f"Expected {len(self._local_matrices)} vectors, got {len(b_list)}")
        
        results = []
        for mat, b in zip(self._local_matrices, b_list):
            x = mat.solve(b, **kwargs)
            results.append(x)
        
        return results
    
    # =========================================================================
    # Conversion
    # =========================================================================
    
    def gather(self) -> "SparseTensorList":
        """
        Gather all graphs back to a single SparseTensorList.
        
        In distributed mode, this collects data from all ranks.
        For partitioned graphs, it reassembles the full graph.
        
        Returns
        -------
        SparseTensorList
            Gathered list of sparse tensors.
        """
        from .sparse_tensor import SparseTensor, SparseTensorList
        
        # For single-node simulation, reconstruct from local data
        # In true distributed, this would involve all_gather
        
        tensors = []
        for mat in self._local_matrices:
            # Get global data from partition
            partition = mat.partition
            
            # Reconstruct global indices
            global_row = partition.local_to_global[mat.local_row]
            global_col = partition.local_to_global[mat.local_col]
            
            sparse = SparseTensor(
                mat.local_values,
                global_row,
                global_col,
                mat.global_shape
            )
            tensors.append(sparse)
        
        return SparseTensorList(tensors)
    
    def to_block_diagonal(self) -> DSparseTensor:
        """
        Convert to a single distributed block-diagonal matrix.
        
        Merges all graphs into one block-diagonal DSparseTensor.
        
        Returns
        -------
        DSparseTensor
            Block-diagonal distributed matrix.
        """
        # First gather to SparseTensorList
        stl = self.gather()
        
        # Convert to block diagonal
        block_diag = stl.to_block_diagonal()
        
        # Create DSparseTensor
        return DSparseTensor(
            block_diag.values,
            block_diag.row_indices,
            block_diag.col_indices,
            block_diag.sparse_shape,
            num_partitions=self._world_size,
            device=self._device,
            verbose=False
        )
    
    # =========================================================================
    # Device Management
    # =========================================================================
    
    def to(self, device: Union[str, torch.device]) -> "DSparseTensorList":
        """Move all matrices to device."""
        if isinstance(device, str):
            device = torch.device(device)
        
        new_matrices = [m.to(device) for m in self._local_matrices]
        return DSparseTensorList(
            new_matrices,
            self._graph_ids.copy(),
            self._graph_sizes.copy(),
            self._is_partitioned.copy(),
            self._rank,
            self._world_size,
            device
        )
    
    def cuda(self) -> "DSparseTensorList":
        """Move to CUDA."""
        return self.to('cuda')
    
    def cpu(self) -> "DSparseTensorList":
        """Move to CPU."""
        return self.to('cpu')
    
    def __repr__(self) -> str:
        n_whole = sum(1 for p in self._is_partitioned if not p)
        n_part = sum(1 for p in self._is_partitioned if p)
        return (f"DSparseTensorList(local={len(self)}, "
                f"whole_graphs={n_whole}, partitioned={n_part}, "
                f"rank={self._rank}/{self._world_size}, device={self._device})")
